---
title: "Summary table of models + methods"
---

## Introduction

Throughout the course, we will go over several supervised and unsupervised machine learning models. This page summarizes the models.

```{r, echo=FALSE, message=FALSE}
# Load the necessary libraries
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(gt,
               gtExtras,
               tidyverse)
```

::: panel-tabset
## Imputation

```{r, echo=FALSE, message=FALSE}
# Create the tibble
imputation_methods <- tibble(
  `Method` = c(
    "[Simple Fill](https://en.wikipedia.org/wiki/Imputation_(statistics)#Mean_and_median_imputation)",
    "[KNN Imputation](https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/)",
    "[Soft Impute](https://en.wikipedia.org/wiki/Matrix_completion)",
    "[Iterative Imputer](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)",
    "[Iterative SVD](https://pubmed.ncbi.nlm.nih.gov/11395428/)",
    "[Matrix Factorization](https://en.wikipedia.org/wiki/Matrix_decomposition)",
    "[Nuclear Norm Minimization](https://arxiv.org/abs/0805.4471)",
    "[BiScaler](https://arxiv.org/abs/1410.2596)"
  ),
  Strengths = c(
    "- Simple and fast\n- Works well with small datasets",
    "- Can capture the relationships between features\n- Works well with moderately missing data",
    "- Effective for matrix completion in large datasets\n- Works well with low-rank data",
    "- Can model complex relationships\n- Suitable for multiple imputation",
    "- Good for matrix completion with low-rank assumption\n- Handles larger datasets",
    "- Useful for recommendation systems\n- Can handle large-scale problems",
    "- Theoretically strong for matrix completion\n- Finds the lowest rank solution",
    "- Normalizes data effectively\n- Often used as a preprocessing step"
  ),
  Limitations = c(
    "- May not handle complex data relationships\n- Sensitive to outliers",
    "- Computationally intensive for large datasets\n- Sensitive to the choice of k",
    "- Assumes low-rank data structure\n- Can be sensitive to hyperparameters",
    "- Computationally expensive\n- Depends on the choice of model",
    "- Sensitive to rank selection\n- Computationally demanding",
    "- Requires careful tuning\n- Not suitable for all types of data",
    "- Very computationally intensive\n- Impractical for very large datasets",
    "- Not an imputation method itself\n- Doesn't always converge"
  ),
  `Example Use Cases` = c(
    "- Basic data analysis\n- Quick data cleaning",
    "- Medical data analysis\n- Market research",
    "- Recommender systems\n- Large-scale data projects",
    "- Complex datasets with multiple types of missing data",
    "- Image and video data processing\n- Large datasets with structure",
    "- Recommendation engines\n- User preference analysis",
    "- Research in theoretical data completion\n- Small to medium datasets",
    "- Preprocessing for other imputation methods\n- Data normalization"
  ),
  `Implementation` = c(
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)",
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)",
    "[Python](https://github.com/iskandr/fancyimpute)",
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)",
    "[Python](https://github.com/iskandr/fancyimpute)",
    "[Python](https://github.com/iskandr/fancyimpute)",
    "[Python](https://github.com/iskandr/fancyimpute)",
    "[Python](https://github.com/iskandr/fancyimpute)"
  )
)

# Convert the tibble to a gt table with Markdown formatting
imputation_methods %>%
  gt() %>%
  gt_theme_538()  %>%
  fmt_markdown(columns = everything())

```

## Classification

```{r, echo=FALSE, message=FALSE}
# Create the tibble
ml_models <- tibble(
  `Model Type` = c(
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)",
    "[Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning)",
    "[Random Forest](https://en.wikipedia.org/wiki/Random_forest)",
    "[Support Vector Machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine)",
    "[K-Nearest Neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)",
    "[Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)",
    "[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning)",
    "[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)",
    "[Gradient Boosting Machines (GBM)](https://en.wikipedia.org/wiki/Gradient_boosting)",
    "[Rule-Based Classification](https://en.wikipedia.org/wiki/Rule-based_machine_learning)",
    "[Bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)",
    "[Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning))",
    "[XGBoost](https://en.wikipedia.org/wiki/Xgboost)",
    "[Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)",
    "[Regularized Models (Shrinking)](https://en.wikipedia.org/wiki/Regularization_(mathematics))",
    "[Stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking)"),
  Strengths = c("- Simple and interpretable\n- Fast to train",
                "- Intuitive\n- Can model non-linear relationships",
                "- Handles overfitting\n- Can model complex relationships",
                "- Effective in high dimensional spaces\n- Works well with clear margin of separation",
                "- Simple and intuitive\n- No training phase",
                "- Capable of approximating complex functions\n- Flexible architecture\nTrainable with backpropagation",
                "- Can model highly complex relationships\n- Excels with vast amounts of data\nState-of-the-art results in many domains",
                "- Fast\n- Works well with large feature sets",
                "- High performance\n- Handles non-linear relationships",
                "- Transparent and explainable\n- Easily updated and modified",
                "- Reduces variance\n- Parallelizable",
                "- Reduces bias\n- Combines weak learners",
                "- Scalable and efficient\n- Regularization",
                "- Dimensionality reduction\n- Simple and interpretable",
                "- Prevents overfitting\n- Handles collinearity",
                "- Combines multiple models\n- Can improve accuracy"),
  Limitations = c("- Assumes linear boundaries\n- Not suitable for complex relationships",
                 "- Prone to overfitting\n- Sensitive to small changes in data",
                 "- Slower to train and predict\n- Black box model",
                 "- Sensitive to kernel choice\n- Slow on large datasets",
                 "- Slow during query phase\n- Sensitive to irrelevant features and scale",
                 "- Can require a large number of parameters\n- Prone to overfitting on small data\nTraining can be slow",
                 "- Requires a lot of data\nComputationally intensive\n- Interpretability challenges",
                 "- Assumes feature independence\n- Not suitable for numerical input features",
                 "- Prone to overfitting if not tuned\n- Slow to train",
                 "- Manual rule creation can be tedious\n- May not capture complex relationships",
                 "- May not handle bias well",
                 "- Sensitive to noisy data and outliers",
                 "- Requires careful tuning\n- Can overfit if not used correctly",
                 "- Assumes Gaussian distributed data and equal class covariances",
                 "- Requires parameter tuning\n- May result in loss of interpretability",
                 "- Increases model complexity\n- Risk of overfitting if base models are correlated"),
  `Example Use Cases` = c("- Credit approval\n- Medical diagnosis",
                          "- Customer segmentation\n- Loan default prediction",
                          "- Fraud detection\n- Stock price movement prediction",
                          "- Image classification\n- Handwriting recognition",
                          "- Product recommendation\n- Document classification",
                          "- Pattern recognition\n- Basic image classification\n- Function approximation",
                          "- Advanced image and speech recognition\n- Machine translation\n- Game playing (like AlphaGo)",
                          "- Spam detection\n- Sentiment analysis",
                          "- Web search ranking\n- Ecology predictions",
                          "- Expert systems\n- Business rule enforcement",
                          "- Random Forest is a popular example",
                          "- AdaBoost\n- Gradient Boosting",
                          "- Competitions on Kaggle\n- Retail prediction",
                          "- Face recognition\n- Marketing segmentation",
                          "- Ridge and Lasso regression",
                          "- Meta-modeling\n- Kaggle competitions"),
  Implementation = c(
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)",
    "[Python](https://scikit-learn.org/stable/modules/tree.html)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#forest)",
    "[Python](https://scikit-learn.org/stable/modules/svm.html)",
    "[Python](https://scikit-learn.org/stable/modules/neighbors.html)",
    "[Python](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)",
    "[Python](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)",
    "[Python](https://scikit-learn.org/stable/modules/naive_bayes.html)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)",
    "[Python](https://www.geeksforgeeks.org/rule-based-classifier-machine-learning/)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#boosting)",
    "[Python](https://xgboost.readthedocs.io/en/latest/)",
    "[Python](https://scikit-learn.org/stable/modules/lda_qda.html)",
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization)"
  )
)

# Convert the tibble to a gt table with Markdown formatting
ml_models %>%
  gt() %>%
  gt_theme_538()  %>%
  fmt_markdown(columns = c(`Model Type`, "Strengths", "Limitations", "Example Use Cases", "Implementation")) 
```

## Regression

```{r, echo=FALSE}
# Create the tibble with Markdown hyperlinks for regression models
regression_models <- tibble(
  `Model Type` = c(
    "[Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)",
    "[Polynomial Regression](https://en.wikipedia.org/wiki/Polynomial_regression)",
    "[Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization)",
    "[Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics))",
    "[Elastic Net Regression](https://en.wikipedia.org/wiki/Elastic_net_regularization)",
    "[Quantile Regression](https://en.wikipedia.org/wiki/Quantile_regression)",
    "[Support Vector Regression (SVR)](https://en.wikipedia.org/wiki/Support_vector_machine#Regression)",
    "[Decision Tree Regression](https://en.wikipedia.org/wiki/Decision_tree_learning)",
    "[Random Forest Regression](https://en.wikipedia.org/wiki/Random_forest)",
    "[Gradient Boosting Regression](https://en.wikipedia.org/wiki/Gradient_boosting)"
  ),
  Strengths = c(
    "- Simple and interpretable",
    "- Can model non-linear relationships",
    "- Prevents overfitting\n- Regularizes the model",
    "- Feature selection\n- Regularizes the model",
    "- Balance between Ridge and Lasso",
    "- Models the median or other quantiles",
    "- Flexible\n- Can handle non-linear relationships",
    "- Handles non-linear data\n- Interpretable",
    "- Handles large datasets\n- Reduces overfitting",
    "- High performance\n- Can handle non-linear relationships"
  ),
  Limitations = c(
    "- Assumes linear relationship\n- Sensitive to outliers",
    "- Can overfit with high degrees",
    "- Does not perform feature selection",
    "- May exclude useful variables",
    "- Requires tuning for mixing parameter",
    "- Less interpretable than ordinary regression",
    "- Sensitive to kernel and hyperparameters",
    "- Can overfit on noisy data",
    "- Requires more computational resources",
    "- Prone to overfitting if not tuned"
  ),
  `Example Use Cases` = c(
    "- Sales forecasting\n- Risk assessment",
    "- Growth prediction\n- Non-linear trend modeling",
    "- High-dimensional data\n- Preventing overfitting",
    "- Feature selection\n- High-dimensional datasets",
    "- High-dimensional datasets with correlated features",
    "- Median house price prediction\n- Financial quantiles modeling",
    "- Stock price prediction\n- Non-linear trend modeling",
    "- Price prediction\n- Quality assessment",
    "- Large datasets\n- Environmental modeling",
    "- Web search ranking\n- Price prediction"
  ),
  `Implementation` = c(
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)",
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression)",
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)",
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#lasso)",
    "[Python](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)",
    "[Python](https://www.statsmodels.org/stable/quantile_regression.html)",
    "[Python](https://scikit-learn.org/stable/modules/svm.html#regression)",
    "[Python](https://scikit-learn.org/stable/modules/tree.html#regression)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#forest)",
    "[Python](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)"
  )
)

# Convert the tibble to a gt table with Markdown formatting
regression_models %>%
  gt() %>%
  gt_theme_538()  %>%
  fmt_markdown(columns = c(`Model Type`, "Strengths", "Limitations", "Example Use Cases", "Implementation"))

```

## Clustering

```{r, echo=FALSE}
# Create the tibble with Markdown hyperlinks for clustering models
clustering_models <- tibble(
  `Model Type` = c(
    "[K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)",
    "[Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)",
    "[DBSCAN (Density-Based Clustering)](https://en.wikipedia.org/wiki/DBSCAN)",
    "[Agglomerative Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering#Agglomerative_clustering_example)",
    "[Mean Shift Clustering](https://en.wikipedia.org/wiki/Mean_shift)",
    "[Affinity Propagation](https://en.wikipedia.org/wiki/Affinity_propagation)",
    "[Spectral Clustering](https://en.wikipedia.org/wiki/Spectral_clustering)"
  ),
  Strengths = c(
    "- Simple and widely used\n- Fast for large datasets",
    "- Doesn't require specifying the number of clusters\n- Produces a dendrogram",
    "- Can find arbitrarily shaped clusters\n- Doesn’t require specifying the number of clusters",
    "- Variety of linkage criteria\n- Produces a hierarchy of clusters",
    "- No need to specify number of clusters\n- Can find arbitrarily shaped clusters",
    "- Automatically determines the number of clusters\n- Good for data with lots of exemplars",
    "- Can capture complex cluster structures\n- Can be used with various affinity matrices"
  ),
  Limitations = c(
    "- Sensitive to initial conditions\n- Requires specifying the number of clusters",
    "- May be computationally expensive for large datasets",
    "- Sensitive to scale\n- Requires density parameters to be set",
    "- Not scalable for very large datasets",
    "- Computationally expensive\n- Bandwidth parameter selection is crucial",
    "- High computational complexity\n- Preference parameter can be difficult to choose",
    "- Choice of affinity matrix is crucial\n- Can be computationally expensive"
  ),
  `Example Use Cases` = c(
    "- Market segmentation\n- Image compression",
    "- Taxonomies\n- Determining evolutionary relationships",
    "- Noise detection and anomaly detection",
    "- Sociological hierarchies\n- Taxonomies",
    "- Image analysis\n- Computer vision tasks",
    "- Image recognition\n- Data with many similar exemplars",
    "- Image and speech processing\n- Graph-based clustering"
  ),
  `Implementation` = c(
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#k-means)",
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)",
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#dbscan)",
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)",
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#mean-shift)",
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation)",
    "[Python](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)"
  )
)

# Convert the tibble to a gt table with Markdown formatting
clustering_models %>%
  gt() %>%
  gt_theme_538()  %>%
  fmt_markdown(columns = c(`Model Type`, "Strengths", "Limitations", "Example Use Cases", "Implementation"))

```

## Unsupervised Learning

```{r, echo=FALSE}
# Create the tibble
unsupervised_methods <- tibble(
  `Method` = c(
    "[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)",
    "[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)",
    "[Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)",
    "[Isolation Forest](https://en.wikipedia.org/wiki/Isolation_forest)",
    "[SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)",
    "[ICA](https://en.wikipedia.org/wiki/Independent_component_analysis)"
  ),
  Strengths = c(
    "- Dimensionality reduction\n- Preserves variance",
    "- Captures non-linear structures\n- Good for visualization",
    "- Dimensionality reduction\n- Non-linear relationships",
    "- Effective for high-dimensional data\n- Fast and scalable",
    "- Matrix factorization\n- Efficient for large datasets",
    "- Identifies independent components\n- Signal separation"
  ),
  Limitations = c(
    "- Linear method\n- Not for categorical data",
    "- Computationally expensive\n- Not for high-dimensional data",
    "- Neural network knowledge\n- Computationally intensive",
    "- Randomized\n- May miss some anomalies",
    "- Assumes linear relationships\n- Sensitive to scaling",
    "- Non-Gaussian components\n- Sensitive to noise"
  ),
  `Example Use Cases` = c(
    "- Feature extraction\n- Data compression",
    "- Data visualization\n- Exploratory analysis",
    "- Feature learning\n- Noise reduction",
    "- Fraud detection\n- Network security",
    "- Recommender systems\n- Latent semantic analysis",
    "- Blind signal separation\n- Feature extraction"
  ),
  `Implementation` = c(
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)",
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)",
    "[Python](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)",
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)",
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)",
    "[Python](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html)"
  )
)

# Convert the tibble to a gt table with Markdown formatting
unsupervised_methods %>%
  gt() %>%
  gt_theme_538()  %>%
  fmt_markdown(columns = everything())

```

## Association

```{r, echo=FALSE}
# Create the tibble
association_methods <- tibble(
  `Method` = c(
    "[Apriori Algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm)",
    "[FP-Growth Algorithm](https://en.wikipedia.org/wiki/Association_rule_learning#FP-growth_algorithm)",
    "[Eclat Algorithm](https://en.wikipedia.org/wiki/Eclat_algorithm)",
    "[GSP (Generalized Sequential Pattern)](https://en.wikipedia.org/wiki/Sequential_Pattern_Mining#GSP_(Generalized_Sequential_Pattern)_Algorithm)",
    "[RuleGrowth Algorithm](https://www.philippe-fournier-viger.com/spmf/rulegrowth.pdf)"
  ),
  Strengths = c(
    "- Well-known and widely used\n- Easy to understand and implement",
    "- Faster than Apriori\n- Efficient for large datasets",
    "- Faster than Apriori\n- Scalable and easy to parallelize",
    "- Identifies sequential patterns\n- Flexible for various datasets",
    "- Efficient for mining sequential rules\n- Works well with sparse datasets"
  ),
  Limitations = c(
    "- Can be slow on large datasets\n- Generates a large number of candidate sets",
    "- Memory intensive\n- Can be complex to implement",
    "- Limited to binary attributes\n- Generates many candidate itemsets",
    "- Can be computationally expensive\n- Not as efficient for very large databases",
    "- Requires careful parameter setting\n- Less known and used than Apriori or FP-Growth"
  ),
  `Example Use Cases` = c(
    "- Market basket analysis\n- Cross-marketing strategies",
    "- Frequent itemset mining in large databases\n- Customer purchase patterns",
    "- Market basket analysis\n- Binary classification tasks",
    "- Customer purchase sequence analysis\n- Event sequence analysis",
    "- Analyzing customer shopping sequences\n- Detecting patterns in web browsing data"
  ),
  `Implementation` = c(
    "[Python](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)",
    "[Python](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/)",
    "[Python](https://github.com/tommyod/Efficient-Apriori)",
    "[Python](https://github.com/jacksonpradolima/gsp-py)",
    "[Python](https://pypi.org/project/spmf/)"
  )
)

# Convert the tibble to a gt table with Markdown formatting
association_methods %>%
  gt() %>%
  gt_theme_538()  %>%
  fmt_markdown(columns = everything())

```
:::
