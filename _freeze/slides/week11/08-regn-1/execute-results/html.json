{
  "hash": "96f86deb675d77cb0fa9a4a513be558a",
  "result": {
    "markdown": "---\ntitle: Regressions II\nsubtitle: Lecture 8\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   HW 04 is due March 29, 11:59pm\n\n-   Final project peer-review is Wed Mar 27\n\n## Setup {.smaller}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code}\n# Import all required libraries\n# Data handling and manipulation\nimport pandas as pd\nimport numpy as np\n\n# Implementing and selecting models\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom itertools import combinations\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# For advanced visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Show computation time\nimport time\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"white\")\n```\n:::\n\n\n# Regressions II\n\n## Our data: Indoor air pollution {.smaller}\n\n::: panel-tabset\n## Read + head\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npollution = pd.read_csv(\"data/merged_pollution.csv\", encoding = 'iso-8859-1')\npollution.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=96}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>Year</th>\n      <th>access_clean_perc</th>\n      <th>GDP</th>\n      <th>popn</th>\n      <th>Death_Rate_ASP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2000.0</td>\n      <td>-1.371886</td>\n      <td>0.000000</td>\n      <td>-0.104197</td>\n      <td>371.951345</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2001.0</td>\n      <td>-1.353313</td>\n      <td>0.000000</td>\n      <td>-0.102565</td>\n      <td>368.490253</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2002.0</td>\n      <td>-1.330292</td>\n      <td>-0.877632</td>\n      <td>-0.100603</td>\n      <td>355.870851</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2003.0</td>\n      <td>-1.302300</td>\n      <td>-0.875238</td>\n      <td>-0.098471</td>\n      <td>350.188748</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2004.0</td>\n      <td>-1.276925</td>\n      <td>-0.877087</td>\n      <td>-0.096407</td>\n      <td>341.858106</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Metadata\n\n| **variable**      | **class** | **description**                                                        |\n|---------------|--------------|--------------------------------------------|\n| Entity            | character | Country, unique number identifier                                      |\n| Year              | double    | Year                                                                   |\n| access_clean_perc | double    | \\% of population with access to clean cooking fuels                    |\n| GDP               | double    | GDP per capita, PPP (constant 2017 international \\$)                   |\n| popn              | character | Country population                                                     |\n| Death_rate_ASP    | double    | Cause of death related to air pollution from solid fuels, standardized |\n\n## Info\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\npollution.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3264 entries, 0 to 3263\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Entity             3264 non-null   int64  \n 1   Year               3264 non-null   float64\n 2   access_clean_perc  3264 non-null   float64\n 3   GDP                3264 non-null   float64\n 4   popn               3264 non-null   float64\n 5   Death_Rate_ASP     3264 non-null   float64\ndtypes: float64(5), int64(1)\nmemory usage: 153.1 KB\n```\n:::\n:::\n\n\n## Describe\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\npollution.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=98}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>Year</th>\n      <th>access_clean_perc</th>\n      <th>GDP</th>\n      <th>popn</th>\n      <th>Death_Rate_ASP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3264.000000</td>\n      <td>3264.00000</td>\n      <td>3.264000e+03</td>\n      <td>3264.000000</td>\n      <td>3.264000e+03</td>\n      <td>3264.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>95.500000</td>\n      <td>2008.00000</td>\n      <td>-1.349683e-16</td>\n      <td>0.000000</td>\n      <td>4.353816e-17</td>\n      <td>70.587846</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>55.433366</td>\n      <td>4.89973</td>\n      <td>1.000153e+00</td>\n      <td>1.000153</td>\n      <td>1.000153e+00</td>\n      <td>87.057969</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>2000.00000</td>\n      <td>-1.598171e+00</td>\n      <td>-0.906714</td>\n      <td>-1.451953e-01</td>\n      <td>0.005738</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>47.750000</td>\n      <td>2004.00000</td>\n      <td>-1.064244e+00</td>\n      <td>-0.742517</td>\n      <td>-1.417803e-01</td>\n      <td>1.090309</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>95.500000</td>\n      <td>2008.00000</td>\n      <td>4.424448e-01</td>\n      <td>-0.337723</td>\n      <td>-1.295730e-01</td>\n      <td>23.828597</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>143.250000</td>\n      <td>2012.00000</td>\n      <td>9.444564e-01</td>\n      <td>0.311943</td>\n      <td>-9.508658e-02</td>\n      <td>135.902705</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>191.000000</td>\n      <td>2016.00000</td>\n      <td>1.013911e+00</td>\n      <td>5.036683</td>\n      <td>1.458832e+01</td>\n      <td>474.973060</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Missing values\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\npollution.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=99}\n```\nEntity               0\nYear                 0\naccess_clean_perc    0\nGDP                  0\npopn                 0\nDeath_Rate_ASP       0\ndtype: int64\n```\n:::\n:::\n\n\n:::\n\n## Multiple regression\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nX = pollution[['Entity', 'Year', 'access_clean_perc', 'GDP', 'popn']]  \ny = pollution['Death_Rate_ASP']  \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX_train_with_const = sm.add_constant(X_train)\nX_test_with_const = sm.add_constant(X_test)\n\nX_train_with_const = X_train_with_const.astype(int)\nX_test_with_const = X_test_with_const.astype(int)\n\nmodel = sm.OLS(y_train, X_train_with_const).fit()\n\nmodel_summary2 = model.summary2()\nprint(model_summary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    Results: Ordinary least squares\n========================================================================\nModel:                 OLS               Adj. R-squared:      0.602     \nDependent Variable:    Death_Rate_ASP    AIC:                 28325.4656\nDate:                  2024-01-26 18:04  BIC:                 28360.6705\nNo. Observations:      2611              Log-Likelihood:      -14157.   \nDf Model:              5                 F-statistic:         791.9     \nDf Residuals:          2605              Prob (F-statistic):  0.00      \nR-squared:             0.603             Scale:               3005.9    \n------------------------------------------------------------------------\n                    Coef.   Std.Err.    t     P>|t|    [0.025    0.975] \n------------------------------------------------------------------------\nconst             3398.4418 439.0748   7.7400 0.0000 2537.4709 4259.4127\nEntity              -0.0078   0.0198  -0.3950 0.6929   -0.0466    0.0309\nYear                -1.6614   0.2186  -7.5988 0.0000   -2.0902   -1.2327\naccess_clean_perc -104.0345   1.8234 -57.0559 0.0000 -107.6099 -100.4591\nGDP                 12.3518   1.9556   6.3161 0.0000    8.5171   16.1866\npopn                 0.0222   1.1368   0.0195 0.9845   -2.2070    2.2513\n------------------------------------------------------------------------\nOmnibus:                412.801        Durbin-Watson:           1.952   \nProb(Omnibus):          0.000          Jarque-Bera (JB):        1004.679\nSkew:                   0.882          Prob(JB):                0.000   \nKurtosis:               5.474          Condition No.:           822678  \n========================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n[2] The condition number is large, 8.23e+05. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n```\n:::\n:::\n\n\n## Regularization\n\n> A technique used in regression to avoid **overfitting** by shrinking the coefficient estimates to 0.\n\n**Two main methods:**\n\n::: incremental\n1.  **Ridge Regression**\n\n2.  **Lasso Regression**\n\n3.  ...but also note cross-validation for hyperparameter tuning\n:::\n\n## Ridge regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"65%\"}\n![](images/ridge-1.png)\n:::\n\n::: {.column width=\"35%\"}\n<br>\n\n![](images/ridge-2.png){width=\"242\"}\n:::\n:::\n\n## Variance inflation factor\n\n::: incremental\n-   VIF quantifies multicollinearity in OLS regressions\n\n-   Assesses how much variation is increased by multicollinearity\n\n-   High VIF indicated that predictor variables can be linearly predicted by each other\n:::\n\n#### Formula\n\n$VIF_j = \\frac{1}{1-R^{2}_j}$\n\n::: incremental\n-   $VIF_j$ is the Variance Inflation Factor for the $j^{th}$ predictor variable.\n\n-   $R^{2}_j$ is the coefficient of determination obtained by regressing the $j^{th}$ predictor variable against all other predictor variables.\n\n-   Ranges from 1-5 (or 10)\n:::\n\n## Formula\n\n$RSS + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n\n::: incremental\n-   Where $j$ ranges from 1 to $p$ and $\\lambda \\geq 0$\n\n-   $\\sum_{j=1}^{p} \\beta_j^2$ is the L2 normalization term\n:::\n\n## Key points\n\n::: incremental\n-   **Penalized Regression**: Adds a penalty to OLS to regularize coefficients, aiding in handling multicollinearity and reducing complexity.\n\n-   **Coefficient Shrinkage**: Coefficients shrink towards zero, enhancing stability and accuracy.\n\n-   **L2 Regularization**: Employs squared coefficient sum as a penalty, regulated by $\\lambda$.\n\n-   **Bias-Variance Trade-off**: Slightly increases bias to reduce variance, preventing overfitting.\n\n-   **Efficient Computation**: Features a closed-form solution, ensuring computational efficiency.\n\n-   **No Feature Elimination**: Maintains all features due to non-zero coefficients, unlike Lasso.\n\n-   **Effective in** $p > n$: Remains effective when predictors outnumber observations.\n\n-   **Interpretability**: Less interpretable because all predictors are included.\n:::\n:::\n\n## Investigate VIF\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nVIFs = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nfor idx, vif in enumerate(VIFs):\n    print(f\"VIF for column {X.columns[idx]}: {vif.round(3)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVIF for column Entity: 4.043\nVIF for column Year: 4.024\nVIF for column access_clean_perc: 1.688\nVIF for column GDP: 1.689\nVIF for column popn: 1.012\n```\n:::\n:::\n\n\n::: incremental\n-   `Entity` and `Year` have relatively high VIF\n\n-   Remaining columns relatively low VIF\n:::\n\n## Ridge regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Add a constant to the model (for statsmodels)\nX_train_const = sm.add_constant(X_train)\nX_test_const = sm.add_constant(X_test)\n\n# Initialize the Ridge Regression model\nridge_reg = Ridge(alpha = 1)  # Alpha is the regularization strength; adjust accordingly\n\n# Fit the model\nridge_reg.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = ridge_reg.predict(X_test)\n\n# Calculate and print the MSE\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse.round(2)}\\n')\n\n# Fit the model using statsmodels for detailed statistics (AIC, BIC, Adjusted R-squared)\nols_model = sm.OLS(y_train, X_train_const).fit()\n\n# Print the summary for AIC, BIC, and Adjusted R-squared\nprint(ols_model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1526.63\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:         Death_Rate_ASP   R-squared:                       0.817\nModel:                            OLS   Adj. R-squared:                  0.816\nMethod:                 Least Squares   F-statistic:                     2321.\nDate:                Fri, 26 Jan 2024   Prob (F-statistic):               0.00\nTime:                        18:04:12   Log-Likelihood:                -13149.\nNo. Observations:                2611   AIC:                         2.631e+04\nDf Residuals:                    2605   BIC:                         2.634e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nconst              2037.3404    299.077      6.812      0.000    1450.888    2623.792\nEntity               -0.0016      0.013     -0.121      0.904      -0.028       0.025\nYear                 -0.9794      0.149     -6.576      0.000      -1.271      -0.687\naccess_clean_perc   -79.3547      0.951    -83.478      0.000     -81.219     -77.491\nGDP                   2.2533      0.953      2.365      0.018       0.385       4.122\npopn                 -2.5824      0.730     -3.536      0.000      -4.015      -1.150\n==============================================================================\nOmnibus:                     1268.738   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            22415.703\nSkew:                           1.864   Prob(JB):                         0.00\nKurtosis:                      16.861   Cond. No.                     8.24e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 8.24e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-1_files/figure-revealjs/cell-10-output-1.png){width=832 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning {.smaller}\n\n> Optimizing the hyperparameters of a machine learning model to enhance its performance. The process aims to find the best combination of hyperparameters that results in the most accurate predictions for a given dataset.\n\n**Key points:**\n\n::: incremental\n-   **Hyperparameters**: Pre-set parameters influencing model behavior, not derived from data.\n\n-   **Search Methods**: Techniques like Grid Search and Random Search to explore hyperparameter spaces.\n\n-   **Cross-Validation**: Essential for assessing model generalizability during tuning.\n\n-   **Performance Metrics**: Criteria like accuracy or MSE to evaluate hyperparameter efficacy.\n\n-   **Computational Cost**: Potentially high, depending on hyperparameter space complexity.\n:::\n\n## Model tuning: ridge regression {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Define a set of alpha values\nalphas = np.logspace(-6, 6, 13)\n\n# Initialize RidgeCV\nridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)\n\n# Fit the model\nridge_cv.fit(X_train, y_train)\n\n# Best alpha value\nprint(f'Best alpha: {ridge_cv.alpha_}')\n\n# Re-initialize and fit the model with the best alpha\nbest_ridge = Ridge(alpha = ridge_cv.alpha_)\nbest_ridge.fit(X_train, y_train)\n\n# Make new predictions\ny_pred_best = best_ridge.predict(X_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest alpha: 0.1\n```\n:::\n:::\n\n\n## Re-evaluate model\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Calculate R-squared\nr2_best = r2_score(y_test, y_pred_best)\nprint(f'R-squared with best alpha: {r2_best.round(4)}')\n\n# Calculate Mean Squared Error (MSE)\nmse_best = mean_squared_error(y_test, y_pred_best)\nprint(f'Mean Squared Error with best alpha: {mse_best.round(3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared with best alpha: 0.8002\nMean Squared Error with best alpha: 1526.568\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Assuming `y_pred` are the predictions from the initial Ridge model\nmse_initial = mean_squared_error(y_test, y_pred)\nr2_initial = r2_score(y_test, y_pred)\n\n# Print comparison\nprint(f'Initial MSE: {mse_initial.round(3)}, Best Alpha MSE: {mse_best.round(3)}')\nprint(f'Initial R-squared: {r2_initial}, Best Alpha R-squared: {r2_best}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial MSE: 1526.632, Best Alpha MSE: 1526.568\nInitial R-squared: 0.800226697062056, Best Alpha R-squared: 0.8002349736913377\n```\n:::\n:::\n\n\n## Residuals\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_best = y_test - y_pred_best\nplt.scatter(y_pred_best, residuals_best, alpha = 0.5)\nplt.axhline(y = 0, color = 'r', linestyle = '--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot with Best Alpha')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-1_files/figure-revealjs/cell-14-output-1.png){width=829 height=455}\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "08-regn-1_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}