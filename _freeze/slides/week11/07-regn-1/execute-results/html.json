{
  "hash": "6cb523134fc3816fd3b641a0aeb6419b",
  "result": {
    "markdown": "---\ntitle: Regressions I\nsubtitle: Lecture 7\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   RQ 3 is due today, 11:59pm\n\n-   HW 3 is due today, 11:59pm\n\n-   Final project peer-review is Wed Mar 27\n\n## Setup {.smaller}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code}\n# Import all required libraries\n# Data handling and manipulation\nimport pandas as pd\nimport numpy as np\n\n# Implementing and selecting models\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom itertools import combinations\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\n\n# For advanced visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Show computation time\nimport time\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"white\")\n```\n:::\n\n\n# Regressions\n\n## Linear regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-3-output-1.png){width=800 height=415}\n:::\n:::\n\n\n## Definition\n\n**Objective**: Minimize the sum of squared differences between observed and predicted.\n\n**Model structure:**\n\n$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$\n\n::: incremental\n-   $Y_i$: Dependent/response variable\n\n-   $X_i$: Independent/predictor variable\n\n-   $\\beta_0$: y-intercept\n\n-   $\\beta_1$: Slope\n\n-   $\\epsilon_i$: Random error term, deviation of the real value from predicted\n:::\n\n## Key points\n\n::: incremental\n1.  **Assumptions**: Linearity, independent residuals, constant variance (homoscedasticity), and normally distributed residuals.\n\n2.  **Goodness of Fit**: Assessed by $R^2$, the proportion of variance in $Y$ explained by $X$.\n\n3.  **Statistical Significance**: Tested by t-tests on the coefficients.\n\n4.  **Confidence Intervals**: Provide a range for the estimated coefficients.\n\n5.  **Predictions**: Use the model to predict $Y$ for new $X$ values.\n\n6.  **Diagnostics**: Check residuals for model assumption violations.\n\n7.  **Sensitivity**: Influenced by outliers which can skew the model.\n\n8.  **Applications**: Used across various fields for predictive modeling and data analysis.\n:::\n\n## \n:::\n\n## Assumptions {.smaller}\n\n::: columns\n::: {.column .fragment width=\"33.3%\" fragment-index=\"1\"}\n**Linearity**\n\n(Linear relationship between Y \\~ X)\n\n![](images/assumptions1.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"2\"}\n**Homoscedasticity**\n\n(Equal variance among variables)\n\n![](images/assumptions2.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"3\"}\n**Multivartiate Normality**\n\n(Normally distributed residuals)\n\n![](images/assumptions3.png)\n:::\n:::\n\n::: columns\n::: {.column .fragment width=\"33.3%\" fragment-index=\"4\"}\n**Independence**\n\n(Observations are independent)\n\n![](images/assumptions4.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"5\"}\n**Lack of Multicollinearity**\n\n(Predictors are not correlated)\n\n![](images/assumptions5.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"6\"}\n**Outlier check**\n\n(Technically not an assumption)\n\n![](images/assumptions6.png)\n:::\n:::\n\n## Ordinary Least Squares (OLS) {.smaller}\n\n::: columns\n::: {.column .fragment width=\"50%\" fragment-index=\"1\"}\n![](images/error-in-machine-learning-ols.webp)\n:::\n\n::: {.column .fragment width=\"50%\" fragment-index=\"2\"}\n![](images/error-ordinary-least-squares-ols.webp)\n:::\n:::\n\n::: {.fragment fragment-index=\"3\"}\n$\\displaystyle y_{i}=\\beta_{1} x{i_1}+\\beta_{2} x{i_2}+\\cdots +\\beta_{p} x{i_p}+\\varepsilon _{i}$\n\n::: incremental\n1.  $y_i$: Dependent variable for the $i$-th observation.\n2.  $\\beta_1, \\beta_2, …, \\beta_p$: Coefficients representing the impact of each independent variable.\n3.  $x_{i1}, ​x_{i1},…, x_{ip}$: Independent variables for the $i$-th observation.\n4.  $\\epsilon_i$​: Error term for the $i$-th observation, indicating unexplained variance.\n:::\n:::\n\n## Assessing accuracy of coefficients {.smaller}\n\n**Two major methods**:\n\n::: incremental\n1.  **Standard errors (SE)**:\n\n-   Indicate the precision of coefficient estimates; smaller values suggest more precise estimates.\n\n-   Formula: $\\sigma_{\\bar{x}} \\approx \\frac{\\sigma_x}{\\sqrt{n}}$\n\n2.  **Confidence Intervals**:\n\n-   Ranges constructed from standard errors that indicate where the true coefficient is likely to fall, typically at a 95% confidence level.\n\n-   Derived from same method as **p-values**\n:::\n\n## Assessing accuracy of model {.smaller}\n\n::: panel-tabset\n## Overview\n\n::: {.fragment fragment-index=\"1\"}\n**Three major methods**:\n\n1.  **Mean square error (MSE)**: MSE of a **predictor** is calculated as the average of the squares of the errors, where the error is the difference between the actual value and the predicted value.\n2.  **R-squared (**$R^2$**)**: Proportion of variance in the dependent variable explained by the model; closer to 1 indicates a better fit.\n3.  **Adjusted R-squared (**$R^2_{adj}$**)**: Modified R² that accounts for the number of predictors; useful for comparing models with different numbers of independent variables.\n4.  **Residual plots**: Visual check for randomness in residuals; patterns may indicate model issues like non-linearity or heteroscedasticity.\n:::\n\n## MSE\n\n> In [statistics](https://en.wikipedia.org/wiki/Statistics \"Statistics\"), the mean squared error (MSE) or mean squared deviation (MSD) of an [estimator](https://en.wikipedia.org/wiki/Estimator \"Estimator\") measures the [average](https://en.wikipedia.org/wiki/Expected_value \"Expected value\") of the squares of the [errors](https://en.wikipedia.org/wiki/Error_(statistics) \"Error (statistics)\")---that is, the average squared difference between the estimated values and the actual value. The MSE either assesses the quality of a [***predictor***](https://en.wikipedia.org/wiki/Predictor_(statistics) \"Predictor (statistics)\")or of an [*estimator*](https://en.wikipedia.org/wiki/Estimator \"Estimator\")*.*\n\n::: {.fragment fragment-index=\"2\"}\n**Predictor**\n\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n\n-   $y_i$: the actual values\n\n-   $\\hat{y}_i$: the predicted values\n\n-   $n$: sample size\n:::\n\n## R-squared\n\n::: {.fragment fragment-index=\"3\"}\nDefine the [residuals](https://en.wikipedia.org/wiki/Residuals_(statistics) \"Residuals (statistics)\") as $e_i = y_i − f_i$(forming a vector $e$).\n:::\n\n::: {.fragment fragment-index=\"4\"}\nIf $\\bar{y}$ is the observed mean:\n\n$\\bar{y}=\\frac{1}{n} \\sum_{i=1}^{n}y_i$\n:::\n\n::: {.fragment fragment-index=\"5\"}\nthen the variability of the data set can be measured with two [sums of squares](https://en.wikipedia.org/wiki/Mean_squared_error \"Mean squared error\") (SS) formulas:\n:::\n\n::: {.fragment fragment-index=\"5\"}\n[**Residual SS**](https://en.wikipedia.org/wiki/Residual_sum_of_squares):\n:::\n\n::: {.fragment fragment-index=\"6\"}\n$RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n:::\n\n::: {.fragment fragment-index=\"5\"}\n[**Total SS**](https://en.wikipedia.org/wiki/Total_sum_of_squares):\n:::\n\n::: {.fragment fragment-index=\"6\"}\n$TSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$\n:::\n\n::: {.fragment fragment-index=\"7\"}\n**R-squared**:\n\n$R^2 = 1 - \\frac{RSS}{TSS}$\n:::\n\n## Adjusted R-squared\n\n::: {.fragment fragment-index=\"8\"}\n**Formula** $R^2 = 1 - \\frac{RSS / df_{res}}{TSS / df_{tot}}$\n:::\n\n::: {.fragment fragment-index=\"9\"}\n**Key points**:\n\n::: incremental\n-   **Penalizes Complexity**: Adjusts for the number of terms in the model, penalizing the addition of irrelevant predictors.\n\n-   **Comparability**: More reliable than R-squared for comparing models with different numbers of independent variables.\n\n-   **Value Range**: Can be negative if the model is worse than using just the mean of the dependent variable, whereas R-squared is always between 0 and 1.\n:::\n:::\n\n::: incremental\n-   $df_{res}$: [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) of the estimate of the population variance around the model\n-   $df_{tot}$: degrees of freedom of the estimate of the population variance around the mean\n:::\n\n## Residual plots\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-4-output-1.png){width=664 height=529}\n:::\n:::\n\n\n:::\n\n## Our data {.smaller}\n\n::: panel-tabset\n## Read\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nelmhurst = pd.read_csv(\"data/elmhurst.csv\")\n```\n:::\n\n\n## Info\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nelmhurst.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50 entries, 0 to 49\nData columns (total 3 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   family_income  50 non-null     float64\n 1   gift_aid       50 non-null     float64\n 2   price_paid     50 non-null     float64\ndtypes: float64(3)\nmemory usage: 1.3 KB\n```\n:::\n:::\n\n\n## Describe\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nelmhurst.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family_income</th>\n      <th>gift_aid</th>\n      <th>price_paid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50.000000</td>\n      <td>50.000000</td>\n      <td>50.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>101.778520</td>\n      <td>19.935560</td>\n      <td>19.544440</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>63.206451</td>\n      <td>5.460581</td>\n      <td>5.979759</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>7.000000</td>\n      <td>8.530000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>64.079000</td>\n      <td>16.250000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>88.061500</td>\n      <td>20.470000</td>\n      <td>19.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>137.174000</td>\n      <td>23.515000</td>\n      <td>23.630000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>271.974000</td>\n      <td>32.720000</td>\n      <td>35.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Plot\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.scatterplot(data = elmhurst, x = \"family_income\", y = \"gift_aid\")\nplt.xlabel(\"Family income ($)\")\nplt.ylabel(\"Gift aid from university ($)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-8-output-1.png){width=813 height=437}\n:::\n:::\n\n\n:::\n\n## OLS regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-line-numbers=\"|1,2|4|5|7|9\"}\nX = elmhurst['family_income']  # Independent variable\ny = elmhurst['gift_aid']  # Dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nX_train_with_const = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train, X_train_with_const).fit()\n\nmodel_summary2 = model.summary2()\nprint(model_summary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Results: Ordinary least squares\n=================================================================\nModel:              OLS              Adj. R-squared:     0.205   \nDependent Variable: gift_aid         AIC:                241.3545\nDate:               2024-02-21 13:03 BIC:                244.7323\nNo. Observations:   40               Log-Likelihood:     -118.68 \nDf Model:           1                F-statistic:        11.05   \nDf Residuals:       38               Prob (F-statistic): 0.00197 \nR-squared:          0.225            Scale:              23.273  \n-----------------------------------------------------------------\n                   Coef.  Std.Err.    t    P>|t|   [0.025  0.975]\n-----------------------------------------------------------------\nconst             24.5170   1.4489 16.9216 0.0000 21.5839 27.4500\nfamily_income     -0.0398   0.0120 -3.3238 0.0020 -0.0640 -0.0156\n-----------------------------------------------------------------\nOmnibus:              0.057        Durbin-Watson:           2.277\nProb(Omnibus):        0.972        Jarque-Bera (JB):        0.249\nSkew:                 0.047        Prob(JB):                0.883\nKurtosis:             2.625        Condition No.:           230  \n=================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-line-numbers=\"|1|2|3|5-9\"}\nX_test_with_const = sm.add_constant(X_test)\npredictions = model.predict(X_test_with_const)\nresiduals = y_test - predictions\n\nsns.residplot(x = predictions, y = residuals)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-10-output-1.png){width=812 height=455}\n:::\n:::\n\n\n## Regression fit plot\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-line-numbers=\"|1|3,4|5|7-11\"}\nplt.scatter(X_test, y_test, label = 'Data')\n\nline_x = np.linspace(X_test.min(), X_test.max(), 100)\nline_y = model.predict(sm.add_constant(line_x))\nplt.plot(line_x, line_y, color = 'red', label = 'OLS Regression Line')\n\nplt.xlabel(\"Family income ($)\")\nplt.ylabel(\"Gift aid from university ($)\")\nplt.title('OLS Regression Fit')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-11-output-1.png){width=826 height=456}\n:::\n:::\n\n\n:::\n\n# Multiple regression\n\n## Multiple regression\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-12-output-1.png){width=497 height=520}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-13-output-1.png){width=464 height=483}\n:::\n:::\n\n\n:::\n:::\n\n## Key points\n\n::: incremental\n1.  **Multiple Predictors**: more than one predictor variable to predict a response variable.\n\n2.  **Model Form**: $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\epsilon$ , where $Y$ is the response variable, $X_1, X_2,...,X_n$ are predictor variables, $\\beta$s are coefficients, and $\\epsilon$ is the error term.\n\n3.  **Coefficient Interpretation**: Each coefficient represents the change in the response variable for one unit change in the predictor, holding other predictors constant.\n\n4.  **Assumptions**: Includes linearity, no perfect multicollinearity, homoscedasticity, independence of errors, and normality of residuals.\n\n5.  **Adjusted R-squared**: Used to determine the model's explanatory power, adjusting for the number of predictors.\n\n6.  **Multicollinearity Concerns**: High correlation between predictor variables can distort the model and make coefficient estimates unreliable.\n\n7.  **Interaction Effects**: Can be included to see if the effect of one predictor on the response variable depends on another predictor.\n:::\n:::\n\n## Our data {.smaller}\n\n::: panel-tabset\n## Question\n\nHow do factors like debt-to-income ratio, bankruptcy history, and loan term affect the interest rate of a loan?\n\n![](images/credit-risk.jpeg){fig-align=\"center\" width=\"1000\"}\n\n## Read + examine\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nloans = pd.read_csv(\"data/loans.csv\")\nloans.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emp_title</th>\n      <th>emp_length</th>\n      <th>state</th>\n      <th>homeownership</th>\n      <th>annual_income</th>\n      <th>verified_income</th>\n      <th>debt_to_income</th>\n      <th>annual_income_joint</th>\n      <th>verification_income_joint</th>\n      <th>debt_to_income_joint</th>\n      <th>...</th>\n      <th>sub_grade</th>\n      <th>issue_month</th>\n      <th>loan_status</th>\n      <th>initial_listing_status</th>\n      <th>disbursement_method</th>\n      <th>balance</th>\n      <th>paid_total</th>\n      <th>paid_principal</th>\n      <th>paid_interest</th>\n      <th>paid_late_fees</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>global config engineer</td>\n      <td>3.0</td>\n      <td>NJ</td>\n      <td>MORTGAGE</td>\n      <td>90000.0</td>\n      <td>Verified</td>\n      <td>18.01</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>C3</td>\n      <td>Mar-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>27015.86</td>\n      <td>1999.33</td>\n      <td>984.14</td>\n      <td>1015.19</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>warehouse office clerk</td>\n      <td>10.0</td>\n      <td>HI</td>\n      <td>RENT</td>\n      <td>40000.0</td>\n      <td>Not Verified</td>\n      <td>5.04</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>C1</td>\n      <td>Feb-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>4651.37</td>\n      <td>499.12</td>\n      <td>348.63</td>\n      <td>150.49</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>assembly</td>\n      <td>3.0</td>\n      <td>WI</td>\n      <td>RENT</td>\n      <td>40000.0</td>\n      <td>Source Verified</td>\n      <td>21.15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>D1</td>\n      <td>Feb-2018</td>\n      <td>Current</td>\n      <td>fractional</td>\n      <td>Cash</td>\n      <td>1824.63</td>\n      <td>281.80</td>\n      <td>175.37</td>\n      <td>106.43</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>customer service</td>\n      <td>1.0</td>\n      <td>PA</td>\n      <td>RENT</td>\n      <td>30000.0</td>\n      <td>Not Verified</td>\n      <td>10.16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>A3</td>\n      <td>Jan-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>18853.26</td>\n      <td>3312.89</td>\n      <td>2746.74</td>\n      <td>566.15</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>security supervisor</td>\n      <td>10.0</td>\n      <td>CA</td>\n      <td>RENT</td>\n      <td>35000.0</td>\n      <td>Verified</td>\n      <td>57.96</td>\n      <td>57000.0</td>\n      <td>Verified</td>\n      <td>37.66</td>\n      <td>...</td>\n      <td>C3</td>\n      <td>Mar-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>21430.15</td>\n      <td>2324.65</td>\n      <td>1569.85</td>\n      <td>754.80</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 55 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Relevant variables\n\n| **Variable**      | **Description**                                                                                                                                                                    |\n|:-------------------------|:---------------------------------------------|\n| `interest_rate`   | Interest rate on the loan, in an annual percentage.                                                                                                                                |\n| `verified_income` | Borrower's income verification: `Verified`, `Source Verified`, and `Not Verified`.                                                                                                 |\n| `debt_to_income`  | Debt-to-income ratio, which is the percentage of total debt of the borrower divided by their total income.                                                                         |\n| `credit_util`     | The fraction of available credit utilized.                                                                                                                                         |\n| `bankruptcy`      | An indicator variable for whether the borrower has a past bankruptcy in their record. This variable takes a value of `1` if the answer is **yes** and `0` if the answer is **no**. |\n| `term`            | The length of the loan, in months.                                                                                                                                                 |\n| `issue_month`     | The month and year the loan was issued.                                                                                                                                            |\n| `credit_checks`   | Number of credit checks in the last 12 months.                                                                                                                                     |\n\n## Info\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nloans.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 55 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   emp_title                         9167 non-null   object \n 1   emp_length                        9183 non-null   float64\n 2   state                             10000 non-null  object \n 3   homeownership                     10000 non-null  object \n 4   annual_income                     10000 non-null  float64\n 5   verified_income                   10000 non-null  object \n 6   debt_to_income                    9976 non-null   float64\n 7   annual_income_joint               1495 non-null   float64\n 8   verification_income_joint         1455 non-null   object \n 9   debt_to_income_joint              1495 non-null   float64\n 10  delinq_2y                         10000 non-null  int64  \n 11  months_since_last_delinq          4342 non-null   float64\n 12  earliest_credit_line              10000 non-null  int64  \n 13  inquiries_last_12m                10000 non-null  int64  \n 14  total_credit_lines                10000 non-null  int64  \n 15  open_credit_lines                 10000 non-null  int64  \n 16  total_credit_limit                10000 non-null  int64  \n 17  total_credit_utilized             10000 non-null  int64  \n 18  num_collections_last_12m          10000 non-null  int64  \n 19  num_historical_failed_to_pay      10000 non-null  int64  \n 20  months_since_90d_late             2285 non-null   float64\n 21  current_accounts_delinq           10000 non-null  int64  \n 22  total_collection_amount_ever      10000 non-null  int64  \n 23  current_installment_accounts      10000 non-null  int64  \n 24  accounts_opened_24m               10000 non-null  int64  \n 25  months_since_last_credit_inquiry  8729 non-null   float64\n 26  num_satisfactory_accounts         10000 non-null  int64  \n 27  num_accounts_120d_past_due        9682 non-null   float64\n 28  num_accounts_30d_past_due         10000 non-null  int64  \n 29  num_active_debit_accounts         10000 non-null  int64  \n 30  total_debit_limit                 10000 non-null  int64  \n 31  num_total_cc_accounts             10000 non-null  int64  \n 32  num_open_cc_accounts              10000 non-null  int64  \n 33  num_cc_carrying_balance           10000 non-null  int64  \n 34  num_mort_accounts                 10000 non-null  int64  \n 35  account_never_delinq_percent      10000 non-null  float64\n 36  tax_liens                         10000 non-null  int64  \n 37  public_record_bankrupt            10000 non-null  int64  \n 38  loan_purpose                      10000 non-null  object \n 39  application_type                  10000 non-null  object \n 40  loan_amount                       10000 non-null  int64  \n 41  term                              10000 non-null  int64  \n 42  interest_rate                     10000 non-null  float64\n 43  installment                       10000 non-null  float64\n 44  grade                             10000 non-null  object \n 45  sub_grade                         10000 non-null  object \n 46  issue_month                       10000 non-null  object \n 47  loan_status                       10000 non-null  object \n 48  initial_listing_status            10000 non-null  object \n 49  disbursement_method               10000 non-null  object \n 50  balance                           10000 non-null  float64\n 51  paid_total                        10000 non-null  float64\n 52  paid_principal                    10000 non-null  float64\n 53  paid_interest                     10000 non-null  float64\n 54  paid_late_fees                    10000 non-null  float64\ndtypes: float64(17), int64(25), object(13)\nmemory usage: 4.2+ MB\n```\n:::\n:::\n\n\n## Describe\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nloans.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emp_length</th>\n      <th>annual_income</th>\n      <th>debt_to_income</th>\n      <th>annual_income_joint</th>\n      <th>debt_to_income_joint</th>\n      <th>delinq_2y</th>\n      <th>months_since_last_delinq</th>\n      <th>earliest_credit_line</th>\n      <th>inquiries_last_12m</th>\n      <th>total_credit_lines</th>\n      <th>...</th>\n      <th>public_record_bankrupt</th>\n      <th>loan_amount</th>\n      <th>term</th>\n      <th>interest_rate</th>\n      <th>installment</th>\n      <th>balance</th>\n      <th>paid_total</th>\n      <th>paid_principal</th>\n      <th>paid_interest</th>\n      <th>paid_late_fees</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9183.000000</td>\n      <td>1.000000e+04</td>\n      <td>9976.000000</td>\n      <td>1.495000e+03</td>\n      <td>1495.000000</td>\n      <td>10000.00000</td>\n      <td>4342.000000</td>\n      <td>10000.00000</td>\n      <td>10000.00000</td>\n      <td>10000.000000</td>\n      <td>...</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.930306</td>\n      <td>7.922215e+04</td>\n      <td>19.308192</td>\n      <td>1.279146e+05</td>\n      <td>19.979304</td>\n      <td>0.21600</td>\n      <td>36.760709</td>\n      <td>2001.29000</td>\n      <td>1.95820</td>\n      <td>22.679600</td>\n      <td>...</td>\n      <td>0.123800</td>\n      <td>16361.922500</td>\n      <td>43.272000</td>\n      <td>12.427524</td>\n      <td>476.205323</td>\n      <td>14458.916610</td>\n      <td>2494.234773</td>\n      <td>1894.448466</td>\n      <td>599.666781</td>\n      <td>0.119516</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.703734</td>\n      <td>6.473429e+04</td>\n      <td>15.004851</td>\n      <td>7.016838e+04</td>\n      <td>8.054781</td>\n      <td>0.68366</td>\n      <td>21.634939</td>\n      <td>7.79551</td>\n      <td>2.38013</td>\n      <td>11.885439</td>\n      <td>...</td>\n      <td>0.337172</td>\n      <td>10301.956759</td>\n      <td>11.029877</td>\n      <td>5.001105</td>\n      <td>294.851627</td>\n      <td>9964.561865</td>\n      <td>3958.230365</td>\n      <td>3884.407175</td>\n      <td>517.328062</td>\n      <td>1.813468</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>1.920000e+04</td>\n      <td>0.320000</td>\n      <td>0.00000</td>\n      <td>1.000000</td>\n      <td>1963.00000</td>\n      <td>0.00000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1000.000000</td>\n      <td>36.000000</td>\n      <td>5.310000</td>\n      <td>30.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>4.500000e+04</td>\n      <td>11.057500</td>\n      <td>8.683350e+04</td>\n      <td>14.160000</td>\n      <td>0.00000</td>\n      <td>19.000000</td>\n      <td>1997.00000</td>\n      <td>0.00000</td>\n      <td>14.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>8000.000000</td>\n      <td>36.000000</td>\n      <td>9.430000</td>\n      <td>256.040000</td>\n      <td>6679.065000</td>\n      <td>928.700000</td>\n      <td>587.100000</td>\n      <td>221.757500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.000000</td>\n      <td>6.500000e+04</td>\n      <td>17.570000</td>\n      <td>1.130000e+05</td>\n      <td>19.720000</td>\n      <td>0.00000</td>\n      <td>34.000000</td>\n      <td>2003.00000</td>\n      <td>1.00000</td>\n      <td>21.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>14500.000000</td>\n      <td>36.000000</td>\n      <td>11.980000</td>\n      <td>398.420000</td>\n      <td>12379.495000</td>\n      <td>1563.300000</td>\n      <td>984.990000</td>\n      <td>446.140000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>10.000000</td>\n      <td>9.500000e+04</td>\n      <td>25.002500</td>\n      <td>1.515455e+05</td>\n      <td>25.500000</td>\n      <td>0.00000</td>\n      <td>53.000000</td>\n      <td>2006.00000</td>\n      <td>3.00000</td>\n      <td>29.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>24000.000000</td>\n      <td>60.000000</td>\n      <td>15.050000</td>\n      <td>644.690000</td>\n      <td>20690.182500</td>\n      <td>2616.005000</td>\n      <td>1694.555000</td>\n      <td>825.420000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10.000000</td>\n      <td>2.300000e+06</td>\n      <td>469.090000</td>\n      <td>1.100000e+06</td>\n      <td>39.980000</td>\n      <td>13.00000</td>\n      <td>118.000000</td>\n      <td>2015.00000</td>\n      <td>29.00000</td>\n      <td>87.000000</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>40000.000000</td>\n      <td>60.000000</td>\n      <td>30.940000</td>\n      <td>1566.590000</td>\n      <td>40000.000000</td>\n      <td>41630.443684</td>\n      <td>40000.000000</td>\n      <td>4216.440000</td>\n      <td>52.980000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 42 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Plot\n\n::: {.cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\nsns.scatterplot(data = loans, x = \"debt_to_income\", y = \"interest_rate\", hue = \"loan_purpose\")\nplt.xlabel(\"Annual income ($)\")\nplt.ylabel(\"Loan interest rate (%)\")\nplt.legend(title = \"Loan purpose\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-17-output-1.png){width=812 height=437}\n:::\n:::\n\n\n:::\n\n## Our data: preprocessed\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nloans['credit_util'] = loans['total_credit_utilized'] / loans['total_credit_limit']\nloans['bankruptcy'] = (loans['public_record_bankrupt'] != 0).astype(int)\nloans['verified_income'] = loans['verified_income'].astype('category').cat.remove_unused_categories()\nloans = loans.rename(columns = {'inquiries_last_12m': 'credit_checks'})\n\nloans = loans[['interest_rate', 'verified_income', 'debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'issue_month']]\n```\n:::\n\n\n## Multiple regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nX = loans[['verified_income', 'debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'issue_month']]  \ny = loans['interest_rate']  \nX = pd.get_dummies(X, columns = ['verified_income', 'issue_month'], drop_first = True)\nX.fillna(0, inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX_train_with_const = sm.add_constant(X_train)\nX_test_with_const = sm.add_constant(X_test)\n\nX_train_with_const = X_train_with_const.astype(int)\nX_test_with_const = X_test_with_const.astype(int)\n\nmodel = sm.OLS(y_train, X_train_with_const).fit()\n\nmodel_summary2 = model.summary2()\nprint(model_summary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Results: Ordinary least squares\n==============================================================================\nModel:                   OLS                 Adj. R-squared:        0.191     \nDependent Variable:      interest_rate       AIC:                   46653.7930\nDate:                    2024-02-21 13:03    BIC:                   46723.6650\nNo. Observations:        8000                Log-Likelihood:        -23317.   \nDf Model:                9                   F-statistic:           210.4     \nDf Residuals:            7990                Prob (F-statistic):    0.00      \nR-squared:               0.192               Scale:                 19.937    \n------------------------------------------------------------------------------\n                                 Coef.  Std.Err.    t    P>|t|   [0.025 0.975]\n------------------------------------------------------------------------------\nconst                            4.0713   0.2319 17.5583 0.0000  3.6167 4.5258\ndebt_to_income                   0.0330   0.0033  9.8715 0.0000  0.0265 0.0396\ncredit_util                      3.0409   0.4077  7.4585 0.0000  2.2417 3.8401\nbankruptcy                       0.5811   0.1543  3.7659 0.0002  0.2786 0.8836\nterm                             0.1445   0.0046 31.5795 0.0000  0.1356 0.1535\ncredit_checks                    0.2122   0.0211 10.0534 0.0000  0.1708 0.2535\nverified_income_Source Verified  1.0041   0.1150  8.7304 0.0000  0.7787 1.2296\nverified_income_Verified         2.4460   0.1355 18.0457 0.0000  2.1803 2.7117\nissue_month_Jan-2018            -0.0778   0.1254 -0.6204 0.5350 -0.3236 0.1680\nissue_month_Mar-2018            -0.0178   0.1233 -0.1445 0.8851 -0.2596 0.2239\n------------------------------------------------------------------------------\nOmnibus:                  762.745          Durbin-Watson:             1.958   \nProb(Omnibus):            0.000            Jarque-Bera (JB):          1001.147\nSkew:                     0.820            Prob(JB):                  0.000   \nKurtosis:                 3.562            Condition No.:             398     \n==============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```\n:::\n:::\n\n\n## Coefficients\n\n\n```{=tex}\n\\begin{aligned}\n\\widehat{\\texttt{interest\\_rate}} &= b_0 \\\\\n&+ b_1 \\times \\texttt{verified\\_income}_{\\texttt{Source Verified}} \\\\\n&+ b_2 \\times \\texttt{verified\\_income}_{\\texttt{Verified}} \\\\\n&+ b_3 \\times \\texttt{debt\\_to\\_income} \\\\\n&+ b_4 \\times \\texttt{credit\\_util} \\\\\n&+ b_5 \\times \\texttt{bankruptcy} \\\\\n&+ b_6 \\times \\texttt{term} \\\\\n&+ b_9 \\times \\texttt{credit\\_checks} \\\\\n&+ b_7 \\times \\texttt{issue\\_month}_{\\texttt{Jan-2018}} \\\\\n&+ b_8 \\times \\texttt{issue\\_month}_{\\texttt{Mar-2018}}\n\\end{aligned}\n```\n\n## Residual plot\n\n::: {.cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nX_test_with_const = sm.add_constant(X_test)\npredictions = model.predict(X_test_with_const)\npredictions = pd.to_numeric(predictions, errors='coerce')\nresiduals = y_test - predictions\nresiduals = pd.to_numeric(residuals, errors='coerce').fillna(0)\n\n# Plotting the residuals\nsns.residplot(x = predictions, y = residuals)\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-20-output-1.png){width=821 height=455}\n:::\n:::\n\n\n## Regression fit\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nplt.scatter(predictions, y_test, label = 'Data')\nplt.plot(y_test, y_test, color = 'red', label = 'Ideal Fit')  \n\nplt.xlabel('Predicted Interest Rate')\nplt.ylabel('Actual Interest Rate')\nplt.title('Multiple Regression Fit')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-21-output-1.png){width=812 height=455}\n:::\n:::\n\n\n:::\n\n## Model optimization {.smaller}\n\n::: panel-tabset\n## Overview\n\n**Three major methods**:\n\n::: incremental\n1.  **AIC (Akaike Information Criterion)**: a measure of the relative quality of a statistical model for a given set of data. **Lower** and **fewer parameters = better**.\n    -   $AIC=2k−2\\ln(L)$, where $k$ is the number of parameters, and $L$ is the likelihood of the model.\n2.  **BIC (Bayesian Information Criterion):** a criterion for model selection based on the likelihood of the model. **Lower** and [**fewer parameters**]{.underline} **= better**.\n    -   $BIC = k \\ln(n) - 2 \\ln(L)$, where $k$ is the number of parameters, $n$ is the sample size, and $L$ is the likelihood of the model\n3.  **Cross-Validation**: a statistical method used for assessing how the results of a predictive model will generalize to an independent data set.\n    -   Most common method is **k-Fold Cross Validation**\n:::\n\n## AIC\n\n**Pros:**\n\n::: incremental\n1.  **Balanced Approach**: AIC balances model fit with complexity, reducing overfitting.\n\n2.  **Comparative Utility**: Suitable for comparing models on the same dataset.\n\n3.  **Widespread Acceptance**: Commonly used and recognized in statistical analysis.\n\n4.  **Versatile**: Applicable across various types of statistical models.\n:::\n\n**Cons:**\n\n::: incremental\n1.  **Relative Measurement**: Only provides a comparative metric and doesn't indicate absolute model quality.\n\n2.  **Risk of Underfitting**: Penalizing complex models may lead to underfitting.\n\n3.  **Sample Size Sensitivity**: Its effectiveness can vary with the size of the dataset.\n\n4.  **Assumption of Large Sample**: Infinite sample size assumption is impractical.\n\n5.  **No Probabilistic Interpretation**: Lacks a direct probabilistic meaning, unlike some other statistical measures.\n:::\n\n## BIC\n\n**Pros**:\n\n::: incremental\n1.  **Penalizes Complexity**: Penalizes complexity, preventing overfitting.\n\n2.  **Good for Large Data**: The penalty term is more significant in larger datasets.\n\n3.  **Model Comparison**: Useful for comparing different models on the same dataset.\n\n4.  **Widely Recognized**: Commonly used in statistical model selection.\n\n**Cons**:\n\n1.  **Relative, Not Absolute**: Only provides a comparative metric and doesn't indicate absolute model quality.\n\n2.  **Can Favor Simpler Models**: Penalizing complex models may lead to underfitting.\n\n3.  **Sample Size Dependent**: The effectiveness of BIC is influenced by sample size.\n\n4.  **Assumes Correct Model is in Set**: Assumes that the true model is given.\n\n5.  **Lacks Probabilistic Meaning**: Lacks a direct probabilistic meaning, unlike some other statistical measures.\n:::\n\n## k-Fold CV\n\n**Pros**:\n\n::: incremental\n1.  **Robustness**: Provides a more accurate measure of out-of-sample accuracy.\n\n2.  **Prevents Overfitting**: Helps in detecting overfitting by evaluating model performance on unseen data.\n\n3.  **Versatile**: Can be used with any predictive modeling technique.\n\n4.  **Tuning Hyperparameters**: Useful for selecting the best model hyperparameters.\n\n**Cons**:\n\n1.  **Computationally Intensive**: Can be slow with large datasets and complex models.\n\n2.  **Data Requirements**: Requires a sufficient amount of data to partition into meaningful subsets.\n\n3.  **Variance**: Results can be sensitive to the way in which data is divided.\n\n4.  **No Single Best Model**: only provides an estimation of how well a model type will perform.\n:::\n:::\n\n# Model selection\n\n## Model selection {.smaller}\n\n> The task of selecting a model from among various candidates on the basis of performance criterion to choose the best one.\n\n#### Two major methods:\n\n::: incremental\n1.  **Best subset selection**: Evaluate all combinations, choose by a criterion - e.g., lowest $RSS$ or highest $R^{2}_{adj}$\n2.  **Stepwise selection**: **Forward Selection** or **Backward Selection**\n:::\n\n## Best subset selection {.smaller}\n\n::: panel-tabset\n## Overview\n\n**Definition**\n\nBest Subset Selection is a statistical method used in regression analysis to select a subset of predictors that provides the best fit for the response variable.\n\n**It involves**:\n\n::: incremental\n1.  **Considering All Possible Predictor Subsets**: For $p$ predictors, all possible combinations (totaling $2_p$) of these predictors are considered.\n\n2.  **Fitting a Model for Each Subset**: A regression model is fitted for each subset of predictors.\n\n3.  **Selecting the Best Model**: The best model is selected based on a criterion that balances fit and complexity, such as the lowest Residual Sum of Squares ($RSS$) or the highest $R^{2}_{adj}$ (or $AIC$, $BIC$).\n:::\n\n## Formula\n\n**For each subset, the model is given by**:\n\n$Y = \\beta_0 + \\sum_{i \\in S} \\beta_i X_i + \\epsilon$\n\n::: incremental\n-   $Y$: Response variable.\n\n-   $\\beta_0$: Intercept.\n\n-   $\\beta_i$: Coefficients for predictors.\n\n-   $X_i$: Predictor variables.\n\n-   $\\epsilon$: Error term.\n\n-   $S$: Set of indices of selected predictors.\n:::\n\nThe quality of each model is assessed using a criterion like:\n\n$RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n\n...or $R^{2}_{adj}$,$AIC$,$BIC$\n\n## Pros + cons\n\n**Pros**:\n\n::: incremental\n1.  **Comprehensive Approach**: Evaluates all possible combinations of predictors, ensuring a thorough search.\n\n2.  **Flexibility**: Can be used with various selection criteria and types of regression models.\n\n3.  **Intuitive**: Provides a clear framework for model selection.\n:::\n\n**Cons**:\n\n::: incremental\n1.  **Computational Intensity**: The number of models to evaluate grows exponentially with the number of predictors, making it computationally demanding.\n\n2.  **Overfitting Risk**: May lead to overfit models, especially when the number of observations is not significantly larger than the number of predictors.\n\n3.  **Model Selection Complexity**: Requires careful choice and interpretation of model selection criteria to balance between model fit and complexity.\n:::\n:::\n\n## Stepwise selection (both types) {.smaller}\n\n::: panel-tabset\n## Overview\n\n**Definition**\n\nStepwise selection is a method used in regression analysis to select predictor variables. There are two main types:\n\n**Forward Selection**:\n\n::: incremental\n1.  Starts with no predictor variables in the model.\n\n2.  Iteratively adds the variable that provides the most significant model improvement.\n\n3.  Continues until no significant improvement is made by adding more variables.\n:::\n\n**Backward Selection**:\n\n::: incremental\n1.  Begins with all candidate predictor variables.\n\n2.  Iteratively removes the least significant variable (that least worsens the model).\n\n3.  Continues until removing more variables significantly worsens the model.\n:::\n\n## Formula\n\nThe criteria for adding or removing a variable are usually based on statistical tests like the **F-test** or **p-values**:\n\n**Forward selection (adding a variable)**\n\n$F = \\frac{(\\text{RSS}_0 - \\text{RSS}_1) / p_1}{\\text{RSS}_1 / (n - p_0 - 1)}$\n\n::: incremental\n-   $RSS_0$ is the Residual Sum of Squares of the current model.\n-   $RSS_1$ is the Residual Sum of Squares with the additional variable.\n-   $p_1$ is the number of predictors in the new model.\n-   $n$ is the number of observations.\n-   $p_0$ is the number of predictors in the current model.\n:::\n\n**Backward selection (removing a variable)**\n\nSimilar criteria but in reverse, considering the increase in $RSS$ or the **p-values** of the coefficients.\n\n## Pros + cons\n\n**Pros**:\n\n::: incremental\n1.  **Simplicity**: More straightforward and computationally less intensive than Best Subset Selection.\n\n2.  **Flexibility**: Can be adapted to various criteria for entering and removing variables.\n\n3.  **Practical**: Useful with multicollinearity and large sets of potential predictors.\n:::\n\n**Cons**:\n\n::: incremental\n1.  **Arbitrary Choices**: Depends on the order of variables and the specific entry and removal criteria.\n\n2.  **Local Optima**: Might not find the best possible model as it doesn't evaluate all possible combinations.\n\n3.  **Overfitting Risk**: Especially in datasets with many variables relative to the number of observations.\n\n4.  **Statistical Issues**: Stepwise methods can inflate the significance of variables and do not account for the search process in assessing the fit.\n:::\n:::\n\n## Best Subset Selection: applied {.smaller}\n\n::: panel-tabset\n## Original\n\n::: {.cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\nX = loans[['verified_income', 'debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'issue_month']]  \ny = loans['interest_rate']  \nX = pd.get_dummies(X, columns = ['verified_income', 'issue_month'], drop_first = True)\nX.fillna(0, inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX_train_with_const = sm.add_constant(X_train)\nX_test_with_const = sm.add_constant(X_test)\n\nX_train_with_const = X_train_with_const.astype(int)\nX_test_with_const = X_test_with_const.astype(int)\n\nmodel = sm.OLS(y_train, X_train_with_const).fit()\n\nmodel_summary2 = model.summary2()\nprint(model_summary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Results: Ordinary least squares\n==============================================================================\nModel:                   OLS                 Adj. R-squared:        0.191     \nDependent Variable:      interest_rate       AIC:                   46653.7930\nDate:                    2024-02-21 13:03    BIC:                   46723.6650\nNo. Observations:        8000                Log-Likelihood:        -23317.   \nDf Model:                9                   F-statistic:           210.4     \nDf Residuals:            7990                Prob (F-statistic):    0.00      \nR-squared:               0.192               Scale:                 19.937    \n------------------------------------------------------------------------------\n                                 Coef.  Std.Err.    t    P>|t|   [0.025 0.975]\n------------------------------------------------------------------------------\nconst                            4.0713   0.2319 17.5583 0.0000  3.6167 4.5258\ndebt_to_income                   0.0330   0.0033  9.8715 0.0000  0.0265 0.0396\ncredit_util                      3.0409   0.4077  7.4585 0.0000  2.2417 3.8401\nbankruptcy                       0.5811   0.1543  3.7659 0.0002  0.2786 0.8836\nterm                             0.1445   0.0046 31.5795 0.0000  0.1356 0.1535\ncredit_checks                    0.2122   0.0211 10.0534 0.0000  0.1708 0.2535\nverified_income_Source Verified  1.0041   0.1150  8.7304 0.0000  0.7787 1.2296\nverified_income_Verified         2.4460   0.1355 18.0457 0.0000  2.1803 2.7117\nissue_month_Jan-2018            -0.0778   0.1254 -0.6204 0.5350 -0.3236 0.1680\nissue_month_Mar-2018            -0.0178   0.1233 -0.1445 0.8851 -0.2596 0.2239\n------------------------------------------------------------------------------\nOmnibus:                  762.745          Durbin-Watson:             1.958   \nProb(Omnibus):            0.000            Jarque-Bera (JB):          1001.147\nSkew:                     0.820            Prob(JB):                  0.000   \nKurtosis:                 3.562            Condition No.:             398     \n==============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```\n:::\n:::\n\n\n## Implementing BSS\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndef best_subset_selection(X, y):\n    results = []\n    for k in range(1, len(X.columns) + 1):\n        for combo in combinations(X.columns, k):\n            X_subset = sm.add_constant(X[list(combo)].astype(int))\n            model = sm.OLS(y, X_subset).fit()\n            results.append({'model': model, 'predictors': combo})\n    return results\n\nsubset_results = best_subset_selection(X_train, y_train)\n```\n:::\n\n\n## AIC\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nbest_aic = np.inf\nbest_model_1 = None\n\nfor result in subset_results:\n    if result['model'].aic < best_aic:\n        best_aic = result['model'].aic\n        best_model_1 = result\n        \nprint(\"Best Model Predictors:\", best_model_1['predictors'])\nprint(\"Best Model AIC:\", best_aic.round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Model Predictors: ('debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified')\nBest Model AIC: 46650.23\n```\n:::\n:::\n\n\n## BIC\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nbest_bic = np.inf\nbest_model_2 = None\n\nfor result in subset_results:\n    if result['model'].bic < best_bic:\n        best_bic = result['model'].bic\n        best_model_2 = result\n        \nprint(\"Best Model Predictors:\", best_model_2['predictors'])\nprint(\"Best Model BIC:\", best_bic.round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Model Predictors: ('debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified')\nBest Model BIC: 46706.13\n```\n:::\n:::\n\n\n## $R^{2}_{adj}$\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nbest_r_sq_adj = -np.inf  \nbest_model_3 = None\n\nfor result in subset_results:\n    if result['model'].rsquared_adj > best_r_sq_adj:  \n        best_r_sq_adj = result['model'].rsquared_adj\n        best_model_3 = result\n        \nprint(\"Best Model Predictors:\", best_model_3['predictors'])\nprint(\"Best Model Adjusted R-squared:\", best_r_sq_adj.round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Model Predictors: ('debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified')\nBest Model Adjusted R-squared: 0.19\n```\n:::\n:::\n\n\n:::\n\n## Step-wise selection: applied {.smaller}\n\n::: panel-tabset\n## Original\n\n::: {.cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\nX = loans[['verified_income', 'debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'issue_month']]  \ny = loans['interest_rate']  \nX = pd.get_dummies(X, columns = ['verified_income', 'issue_month'], drop_first = True)\nX.fillna(0, inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX_train_with_const = sm.add_constant(X_train)\nX_test_with_const = sm.add_constant(X_test)\n\nX_train_with_const = X_train_with_const.astype(int)\nX_test_with_const = X_test_with_const.astype(int)\n\nmodel = sm.OLS(y_train, X_train_with_const).fit()\n\nmodel_summary2 = model.summary2()\nprint(model_summary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Results: Ordinary least squares\n==============================================================================\nModel:                   OLS                 Adj. R-squared:        0.191     \nDependent Variable:      interest_rate       AIC:                   46653.7930\nDate:                    2024-02-21 13:03    BIC:                   46723.6650\nNo. Observations:        8000                Log-Likelihood:        -23317.   \nDf Model:                9                   F-statistic:           210.4     \nDf Residuals:            7990                Prob (F-statistic):    0.00      \nR-squared:               0.192               Scale:                 19.937    \n------------------------------------------------------------------------------\n                                 Coef.  Std.Err.    t    P>|t|   [0.025 0.975]\n------------------------------------------------------------------------------\nconst                            4.0713   0.2319 17.5583 0.0000  3.6167 4.5258\ndebt_to_income                   0.0330   0.0033  9.8715 0.0000  0.0265 0.0396\ncredit_util                      3.0409   0.4077  7.4585 0.0000  2.2417 3.8401\nbankruptcy                       0.5811   0.1543  3.7659 0.0002  0.2786 0.8836\nterm                             0.1445   0.0046 31.5795 0.0000  0.1356 0.1535\ncredit_checks                    0.2122   0.0211 10.0534 0.0000  0.1708 0.2535\nverified_income_Source Verified  1.0041   0.1150  8.7304 0.0000  0.7787 1.2296\nverified_income_Verified         2.4460   0.1355 18.0457 0.0000  2.1803 2.7117\nissue_month_Jan-2018            -0.0778   0.1254 -0.6204 0.5350 -0.3236 0.1680\nissue_month_Mar-2018            -0.0178   0.1233 -0.1445 0.8851 -0.2596 0.2239\n------------------------------------------------------------------------------\nOmnibus:                  762.745          Durbin-Watson:             1.958   \nProb(Omnibus):            0.000            Jarque-Bera (JB):          1001.147\nSkew:                     0.820            Prob(JB):                  0.000   \nKurtosis:                 3.562            Condition No.:             398     \n==============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```\n:::\n:::\n\n\n## Forward selection\n\n::: {.cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nlr = LinearRegression()\n\n# Initialize SequentialFeatureSelector (SFS) for forward selection\nsfs = SFS(lr, \n          k_features = 'best',  # Select the best number of features based on criterion\n          forward = True,  # Forward selection\n          floating = False,\n          scoring = 'neg_mean_squared_error',  # Using negative MSE as scoring criterion\n          cv = 0)  # No cross-validation\n\n# Fit SFS on the training data\nsfs.fit(X_train, y_train)\n\n# Get the names of the selected features\nselected_features = list(sfs.k_feature_names_)\n\n# Convert the DataFrame to numeric to ensure all features are numeric\nX_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)\nX_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)\n\n# Select the features identified by forward selection\nX_train_selected = X_train[selected_features]\n\n# Fit the final Linear Regression model using selected features\nfinal_model = lr.fit(X_train_selected, y_train)\n\n# For AIC, BIC, and adjusted R-squared, use statsmodels\nX_train_selected_with_const = sm.add_constant(X_train_selected)\n\n# Ensure data type consistency\nX_train_selected_with_const = X_train_selected_with_const.astype(float)\n\n# Fit the OLS model\nmodel_sm = sm.OLS(y_train, X_train_selected_with_const).fit()\n\n# Output the results\nprint(\"Forward Selection Results\")\nprint(\"Selected predictors:\", selected_features)\nprint(\"AIC:\", model_sm.aic.round(3))\nprint(\"BIC:\", model_sm.bic.round(3))\nprint(\"Adjusted R-squared:\", model_sm.rsquared_adj.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nForward Selection Results\nSelected predictors: ['debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified', 'issue_month_Jan-2018', 'issue_month_Mar-2018']\nAIC: 46034.53\nBIC: 46104.402\nAdjusted R-squared: 0.251\n```\n:::\n:::\n\n\n## Backward selection\n\n::: {.cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\n# Initialize the linear regression model\nlr = LinearRegression()\n\n# Initialize SequentialFeatureSelector (SFS) for forward selection\nsfs = SFS(lr, \n          k_features = 'best',  # Select the best number of features based on criterion\n          forward = False,  # Forward selection\n          floating = False,\n          scoring = 'neg_mean_squared_error',  # Using negative MSE as scoring criterion\n          cv = 0)  # No cross-validation\n\n# Fit SFS on the training data\nsfs.fit(X_train, y_train)\n\n# Get the names of the selected features\nselected_features = list(sfs.k_feature_names_)\n\n# Convert the DataFrame to numeric to ensure all features are numeric\nX_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)\nX_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)\n\n# Select the features identified by forward selection\nX_train_selected = X_train[selected_features]\n\n# Fit the final Linear Regression model using selected features\nfinal_model = lr.fit(X_train_selected, y_train)\n\n# For AIC, BIC, and adjusted R-squared, use statsmodels\nX_train_selected_with_const = sm.add_constant(X_train_selected)\n\n# Ensure data type consistency\nX_train_selected_with_const = X_train_selected_with_const.astype(float)\n\n# Fit the OLS model\nmodel_sm = sm.OLS(y_train, X_train_selected_with_const).fit()\n\n# Output the results\nprint(\"Backward Selection Results\")\nprint(\"Selected predictors:\", selected_features)\nprint(\"AIC:\", model_sm.aic.round(3))\nprint(\"BIC:\", model_sm.bic.round(3))\nprint(\"Adjusted R-squared:\", model_sm.rsquared_adj.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBackward Selection Results\nSelected predictors: ['debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified', 'issue_month_Jan-2018', 'issue_month_Mar-2018']\nAIC: 46034.53\nBIC: 46104.402\nAdjusted R-squared: 0.251\n```\n:::\n:::\n\n\n:::\n\n## Cross validation {.smaller}\n\n::: panel-tabset\n## Best Subset Selection\n\n::: {.cell execution_count=29}\n``` {.python .cell-code code-fold=\"true\"}\ndef best_subset_cv(X, y, max_features=5):\n    best_score = np.inf\n    best_subset = None\n    \n    # Limiting the number of features for computational feasibility\n    for k in range(1, max_features + 1):\n        for subset in combinations(X.columns, k):\n            # Define the model\n            model = LinearRegression()\n            \n            # Perform k-fold cross-validation\n            kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n            cv_scores = cross_val_score(model, X[list(subset)], y, cv = kf, scoring = 'neg_mean_squared_error')\n            \n            # Compute the average score\n            score = -np.mean(cv_scores)  # Convert back to positive MSE\n            \n            # Update the best score and subset\n            if score < best_score:\n                best_score = score\n                best_subset = subset\n    \n    return best_subset, best_score\n\n# Start timing\nstart_time = time.time()\n\n# Assuming X and y are already defined and preprocessed\nbest_subset, best_score = best_subset_cv(X, y)\n\n# End timing\nend_time = time.time()\n\n# Calculate duration\nduration = end_time - start_time\n\nprint(\"Best Subset:\", best_subset)\nprint(\"Best CV Score (MSE):\", best_score)\nprint(\"Computation Time: {:.2f} seconds\".format(duration))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Subset: ('credit_util', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified')\nBest CV Score (MSE): 18.664618422927752\nComputation Time: 3.54 seconds\n```\n:::\n:::\n\n\n## Stepwise Selection\n\n::: {.cell execution_count=30}\n``` {.python .cell-code code-fold=\"true\"}\n# Initialize the linear regression model\nlr = LinearRegression()\n\n# Initialize SequentialFeatureSelector for forward selection with cross-validation\nsfs = SFS(lr,\n          k_features = 'best',  # 'best' or specify a number with ('1, 10') for range\n          forward=True,\n          floating=False,\n          scoring = 'neg_mean_squared_error',\n          cv = KFold(n_splits = 5, shuffle = True, random_state = 42))\n\n# Start timing\nstart_time = time.time()\n\n# Fit SFS on the training data\nsfs.fit(X, y)\n\n# End timing\nend_time = time.time()\n\n# Calculate duration\nduration = end_time - start_time\n\n# Get the names of the selected features\nselected_features = list(sfs.k_feature_names_)\n\nprint(\"Selected predictors (forward selection):\", selected_features)\nprint(\"Computation Time: {:.2f} seconds\".format(duration))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected predictors (forward selection): ['debt_to_income', 'credit_util', 'bankruptcy', 'term', 'credit_checks', 'verified_income_Source Verified', 'verified_income_Verified']\nComputation Time: 0.71 seconds\n```\n:::\n:::\n\n\n## Model output\n\n::: {.cell execution_count=31}\n``` {.python .cell-code code-fold=\"true\"}\n# Convert the DataFrame to numeric to ensure all features are numeric\nX_train = X_train.apply(pd.to_numeric, errors = 'coerce').fillna(0)\nX_test = X_test.apply(pd.to_numeric, errors = 'coerce').fillna(0)\n\n# Select the features identified by forward selection\nX_train_selected = X_train[selected_features]\n\n# Fit the final Linear Regression model using selected features\nfinal_model = lr.fit(X_train_selected, y_train)\n\n# For AIC, BIC, and adjusted R-squared, use statsmodels\nX_train_selected_with_const = sm.add_constant(X_train_selected)\n\n# Ensure data type consistency\nX_train_selected_with_const = X_train_selected_with_const.astype(float)\n\n# Fit the OLS model\nmodel_sm = sm.OLS(y_train, X_train_selected_with_const).fit()\n\n# Model results\nmodel_summary2 = model_sm.summary2()\nprint(\"Forward selection best fit:\\n\")\nprint(model_summary2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nForward selection best fit:\n\n                      Results: Ordinary least squares\n============================================================================\nModel:                 OLS                 Adj. R-squared:        0.251     \nDependent Variable:    interest_rate       AIC:                   46030.6466\nDate:                  2024-02-21 13:03    BIC:                   46086.5442\nNo. Observations:      8000                Log-Likelihood:        -23007.   \nDf Model:              7                   F-statistic:           384.2     \nDf Residuals:          7992                Prob (F-statistic):    0.00      \nR-squared:             0.252               Scale:                 18.448    \n----------------------------------------------------------------------------\n                                Coef.  Std.Err.    t    P>|t|  [0.025 0.975]\n----------------------------------------------------------------------------\nconst                           2.1781   0.2216  9.8311 0.0000 1.7438 2.6125\ndebt_to_income                  0.0214   0.0032  6.5925 0.0000 0.0150 0.0277\ncredit_util                     4.7592   0.1794 26.5247 0.0000 4.4075 5.1109\nbankruptcy                      0.4028   0.1486  2.7109 0.0067 0.1115 0.6940\nterm                            0.1493   0.0044 33.8811 0.0000 0.1406 0.1579\ncredit_checks                   0.2238   0.0203 11.0250 0.0000 0.1840 0.2636\nverified_income_Source Verified 0.9396   0.1106  8.4934 0.0000 0.7228 1.1565\nverified_income_Verified        2.5107   0.1304 19.2581 0.0000 2.2551 2.7663\n----------------------------------------------------------------------------\nOmnibus:                908.105          Durbin-Watson:             1.953   \nProb(Omnibus):          0.000            Jarque-Bera (JB):          1279.631\nSkew:                   0.884            Prob(JB):                  0.000   \nKurtosis:               3.843            Condition No.:             242     \n============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```\n:::\n:::\n\n\n:::\n\n## Conclusions\n\n-   OLS regression is a great baseline model to compare against\n\n-   Falls victim to collinearity, overly complex models\n\n    -   Hence selection methods with model assessment\n\n-   Best subset selection can overfit easier than stepwise selection\n\n-   Stepwise selection is faster\n\n-   Stepwise inflates Type I error with multiple testing...\n\n## Live coding: Indoor air pollution\n\n::: panel-tabset\n## Fuel access\n\n| variable          | class     | description                                                          |\n|-----------------|-----------------|--------------------------------------|\n| Entity            | character | Country name                                                         |\n| Code              | character | Country code                                                         |\n| Year              | double    | Year                                                                 |\n| access_clean_perc | double    | Access to clean fuels and technologies for cooking (% of population) |\n\n## Fuel GDP\n\n| variable          | class     | description                                          |\n|------------------|------------------|-------------------------------------|\n| Entity            | character | The country                                          |\n| Code              | character | Country code                                         |\n| Year              | double    | Year                                                 |\n| access_clean_perc | double    | \\% of population with access to clean cooking fuels  |\n| GDP               | double    | GDP per capita, PPP (constant 2017 international \\$) |\n| popn              | double    | Country population                                   |\n| Continent         | character | Continent the country resides on                     |\n\n## Death source\n\n| variable       | class     | description                                                            |\n|-----------------|-----------------|---------------------------------------|\n| Entity         | character | The country                                                            |\n| Code           | character | Country code                                                           |\n| Year           | double    | Year                                                                   |\n| Death_Rate_ASP | double    | Cause of death related to air pollution from solid fuels, standardized |\n:::\n\n## Live coding {.smaller}\n\n::: {.cell execution_count=32}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.impute import SimpleImputer\nfrom feature_engine.imputation import CategoricalImputer\n\n# Read in data\nfuel_access = pd.read_csv('data/fuel_access.csv')\nfuel_gdp = pd.read_csv('data/fuel_gdp.csv')\ndeath_source = pd.read_csv('data/death_source.csv')\n\n# Select relevant columns and rename for consistency if needed\nfuel_access = fuel_access[['Entity', 'Year', 'access_clean_perc']]\nfuel_gdp = fuel_gdp[['Entity', 'Year', 'GDP', 'popn']]\ndeath_source = death_source[['Entity', 'Year', 'Death_Rate_ASP']]\n\n# Ensure 'Year' is an integer\nfuel_access['Year'] = fuel_access['Year'].astype(int)\nfuel_gdp['Year'] = fuel_gdp['Year'].astype(int)\ndeath_source['Year'] = death_source['Year'].astype(int)\n\n# Merge datasets on 'Entity' and 'Year'\nmerged_data = pd.merge(fuel_access, fuel_gdp, on = ['Entity', 'Year'], how = 'inner')\nmerged_data = pd.merge(merged_data, death_source, on = ['Entity', 'Year'], how = 'inner')\n\n# Check for missing values\nprint(merged_data.isnull().sum())\n\n# Handle missing values if necessary (e.g., fill with mean or median, or drop)\nnum_imputer = SimpleImputer(strategy = 'mean')  # or 'median'\nnumerical_cols = merged_data.select_dtypes(include = ['int64', 'float64']).columns.tolist()\nmerged_data[numerical_cols] = num_imputer.fit_transform(merged_data[numerical_cols])\n\nmerged_data.dropna()\nmerged_data['Entity'] = pd.factorize(merged_data['Entity'])[0]\n\n# Final check for data types and missing values\nprint(merged_data.info())\nprint(merged_data.isnull().sum())\n\n# Optional: Standardize/normalize numerical columns if necessary\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnumerical_columns = ['access_clean_perc', 'GDP', 'popn']\nmerged_data[numerical_columns] = scaler.fit_transform(merged_data[numerical_columns])\nmerged_data.to_csv()\n# Define the predictor variables and the response variable\nX = merged_data.drop(['Death_Rate_ASP'], axis = 1)  # Assuming 'Death_Rate_ASP' is the response variable\ny = merged_data['Death_Rate_ASP']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Initialize LinearRegression model\nlr = LinearRegression()\n\n# Initialize SequentialFeatureSelector for forward selection with cross-validation\nsfs = SFS(lr,\n          k_features = 'best',  # 'best' or specify a number with ('1, 10') for range\n          forward = True,  # Forward selection\n          floating = False,  # Set to True for stepwise selection\n          scoring = 'neg_mean_squared_error',  # Using negative MSE as scoring criterion\n          cv = KFold(n_splits = 5, shuffle = True, random_state = 42))  # 5-fold cross-validation\n\n# Fit SFS on the training data\nsfs.fit(X_train, y_train)\n\n# Get the names of the selected features\nselected_features = list(sfs.k_feature_names_)\n\n# Fit the final Linear Regression model using selected features\nfinal_model = lr.fit(X_train[selected_features], y_train)\n\n# Evaluate the model performance on the test set\nfrom sklearn.metrics import mean_squared_error\n\ny_pred = final_model.predict(X_test[selected_features])\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"Selected predictors (forward selection):\", selected_features)\nprint(\"Test MSE:\", mse)\n\n# Extracting predictor variables (X) and the target variable (y)\nX = merged_data[selected_features]\nY = merged_data['Death_Rate_ASP']\n\n# Adding a constant for the intercept term\nX = sm.add_constant(X)\n\n# Fit the regression model\nmodel = sm.OLS(y, X).fit()\n\nmodel_summary = model.summary2()\nprint(model_summary)\n```\n:::\n\n\n",
    "supporting": [
      "07-regn-1_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}