{
  "hash": "14fb7f2cf0a66e799198493279259e82",
  "result": {
    "markdown": "---\ntitle: Model Evaluation\nsubtitle: Lecture 6\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   HW 03 is due today, 11:59pm\n\n-   RQ #3 is due today, 11:59pm\n\n-   Spring break next week!\n\n-   Project 1 Presentations are on March 13th, 1pm\n\n## Setup {.smaller}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code}\n# Import all required libraries\n# Data handling and manipulation\nimport pandas as pd\nimport numpy as np\n\n# Machine learning models\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nimport mord as m\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\n# Model evaluation and validation methods\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import resample\n\n# Metrics for model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\n# Utility for data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import label_binarize\n\n# For advanced visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"white\")\n```\n:::\n\n\n# Regressions\n\n## Linear regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](07-regn-1_files/figure-revealjs/cell-3-output-1.png){width=800 height=415}\n:::\n:::\n\n\n## Definition\n\n**Objective**: Minimize the sum of squared differences between observed and predicted.\n\n**Model structure:**\n\n$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$\n\n::: incremental\n-   $Y_i$: Dependent/response variable\n\n-   $X_i$: Independent/predictor variable\n\n-   $\\beta_0$: y-intercept\n\n-   $\\beta_1$: Slope\n\n-   $\\epsilon_i$: Random error term, deviation of the real value from predicted\n:::\n\n## Key points\n\n::: incremental\n1.  **Assumptions**: Linearity, independent residuals, constant variance (homoscedasticity), and normally distributed residuals.\n\n2.  **Goodness of Fit**: Assessed by $R^2$, the proportion of variance in $Y$ explained by $X$.\n\n3.  **Statistical Significance**: Tested by t-tests on the coefficients.\n\n4.  **Confidence Intervals**: Provide a range for the estimated coefficients.\n\n5.  **Predictions**: Use the model to predict $Y$ for new $X$ values.\n\n6.  **Diagnostics**: Check residuals for model assumption violations.\n\n7.  **Sensitivity**: Influenced by outliers which can skew the model.\n\n8.  **Applications**: Used across various fields for predictive modeling and data analysis.\n:::\n\n## \n:::\n\n## Assumptions {.smaller}\n\n::: columns\n::: {.column .fragment width=\"33.3%\" fragment-index=\"1\"}\n**Linearity**\n\n(Linear relationship between Y \\~ X)\n\n![](images/assumptions1.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"2\"}\n**Homoscedasticity**\n\n(Equal variance among variables)\n\n![](images/assumptions2.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"3\"}\n**Multivartiate Normality**\n\n(Normally distributed residuals)\n\n![](images/assumptions3.png)\n:::\n:::\n\n::: columns\n::: {.column .fragment width=\"33.3%\" fragment-index=\"4\"}\n**Independence**\n\n(Observations are independent)\n\n![](images/assumptions4.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"5\"}\n**Lack of Multicollinearity**\n\n(Predictors are not correlated)\n\n![](images/assumptions5.png)\n:::\n\n::: {.column .fragment width=\"33.3%\" fragment-index=\"6\"}\n**Outlier check**\n\n(Technically not an assumption)\n\n![](images/assumptions6.png)\n:::\n:::\n\n## Ordinary Least Squares (OLS) {.smaller}\n\n::: columns\n::: {.column .fragment width=\"50%\" fragment-index=\"1\"}\n![](images/error-in-machine-learning-ols.webp)\n:::\n\n::: {.column .fragment width=\"50%\" fragment-index=\"2\"}\n![](images/error-ordinary-least-squares-ols.webp)\n:::\n:::\n\n::: {.fragment fragment-index=\"3\"}\n$\\displaystyle y_{i}=\\beta_{1} x{i_1}+\\beta_{2} x{i_2}+\\cdots +\\beta_{p} x{i_p}+\\varepsilon _{i}$\n\n::: incremental\n1.  $y_i$: Dependent variable for the $i$-th observation.\n2.  $\\beta_1, \\beta_2, …, \\beta_p$: Coefficients representing the impact of each independent variable.\n3.  $x_{i1}, ​x_{i1},…, x_{ip}$: Independent variables for the $i$-th observation.\n4.  $\\epsilon_i$​: Error term for the $i$-th observation, indicating unexplained variance.\n:::\n:::\n\n",
    "supporting": [
      "07-regn-1_files"
    ],
    "filters": [],
    "includes": {}
  }
}