{
  "hash": "6eb038e38c4ba5673e054f3dec7ec9cd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Regressions II\nsubtitle: Lecture 8\nauthor: \"{{< var slides.author >}}\"\ninstitute: \"{{< var slides.institute >}}\"\nfooter: \"{{< var slides.footer >}}\"\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   HW 04 is due Apr 03, 11:59pm\n\n-   Final project proposals are due Wed Mar 27, 1pm for peer-review\n\n## Setup {.smaller}\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Import all required libraries\n# Data handling and manipulation\nimport pandas as pd\nimport numpy as np\n\n# Implementing and selecting models\nimport statsmodels.api as sm\nfrom itertools import combinations\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# For advanced visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Show computation time\nimport time\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"white\", palette = \"colorblind\")\n```\n:::\n\n\n# Regressions II\n\n## Our data: Indoor air pollution {.smaller}\n\n::: panel-tabset\n## Read + head\n\n::: {#e2a0ce50 .cell execution_count=2}\n``` {.python .cell-code}\nfrom skimpy import clean_columns\n\npollution = pd.read_csv(\"data/merged_pollution.csv\", encoding = 'iso-8859-1')\npollution = clean_columns(pollution)\n\npollution.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>year</th>\n      <th>access_clean_perc</th>\n      <th>gdp</th>\n      <th>popn</th>\n      <th>death_rate_asp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2000.0</td>\n      <td>-1.371886</td>\n      <td>0.000000</td>\n      <td>-0.104197</td>\n      <td>371.951345</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2001.0</td>\n      <td>-1.353313</td>\n      <td>0.000000</td>\n      <td>-0.102565</td>\n      <td>368.490253</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2002.0</td>\n      <td>-1.330292</td>\n      <td>-0.877632</td>\n      <td>-0.100603</td>\n      <td>355.870851</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2003.0</td>\n      <td>-1.302300</td>\n      <td>-0.875238</td>\n      <td>-0.098471</td>\n      <td>350.188748</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2004.0</td>\n      <td>-1.276925</td>\n      <td>-0.877087</td>\n      <td>-0.096407</td>\n      <td>341.858106</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Metadata\n\n| **variable**      | **class** | **description**                                                        |\n|------------------|------------------|------------------------------------|\n| entity            | character | Country, unique number identifier                                      |\n| year              | double    | Year                                                                   |\n| access_clean_perc | double    | \\% of population with access to clean cooking fuels                    |\n| gdp               | double    | GDP per capita, PPP (constant 2017 international \\$)                   |\n| popn              | character | Country population                                                     |\n| death_rate_asp    | double    | Cause of death related to air pollution from solid fuels, standardized |\n\n## Info\n\n::: {#6ace25a1 .cell execution_count=3}\n``` {.python .cell-code}\npollution.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3264 entries, 0 to 3263\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   entity             3264 non-null   int64  \n 1   year               3264 non-null   float64\n 2   access_clean_perc  3264 non-null   float64\n 3   gdp                3264 non-null   float64\n 4   popn               3264 non-null   float64\n 5   death_rate_asp     3264 non-null   float64\ndtypes: float64(5), int64(1)\nmemory usage: 153.1 KB\n```\n:::\n:::\n\n\n## Describe\n\n::: {#1f97c056 .cell execution_count=4}\n``` {.python .cell-code}\npollution.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>year</th>\n      <th>access_clean_perc</th>\n      <th>gdp</th>\n      <th>popn</th>\n      <th>death_rate_asp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3264.000000</td>\n      <td>3264.00000</td>\n      <td>3.264000e+03</td>\n      <td>3.264000e+03</td>\n      <td>3.264000e+03</td>\n      <td>3264.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>95.500000</td>\n      <td>2008.00000</td>\n      <td>-1.349683e-16</td>\n      <td>8.707632e-18</td>\n      <td>4.353816e-17</td>\n      <td>70.587846</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>55.433366</td>\n      <td>4.89973</td>\n      <td>1.000153e+00</td>\n      <td>1.000153e+00</td>\n      <td>1.000153e+00</td>\n      <td>87.057969</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>2000.00000</td>\n      <td>-1.598171e+00</td>\n      <td>-9.067143e-01</td>\n      <td>-1.451953e-01</td>\n      <td>0.005738</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>47.750000</td>\n      <td>2004.00000</td>\n      <td>-1.064244e+00</td>\n      <td>-7.425174e-01</td>\n      <td>-1.417803e-01</td>\n      <td>1.090309</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>95.500000</td>\n      <td>2008.00000</td>\n      <td>4.424448e-01</td>\n      <td>-3.377230e-01</td>\n      <td>-1.295730e-01</td>\n      <td>23.828597</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>143.250000</td>\n      <td>2012.00000</td>\n      <td>9.444564e-01</td>\n      <td>3.119429e-01</td>\n      <td>-9.508658e-02</td>\n      <td>135.902705</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>191.000000</td>\n      <td>2016.00000</td>\n      <td>1.013911e+00</td>\n      <td>5.036683e+00</td>\n      <td>1.458832e+01</td>\n      <td>474.973060</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Missing values\n\n::: {#6bb7c0a3 .cell execution_count=5}\n``` {.python .cell-code}\npollution.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nentity               0\nyear                 0\naccess_clean_perc    0\ngdp                  0\npopn                 0\ndeath_rate_asp       0\ndtype: int64\n```\n:::\n:::\n\n\n:::\n\n## Multiple regression\n\n::: {#685585e9 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\n# Assuming pollution DataFrame is predefined\nX = pollution[['year', 'access_clean_perc', 'gdp', 'popn']]\ny = pollution['death_rate_asp']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Adding a constant for OLS\nX_train_with_const = sm.add_constant(X_train)\nX_test_with_const = sm.add_constant(X_test)\n\n# Fitting the OLS model\nmodel = sm.OLS(y_train, X_train_with_const).fit()\n\n# Making predictions\ny_pred = model.predict(X_test_with_const)\n\n# Calculating MSE\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\n\n# Extracting Adjusted R-squared from the model's summary\nr2 = r2_score(y_test, y_pred)\nprint(f'R-squared: {r2:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1526.64\nR-squared: 0.8002\n```\n:::\n:::\n\n\n## Regularization\n\n> A technique used in regression to avoid **overfitting** by shrinking the coefficient estimates to 0.\n\n**Two main methods:**\n\n::: incremental\n1.  **Ridge Regression**\n\n2.  **Lasso Regression**\n\n3.  ...but also note cross-validation for hyperparameter tuning\n:::\n\n## Ridge regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"65%\"}\n![](images/ridge-1.png)\n:::\n\n::: {.column width=\"35%\"}\n<br>\n\n![](images/ridge-2.png){width=\"242\"}\n:::\n:::\n\n## Variance inflation factor\n\n::: incremental\n-   VIF quantifies multicollinearity in OLS regressions\n\n-   Assesses how much variation is increased by multicollinearity\n\n-   High VIF indicated that predictor variables can be linearly predicted by each other\n:::\n\n#### Formula\n\n$VIF_j = \\frac{1}{1-R^{2}_j}$\n\n::: incremental\n-   $VIF_j$ is the Variance Inflation Factor for the $j^{th}$ predictor variable.\n\n-   $R^{2}_j$ is the coefficient of determination obtained by regressing the $j^{th}$ predictor variable against all other predictor variables.\n\n-   Ranges from 1-5 (or 10)\n:::\n\n## Formula\n\n$RSS + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n\n::: incremental\n-   Where $j$ ranges from 1 to $p$ and $\\lambda \\geq 0$\n\n-   $\\sum_{j=1}^{p} \\beta_j^2$ is the L2 normalization term\n:::\n\n## Key points\n\n::: incremental\n-   **Penalized Regression**: Adds a penalty to OLS to regularize coefficients, aiding in handling multicollinearity and reducing complexity.\n\n-   **Coefficient Shrinkage**: Coefficients shrink towards zero, enhancing stability and accuracy.\n\n-   **L2 Regularization**: Employs squared coefficient sum as a penalty, regulated by $\\lambda$.\n\n-   **Bias-Variance Trade-off**: Slightly increases bias to reduce variance, preventing overfitting.\n\n-   **Efficient Computation**: Features a closed-form solution, ensuring computational efficiency.\n\n-   **No Feature Elimination**: Maintains all features due to non-zero coefficients, unlike Lasso.\n\n-   **Effective in** $p > n$: Remains effective when predictors outnumber observations.\n\n-   **Interpretability**: Less interpretable because all predictors are included.\n:::\n:::\n\n## Investigate VIF\n\n::: {#f3829597 .cell execution_count=7}\n``` {.python .cell-code}\nVIFs = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nfor idx, vif in enumerate(VIFs):\n    print(f\"VIF for column {X.columns[idx]}: {round(vif, 3)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVIF for column year: 1.0\nVIF for column access_clean_perc: 1.678\nVIF for column gdp: 1.679\nVIF for column popn: 1.001\n```\n:::\n:::\n\n\n::: incremental\n-   `Entity` and `Year` have relatively high VIF\n\n-   Remaining columns relatively low VIF\n:::\n\n## Ridge regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#570d525f .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Add a constant to the model (for statsmodels)\nX_train_const = sm.add_constant(X_train)\nX_test_const = sm.add_constant(X_test)\n\n# Initialize the Ridge Regression model\nridge_reg = Ridge(alpha = 1)  # Alpha is the regularization strength; adjust accordingly\n\n# Fit the model\nridge_reg.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = ridge_reg.predict(X_test)\n\n# Calculate and print the MSE\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {round(mse, 2)}\\n')\n\n# Since Ridge doesn't provide AIC, BIC directly, we focus on what's available\nprint(f'R-squared: {round(r2, 2)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1526.71\n\nR-squared: 0.8\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {#978e22f9 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-10-output-1.png){width=832 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning {.smaller}\n\n> Optimizing the hyperparameters of a machine learning model to enhance its performance. The process aims to find the best combination of hyperparameters that results in the most accurate predictions for a given dataset.\n\n**Key points:**\n\n::: incremental\n-   **Hyperparameters**: Pre-set parameters influencing model behavior, not derived from data.\n\n-   **Search Methods**: Techniques like Grid Search and Random Search to explore hyperparameter spaces.\n\n-   **Cross-Validation**: Essential for assessing model generalizability during tuning.\n\n-   **Performance Metrics**: Criteria like accuracy or MSE to evaluate hyperparameter efficacy.\n\n-   **Computational Cost**: Potentially high, depending on hyperparameter space complexity.\n:::\n\n## Model tuning: Ridge regression {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {#a4b942f5 .cell execution_count=10}\n``` {.python .cell-code}\n# Define a set of alpha values\nalphas = np.logspace(-6, 6, 13)\n\n# Initialize RidgeCV\nridge_cv = RidgeCV(alphas = alphas, store_cv_results = True)\n\n# Fit the model\nridge_cv.fit(X_train, y_train)\n\n# Best alpha value\nprint(f'Best alpha: {ridge_cv.alpha_}')\n\n# Re-initialize and fit the model with the best alpha\nbest_ridge = Ridge(alpha = ridge_cv.alpha_)\nbest_ridge.fit(X_train, y_train)\n\n# Make new predictions\ny_pred_best = best_ridge.predict(X_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest alpha: 0.1\n```\n:::\n:::\n\n\n## Re-evaluate model\n\n::: {#a31419c5 .cell execution_count=11}\n``` {.python .cell-code}\n# Calculate R-squared\nr2_best = r2_score(y_test, y_pred_best)\nprint(f'R-squared with best alpha: {round(r2_best, 4)}')\n\n# Calculate Mean Squared Error (MSE)\nmse_best = mean_squared_error(y_test, y_pred_best)\nprint(f'Mean Squared Error with best alpha: {round(mse_best, 3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared with best alpha: 0.8002\nMean Squared Error with best alpha: 1526.646\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {#7bdb7d88 .cell execution_count=12}\n``` {.python .cell-code}\n# Assuming `y_pred` are the predictions from the initial Ridge model\nmse_initial = mean_squared_error(y_test, y_pred)\nr2_initial = r2_score(y_test, y_pred)\n\n# Print comparison\nprint(f'Initial MSE: {round(mse_initial, 3)}, Best Alpha MSE: {round(mse_best, 3)}')\nprint(f'Initial R-squared: {round(r2_initial, 4)}, Best Alpha R-squared: {round(r2_best, 5)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial MSE: 1526.708, Best Alpha MSE: 1526.646\nInitial R-squared: 0.8002, Best Alpha R-squared: 0.80022\n```\n:::\n:::\n\n\n## Residuals\n\n::: {#9030a461 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_best = y_test - y_pred_best\nplt.scatter(y_pred_best, residuals_best, alpha = 0.5)\nplt.axhline(y = 0, color = 'r', linestyle = '--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot with Best Alpha')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-14-output-1.png){width=829 height=455}\n:::\n:::\n\n\n:::\n\n## Lasso regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"65%\"}\n![](images/lasso-1.png)\n:::\n\n::: {.column width=\"35%\"}\n<br>\n\n![](images/lasso-2.png){width=\"242\"}\n:::\n:::\n\n## Formula\n\n$RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n\n::: incremental\n-   Where $j$ ranges from 1 to $p$ and $\\lambda \\geq 0$\n\n-   $\\sum_{j=1}^{p} |\\beta_j|$ is the L1 normalization term\n:::\n\n## Key points\n\n::: incremental\n-   **Penalized Regression**: Implements OLS with an added L1 penalty on coefficients' absolute values to reduce complexity and tackle multicollinearity.\n\n-   **Feature Selection**: Effectively zeroes out less significant coefficients, offering built-in feature selection for model simplicity.\n\n-   **L1 Regularization**: Uses $\\sum_{j=1}^{p} |\\beta_j|$ as the penalty, with $λ$ tuning the penalty's strength.\n\n-   **Bias-Variance**: Increases bias to lower variance, aiding in overfitting prevention.\n\n-   **Computation**: May require iterative optimization, lacking a closed-form solution, especially in high-dimensional datasets.\n\n-   **Sparse Solutions**: Ideal for models expecting many non-influential features, providing sparsity.\n\n-   **Interpretability**: Enhances model interpretability by retaining only relevant features.\n:::\n:::\n\n## Lasso regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#6c242766 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\n# Prepare the data (assuming X and y are already defined)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Initialize the Lasso Regression model\nlasso_reg = Lasso(alpha = 1.0)  # Alpha is the regularization strength; adjust accordingly\n\n# Fit the model\nlasso_reg.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = lasso_reg.predict(X_test)\n\n# Calculate and print the MSE\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {round(mse, 2)}')\n\n# Since Lasso doesn't provide AIC, BIC directly, we focus on what's available\nr2 = r2_score(y_test, y_pred)\nprint(f'R-squared: {round(r2, 2)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1535.4\nR-squared: 0.8\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {#f50a4e0a .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Lasso Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-16-output-1.png){width=839 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning: Lasso regression {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {#2235d64c .cell execution_count=16}\n``` {.python .cell-code}\n# Define a range of alpha values for Lasso\nalphas = np.logspace(-6, 6, 13)\n\n# Initialize LassoCV\nlasso_cv = LassoCV(alphas = alphas, cv = 5, random_state = 42)\n\n# Fit the model\nlasso_cv.fit(X_train, y_train)\n\n# Optimal alpha value\nprint(f'Optimal alpha: {lasso_cv.alpha_}')\n\n# Re-initialize and fit the model with the optimal alpha\nbest_lasso = Lasso(alpha = lasso_cv.alpha_)\nbest_lasso.fit(X_train, y_train)\n\n# Make new predictions\ny_pred_best = best_lasso.predict(X_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal alpha: 1e-06\n```\n:::\n:::\n\n\n## Re-evaluate model\n\n::: {#7c107ba6 .cell execution_count=17}\n``` {.python .cell-code}\n# Calculate R-squared with the best alpha\nr2_best = r2_score(y_test, y_pred_best)\nprint(f'R-squared with optimal alpha: {round(r2_best, 4)}')\n\n# Calculate Mean Squared Error (MSE) with the best alpha\nmse_best = mean_squared_error(y_test, y_pred_best)\nprint(f'Mean Squared Error with optimal alpha: {round(mse_best, 3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared with optimal alpha: 0.8002\nMean Squared Error with optimal alpha: 1526.64\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {#70b5ff66 .cell execution_count=18}\n``` {.python .cell-code}\n# Assuming `y_pred` are the predictions from the initial Lasso model\nmse_initial = mean_squared_error(y_test, y_pred)\nr2_initial = r2_score(y_test, y_pred)\n\n# Print comparison\nprint(f'Initial MSE: {round(mse_initial, 3)}, Optimal Alpha MSE: {round(mse_best, 3)}')\nprint(f'Initial R-squared: {round(r2_initial, 4)}, Optimal Alpha R-squared: {round(r2_best, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial MSE: 1535.4, Optimal Alpha MSE: 1526.64\nInitial R-squared: 0.7991, Optimal Alpha R-squared: 0.8002\n```\n:::\n:::\n\n\n## Residuals\n\n::: {#28fda654 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_best = y_test - y_pred_best\nplt.scatter(y_pred_best, residuals_best, alpha = 0.5)\nplt.axhline(y = 0, color = 'r', linestyle = '--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot with Optimal Alpha')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-20-output-1.png){width=829 height=455}\n:::\n:::\n\n\n:::\n\n## Elastic net regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"65%\"}\n![](images/elastic-1.png)\n:::\n\n::: {.column width=\"35%\"}\n<br>\n\n![](images/elastic-2.png){width=\"242\"}\n:::\n:::\n\n## Formula\n\n$RSS + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2$\n\n::: incremental\n-   Where $j$ ranges from 1 to $p$ and $\\lambda \\geq 0$\n\n-   $\\sum_{j=1}^{p} |\\beta_j|$ is the L1 normalization term, which encourages sparsity in the coefficients\n\n-   $\\sum_{j=1}^{p} \\beta_j^2$ is the L2 normalization term, which encourages smoothness in the coefficients by penalizing large values.\n:::\n\n## Key points\n\n::: incremental\n-   **Combines L1 and L2 Penalties**: Merges Ridge and Lasso advantages for multicollinearity and feature selection.\n\n-   **Optimizes Feature Selection**: L1 part zeroes out insignificant coefficients; L2 part shrinks coefficients to manage multicollinearity.\n\n-   **Requires Parameter Tuning**: Optimal $\\lambda_1$​ and $\\lambda_2$ balance feature elimination and coefficient reduction.\n\n-   **Mitigates Overfitting**: Adjusts bias-variance trade-off, reducing overfitting risk.\n\n-   **Iterative Optimization**: No closed-form solution due to L1 penalty; relies on optimization methods.\n\n-   **Effective in High Dimensions**: Suitable for datasets with more features than observations.\n\n-   **Balances Sparsity and Stability**: Ensures model relevance and stability through L1 and L2 penalties.\n\n-   **Enhances Interpretability**: Simplifies the model by keeping only relevant predictors, improving model interpretability.\n:::\n:::\n\n## Elastic net regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#61b20f01 .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\n# Assuming X and y are already defined and preprocessed\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Initialize the Elastic Net Regression model with a mix of L1 and L2 regularization\nelastic_net = ElasticNet(alpha = 1.0, l1_ratio = 0.5)  # Adjust alpha and l1_ratio accordingly\n\n# Fit the model\nelastic_net.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = elastic_net.predict(X_test)\n\n# Calculate and print the MSE and R-squared\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {round(mse, 2)}')\nprint(f'R-squared: {round(r2, 2)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 2258.21\nR-squared: 0.7\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {#de9f6848 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Elastic Net Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-22-output-1.png){width=829 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning: Elastic net regression {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {#b9393157 .cell execution_count=22}\n``` {.python .cell-code}\n# Define a range of alpha values and l1_ratios for Elastic Net\nalphas = np.logspace(-6, 6, 13)\nl1_ratios = np.linspace(0.1, 0.9, 9)\n\n# Initialize ElasticNetCV\nelastic_net_cv = ElasticNetCV(alphas = alphas, l1_ratio = l1_ratios, cv = 5, random_state = 42)\n\n# Fit the model to find the optimal alpha and l1_ratio\nelastic_net_cv.fit(X_train, y_train)\n\n# Optimal alpha and l1_ratio\nprint(f'Optimal alpha: {elastic_net_cv.alpha_}')\nprint(f'Optimal l1_ratio: {elastic_net_cv.l1_ratio_}')\n\n# Re-initialize and fit the model with the optimal parameters\nbest_elastic_net = ElasticNet(alpha=elastic_net_cv.alpha_, l1_ratio=elastic_net_cv.l1_ratio_)\nbest_elastic_net.fit(X_train, y_train)\n\n# Make new predictions\ny_pred_best = best_elastic_net.predict(X_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal alpha: 0.0001\nOptimal l1_ratio: 0.1\n```\n:::\n:::\n\n\n## Re-evaluate model\n\n::: {#bd682a33 .cell execution_count=23}\n``` {.python .cell-code}\n# Calculate R-squared with the best parameters\nr2_best = r2_score(y_test, y_pred_best)\nprint(f'R-squared with optimal parameters: {round(r2_best, 4)}')\n\n# Calculate Mean Squared Error (MSE) with the best parameters\nmse_best = mean_squared_error(y_test, y_pred_best)\nprint(f'Mean Squared Error with optimal parameters: {round(mse_best, 3)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared with optimal parameters: 0.8002\nMean Squared Error with optimal parameters: 1526.655\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {#38dc5455 .cell execution_count=24}\n``` {.python .cell-code}\n# Print comparison of MSE and R-squared before and after tuning\nmse_initial = mean_squared_error(y_test, y_pred)\nr2_initial = r2_score(y_test, y_pred)\nprint(f'Initial MSE: {round(mse_initial, 3)}, Optimal Parameters MSE: {round(mse_best, 3)}')\nprint(f'Initial R-squared: {round(r2_initial, 4)}, Optimal Parameters R-squared: {round(r2_best, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial MSE: 2258.214, Optimal Parameters MSE: 1526.655\nInitial R-squared: 0.7045, Optimal Parameters R-squared: 0.8002\n```\n:::\n:::\n\n\n## Residuals\n\n::: {#a29421dd .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_best = y_test - y_pred_best\nplt.scatter(y_pred_best, residuals_best, alpha = 0.5)\nplt.axhline(y = 0, color = 'r', linestyle = '--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot with Optimal Parameters')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-26-output-1.png){width=829 height=455}\n:::\n:::\n\n\n:::\n\n## Summary: regularization {.smaller}\n\n::: incremental\n1.  **Reduces Overfitting**: Regularization (Ridge, Lasso, Elastic Net) constrains model complexity, preventing overfitting by penalizing large coefficients.\n\n2.  **Parameter Optimization via CV**: Cross-validation determines the optimal regularization strength (λ), ensuring model generalizability.\n\n3.  **Feature Selection with Lasso**: Lasso (L1 penalty) zeroes out less significant coefficients, enabling automatic feature selection.\n\n4.  **Ridge Tackles Multicollinearity**: Ridge regression (L2 penalty) minimizes collinearity effects among predictors by evenly shrinking coefficients.\n\n5.  **Elastic Net Merges Penalties**: Elastic Net combines L1 and L2 penalties, ideal for correlated features and [high-dimensional data]{.underline}, with CV tuning for optimal balance.\n\n6.  **CV Enhances Model Accuracy**: Cross-validation across multiple folds improves accuracy and reliability of the chosen regularization parameter.\n\n7.  **Supports Sparse Solutions**: Lasso and Elastic Net support sparse solutions, effectively reducing model complexity by selecting only relevant features.\n\n8.  **Bias-Variance Trade-off**: Regularization adjusts the bias-variance trade-off, slightly increasing bias but significantly reducing variance.\n:::\n\n# High dimensionality\n\n## Dimensional reduction: revisited\n\n> Dimension reduction techniques reduce the number of input variables in a dataset. In regression, these methods can help mitigate issues related to multicollinearity, overfitting, and the curse of dimensionality, particularly in high-dimensional data.\n\n::: fragment\n### Two main methods:\n\n::: incremental\n1.  **Principal components regression (PCR)**\n2.  **Partial least squares (PLS)**\n:::\n:::\n\n## Principal components regression (PCR) {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/pcr-1.png){fig-align=\"center\" width=\"336\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/pcr-3.png){fig-align=\"center\" width=\"342\"}\n:::\n:::\n\n![](images/pcr-2.png){fig-align=\"center\" width=\"785\"}\n\n## Formula\n\n> PCR involves two major steps---Principal Component Analysis (PCA) for dimensionality reduction, followed by regression on these principal components.\n\n::: incremental\n1.  **PCA Transformation**\n\n    $Z=XW$\n\n    Where $X$ is the original dataset, $W$ represents the weight matrix for PCA, and $Z$ contains the principal components (PCs).\n\n2.  **Regression on PCs**\n\n    $\\hat{Y} = ZB + \\epsilon$\n\n    Where $\\hat{Y}$ is the predicted outcome, $Z$ are the PC predictors, $B$ is the coefficient matrix, $\\epsilon$ is the error term\n:::\n\n## Key points\n\n::: incremental\n1.  **Dimensionality Reduction**: Transforms predictors into fewer uncorrelated principal components.\n\n2.  **Overcomes Multicollinearity**: Uses orthogonal components, mitigating multicollinearity in the original predictors.\n\n3.  **Extracts Informative Features**: Captures most data variance through principal components, enhancing model efficiency.\n\n4.  **Challenging Interpretation**: Coefficients relate to principal components, complicating interpretation relative to original predictors.\n\n5.  **Critical Component Selection**: Involves choosing the optimal number of components based on variance explained or cross-validation.\n\n6.  **Requires Standardization**: PCA step necessitates variable scaling to ensure consistent variance across features.\n\n7.  **No Direct Feature Elimination**: While reducing dimensions, PCR retains linear combinations of all features, unlike methods that select individual variables.\n:::\n:::\n\n## Principal components regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#a3380f6b .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\n# Assuming X and y are already defined\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Standardizing the features\nscaler = StandardScaler()\n\n# PCA for dimensionality reduction\npca = PCA()\n\n# Linear Regression\nlin_reg = LinearRegression()\n\n# Creating a pipeline for PCR\npipeline = Pipeline([('scaler', scaler), ('pca', pca), ('lin_reg', lin_reg)])\n\n# Fit the PCR model\npipeline.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Squared Error: {round(mse, 2)}')\nprint(f'R-squared: {round(r2, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1526.64\nR-squared: 0.8002\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {#df08fdda .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for PCR')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-28-output-1.png){width=831 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning: PCR {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {#a27cd2cd .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\n# Function to calculate MSE with cross-validation\ndef cv_mse(n_components):\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()), \n        ('pca', PCA(n_components = n_components)), \n        ('lin_reg', LinearRegression())\n    ])\n    mse = -cross_val_score(pipeline, X_train, y_train, scoring = 'neg_mean_squared_error', cv = 5).mean()\n    return mse\n\n# Testing different numbers of components\ncomponents_range = range(1, min(len(X_train.columns), len(X_train)) + 1)\nmse_scores = [cv_mse(n) for n in components_range]\n\n# Find the optimal number of components\noptimal_components = components_range[np.argmin(mse_scores)]\nprint(f'Optimal number of components: {optimal_components}')\n\n# Re-fit the PCR model with the optimal number of components\npipeline.set_params(pca__n_components = optimal_components)\npipeline.fit(X_train, y_train)\n\n# New predictions and evaluation\ny_pred_opt = pipeline.predict(X_test)\nmse_opt = mean_squared_error(y_test, y_pred_opt)\nr2_opt = r2_score(y_test, y_pred_opt)\n\nprint(f'Optimized Mean Squared Error: {round(mse_opt, 2)}')\nprint(f'Optimized R-squared: {round(r2_opt, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal number of components: 4\nOptimized Mean Squared Error: 1526.64\nOptimized R-squared: 0.8002\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {#732ec712 .cell execution_count=29}\n``` {.python .cell-code}\n# Comparing initial and optimized model performances\nprint(f'Initial vs. Optimized MSE: {round(mse, 2)} vs. {round(mse_opt, 2)}')\nprint(f'Initial vs. Optimized R-squared: {round(r2, 4)} vs. {round(r2_opt, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial vs. Optimized MSE: 1526.64 vs. 1526.64\nInitial vs. Optimized R-squared: 0.8002 vs. 0.8002\n```\n:::\n:::\n\n\n## Residuals\n\n::: {#6db039b7 .cell execution_count=30}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_opt = y_test - y_pred_opt\nsns.residplot(x = y_pred_opt, y = residuals_opt, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Optimized Residual Plot for PCR')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-31-output-1.png){width=831 height=455}\n:::\n:::\n\n\n:::\n\n## Partial least squares regression (PLS) {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/pls-1.png){fig-align=\"center\" width=\"551\"}\n\n## Formula\n\n> PLS focuses on predicting response variables by finding the linear regression model in a transformed space. It differs from PCR by considering the response variable $Y$ when determining the new feature space.\n\n::: incremental\n1.  **PLS Transformation**\n\n    $T=XW^{*}$\n\n    Where $X$ is the original dataset, $W^{*}$ represents the weight matrix for PLS, and $T$ represents the scores (latent variables).\n\n2.  **Regression on PCs**\n\n    $\\hat{Y} = TB + \\epsilon$\n\n    Where $T$ are the PLS latent variables (predictors), $B$ is the coefficient matrix, $\\epsilon$ is the error term\n:::\n\n## Key points\n\n::: incremental\n1.  **Dimensionality Reduction and Prediction**: Reduces predictor dimensions while considering response variable $Y$, optimizing for predictive power.\n\n2.  **Handles Multicollinearity**: By generating latent variables, PLS mitigates multicollinearity, similar to PCR but with a focus on prediction.\n\n3.  **Incorporates Response in Feature Extraction**: Unlike PCR, PLS extracts features based on their covariance with $Y$, enhancing relevant feature capture.\n\n4.  **Component Selection is Crucial**: The number of latent variables (components) is key to model performance, typically determined via cross-validation.\n\n5.  **Standardization May Be Necessary**: Just like PCR, variable scaling can be important depending on the data characteristics.\n\n6.  **Suitable for High-Dimensional Data**: Effective in scenarios where predictors far exceed observations.\n\n7.  **Direct Interpretation Challenging**: Coefficients apply to latent variables, complicating direct interpretation relative to original predictors.\n:::\n:::\n\n## Parial least squares regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#5d5a6a8f .cell execution_count=31}\n``` {.python .cell-code code-fold=\"true\"}\n# Assuming X and y are already defined\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Initialize PLS Regression model\npls = PLSRegression(n_components = 2)  # Adjust n_components as needed\n\n# Fit the model\npls.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = pls.predict(X_test)\n\n# Calculate and print the MSE and R-squared\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred.flatten())  # Flatten if y_pred is 2D\n\nprint(f'Mean Squared Error: {round(mse, 2)}')\nprint(f'R-squared: {round(r2, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 1534.9\nR-squared: 0.7991\n```\n:::\n:::\n\n\n## Residual plot\n\n::: {#7291f3e3 .cell execution_count=32}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred.flatten()\nsns.residplot(x = y_pred.flatten(), y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for PLS Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-33-output-1.png){width=837 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning: PLS {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {#52dd168e .cell execution_count=33}\n``` {.python .cell-code code-fold=\"true\"}\n# Function to perform cross-validation and calculate MSE\ndef cv_mse_pls(n_components):\n    pls = PLSRegression(n_components = n_components)\n    mse = -cross_val_score(pls, X_train, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n    return mse\n\n# Determining the optimal number of components\ncomponents_range = range(1, min(len(X_train.columns), len(X_train)) + 1)\nmse_scores = [cv_mse_pls(n) for n in components_range]\n\n# Optimal number of components\noptimal_components = components_range[np.argmin(mse_scores)]\nprint(f'Optimal number of components: {optimal_components}')\n\n# Re-fit PLS model with the optimal number of components\npls.set_params(n_components = optimal_components)\npls.fit(X_train, y_train)\n\n# New predictions and evaluation\ny_pred_opt = pls.predict(X_test)\nmse_opt = mean_squared_error(y_test, y_pred_opt)\nr2_opt = r2_score(y_test, y_pred_opt.flatten())\n\nprint(f'Optimized Mean Squared Error: {round(mse_opt, 2)}')\nprint(f'Optimized R-squared: {round(r2_opt, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal number of components: 3\nOptimized Mean Squared Error: 1526.57\nOptimized R-squared: 0.8002\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {#6fa1b53c .cell execution_count=34}\n``` {.python .cell-code}\n# Comparing initial and optimized model performances\nprint(f'Initial vs. Optimized MSE: {round(mse, 2)} vs. {round(mse_opt, 2)}')\nprint(f'Initial vs. Optimized R-squared: {round(r2, 4)} vs. {round(r2_opt, 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial vs. Optimized MSE: 1534.9 vs. 1526.57\nInitial vs. Optimized R-squared: 0.7991 vs. 0.8002\n```\n:::\n:::\n\n\n## Residuals\n\n::: {#ea6ed37a .cell execution_count=35}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_opt = y_test - y_pred_opt.flatten()\nsns.residplot(x = y_pred_opt.flatten(), y = residuals_opt, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Optimized Residual Plot for PLS Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-regn-2_files/figure-revealjs/cell-36-output-1.png){width=831 height=455}\n:::\n:::\n\n\n:::\n\n## Considerations in high dimensions {.smaller}\n\n::: incremental\n-   **Overfitting in High-Dimensional Data**: Models may capture noise as signal, leading to overfitting.\n\n-   **Curse of Dimensionality**: Increased dimensionality exponentially expands data space, causing data sparsity and learning difficulties.\n\n-   **Multicollinearity Risks**: More features increase the likelihood of correlated predictors, destabilizing models.\n\n-   **Use of Regularization**: Techniques like Lasso and Ridge effectively address high-dimensional challenges by shrinking coefficients and simplifying models.\n\n-   **Model Simplification**: PCR and PLS reduce variable count, aiding interpretability.\n\n-   **Feature Importance Analysis**: Identifying influential features becomes harder in high dimensions; Lasso's feature selection is beneficial.\n\n-   **Need for Dimensionality Reduction**: Essential for crafting robust, interpretable models in high-dimensional scenarios.\n:::\n\n## Conclusions {.smaller}\n\n::: panel-tabset\n## Table\n\n| Model                                          | MSE         | $R^{2}$ |\n|------------------------------------------------|-------------|---------|\n| **Multiple regression (OLS, best fit)**        | 1526.64     | 0.8002  |\n| Ridge regression (best $\\lambda$)              | 1526.646    | 0.8002  |\n| Lasso regression (best $\\alpha$)               | 1526.64     | 0.8002  |\n| Elastic net (best $\\lambda, \\alpha$)           | 1526.655    | 0.8002  |\n| Principle components (best \\# components)      | 1526.64     | 0.8002  |\n| **Partial least squares (best \\# components)** | **1526.57** | 0.8002  |\n\n::: fragment\n**Occam's Razor loses!**\n:::\n\n## Note, why might best MSE and $R^{2}$ not match?\n\n::: incremental\n-   **Simpler Models**: A model with **fewer predictors** may achieve lower MSE without significantly improving explanation, resulting in lower Adjusted R-squared.\n\n-   **Scale & Variance**: Models on **low-variance data** can show lower MSE but not necessarily higher Adjusted R-squared, reflecting prediction efficiency over variance explanation.\n\n-   **Balance**: **Overfit models** might have lower MSE but lower Adjusted R-squared, indicating poor generalization versus models that balance complexity and predictive power.\n\n-   **Split Variability**: **Train-test split differences** can affect MSE more than Adjusted R-squared, especially with test set anomalies.\n\n-   **Error Patterns**: Models with **evenly distributed errors** tend to have **higher Adjusted R-squared** [even if MSE is slightly higher]{.underline}, compared to those with uneven error distribution.\n:::\n:::\n\n## In-class Exercise \n\n::: task\nGo to [ex-08](https://{{< var website.url >}}/exercises/ex-08.html) and perform the tasks\n:::\n\n",
    "supporting": [
      "08-regn-2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}