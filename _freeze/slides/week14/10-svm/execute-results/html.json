{
  "hash": "3765bd82e1d13d6c720a2b0b662ed4c1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Support Vector Machines (SVMs)\nsubtitle: Lecture 10\nauthor: \"{{< var slides.author >}}\"\ninstitute: \"{{< var slides.institute >}}\"\nfooter: \"{{< var slides.footer >}}\"\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   RQ 04 is due tonight, 11:59pm\n\n-   HW 05 is due Fri Apr 26, 11:59pm\n\n## Setup {.smaller}\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Data Handling and Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n\n# Machine Learning Models\nfrom sklearn.svm import SVC  # Support Vector Classifier for classification tasks\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression  # For polynomial regression\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Additional Tools\nfrom skopt.plots import plot_convergence, plot_objective, plot_evaluations\nfrom sklearn.metrics import accuracy_score, make_scorer\n\n# Set the default style for visualization\nsns.set_theme(style = \"white\", palette = \"colorblind\")\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n```\n:::\n\n\n# A short lesson in Linear Algebra\n\n# Just kidding... too complicated...\n\n::: fragment\nBut see the free [deeplearning.ai](https://deeplearning.ai) course on [Mathematics for Data Science and Machine Learning](https://www.deeplearning.ai/courses/mathematics-for-machine-learning-and-data-science-specialization/).\n:::\n\n# Support Vector Machines (SVMs)\n\n## Question:\n\nDoes mentioning **special skills**, **computer skills**, and the **presence of honors** in a resume [increase the likelihood]{.underline} of **receiving a call-back** for a job interview, and how does this effect [vary]{.underline} by [gender and race]{.underline}?\n\n## Our data: Labor market discrimination {.smaller}\n\n::: panel-tabset\n## Read + head\n\n::: {#ac3a51fe .cell execution_count=2}\n``` {.python .cell-code}\nlaborDiscrim = pd.read_csv(\"data/labor_market_discrimination.csv\", encoding = 'iso-8859-1')[[\"education\", \"special_skills\", \"computer_skills\", \"honors\", \"sex\", \"race\", \"call\"]]\nlaborDiscrim.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>education</th>\n      <th>special_skills</th>\n      <th>computer_skills</th>\n      <th>honors</th>\n      <th>sex</th>\n      <th>race</th>\n      <th>call</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>f</td>\n      <td>w</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>f</td>\n      <td>w</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>f</td>\n      <td>b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>f</td>\n      <td>b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>f</td>\n      <td>w</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Metadata\n\n| **variable**      | **class**     | **description**                                                                                                                          |\n|-------------------|-------------------|----------------------------------|\n| `education`       | int           | Level of education: `0` = not reported; `1` = high school diploma; `2` = high school graduate; `3` = some college; `4` = college or more |\n| `special_skills`  | int / boolean | Whether they mentioned special skills                                                                                                    |\n| `computer_skills` | int / boolean | Whether they mentioned computer skills                                                                                                   |\n| `honors`          | int / boolean | Whether they mentioned some honors                                                                                                       |\n| `sex`             | object        | Applicant's gender                                                                                                                       |\n| `race`            | object        | Applicant's race                                                                                                                         |\n| `call`            | int / boolean | Whether they received a call back                                                                                                        |\n\n## Info\n\n::: {#717bd762 .cell execution_count=3}\n``` {.python .cell-code}\nlaborDiscrim.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4870 entries, 0 to 4869\nData columns (total 7 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   education        4870 non-null   int64 \n 1   special_skills   4870 non-null   int64 \n 2   computer_skills  4870 non-null   int64 \n 3   honors           4870 non-null   int64 \n 4   sex              4870 non-null   object\n 5   race             4870 non-null   object\n 6   call             4870 non-null   int64 \ndtypes: int64(5), object(2)\nmemory usage: 266.5+ KB\n```\n:::\n:::\n\n\n## Categories\n\n::: {#88d85154 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Select categorical columns\ncategorical_cols = laborDiscrim.columns\n\n# Initialize a dictionary to store results\ncategory_analysis = {}\n\n# Loop through each categorical column\nfor col in categorical_cols:\n    counts = laborDiscrim[col].value_counts()\n    proportions = laborDiscrim[col].value_counts(normalize=True)\n    unique_levels = laborDiscrim[col].unique()\n    \n    # Store results in dictionary\n    category_analysis[col] = {\n        'Unique Levels': unique_levels,\n        'Counts': counts,\n        'Proportions': proportions\n    }\n\n# Print results\nfor col, data in category_analysis.items():\n    print(f\"Analysis for {col}:\\n\")\n    print(\"Unique Levels:\", data['Unique Levels'])\n    print(\"\\nCounts:\\n\", data['Counts'])\n    print(\"\\nProportions:\\n\", data['Proportions'])\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis for education:\n\nUnique Levels: [4 3 1 2 0]\n\nCounts:\n education\n4    3504\n3    1006\n2     274\n0      46\n1      40\nName: count, dtype: int64\n\nProportions:\n education\n4    0.719507\n3    0.206571\n2    0.056263\n0    0.009446\n1    0.008214\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for special_skills:\n\nUnique Levels: [0 1]\n\nCounts:\n special_skills\n0    3269\n1    1601\nName: count, dtype: int64\n\nProportions:\n special_skills\n0    0.671253\n1    0.328747\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for computer_skills:\n\nUnique Levels: [1 0]\n\nCounts:\n computer_skills\n1    3996\n0     874\nName: count, dtype: int64\n\nProportions:\n computer_skills\n1    0.820534\n0    0.179466\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for honors:\n\nUnique Levels: [0 1]\n\nCounts:\n honors\n0    4613\n1     257\nName: count, dtype: int64\n\nProportions:\n honors\n0    0.947228\n1    0.052772\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for sex:\n\nUnique Levels: ['f' 'm']\n\nCounts:\n sex\nf    3746\nm    1124\nName: count, dtype: int64\n\nProportions:\n sex\nf    0.769199\nm    0.230801\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for race:\n\nUnique Levels: ['w' 'b']\n\nCounts:\n race\nw    2435\nb    2435\nName: count, dtype: int64\n\nProportions:\n race\nw    0.5\nb    0.5\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for call:\n\nUnique Levels: [0 1]\n\nCounts:\n call\n0    4478\n1     392\nName: count, dtype: int64\n\nProportions:\n call\n0    0.919507\n1    0.080493\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\n```\n:::\n:::\n\n\n## Visualize\n\n::: {#5f5dc223 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](10-svm_files/figure-revealjs/cell-6-output-1.png){width=944 height=457}\n:::\n:::\n\n\n:::\n\n## Support Vector Machines {.smaller}\n\n> SVMs are a set of supervised learning methods used for classification, regression, and outliers detection. The objective of the SVM algorithm is to find a hyperplane in an N-dimensional space (N --- the number of features) that distinctly classifies the data points.\n\n![](images/svm-1.webp){fig-align=\"center\" width=\"450\"}\n\n## Maximum margin classifier {.smaller}\n\n::: incremental\n1.  **Fundamental Concept**: A linear classifier aiming to find a hyperplane that separates classes with the maximum margin, enhancing generalization.\n\n2.  **Hyperplane Definition**: An $n−1$-dimensional decision boundary in $n$-dimensional space, critical for class separation.\n\n3.  **Margin Maximization**: Focuses on creating the widest possible \"street\" between classes, determined by the closest data points, known as support vectors.\n\n4.  **Linear Separability Assumption**: Assumes data can be perfectly divided into two classes with a straight line (or hyperplane in higher dimensions).\n\n5.  **Robustness and Overfitting**: Aims to improve model's robustness to overfitting by maximizing the margin, effective in truly linearly separable and low-noise scenarios.\n\n6.  **Optimization Challenge**: Involves solving a quadratic optimization problem to find the hyperplane, which can be computationally intensive.\n\n7.  **Limitations**: Not directly applicable to non-linearly separable data without modifications, such as using slack variables or kernel methods.\n\n8.  **Evolutionary Significance**: Serves as a foundation for more sophisticated SVM techniques, addressing limitations like non-linear separability and data overlap.\n:::\n\n## Classification using a hyperplane {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/hyperplane.png){fig-align=\"center\" width=\"450\"}\n\n## Formula\n\n> Hyperplanes serve as decision boundaries in SVMs. The goal is to find the hyperplane that maximally separates different classes with the largest margin.\n\n$w^{T}x + b$\n\n**Where**:\n\n::: incremental\n-   $w$ is the weight vector\n\n-   $x$ is the input feature vector\n\n-   $b$ is the bias\n:::\n\nThe objective is to classify data points into two classes based on which side of the hyperplane they fall.\n\n::: incremental\n-   If $w^{T}x + b > 0$, the data point is classified into one class\n\n-   If $w^{T}x + b < 0$it is classified into the other class\n:::\n:::\n\n## Constructing the marginal classifier {.smaller}\n\n::: panel-tabset\n## Visual\n\n**Goal**: Construct a hyperplane that maximizes the margin between classes, creating optimal separation with a buffer zone around the hyperplane that's free of data.\n\n::: {#eab0a554 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](10-svm_files/figure-revealjs/cell-7-output-1.png){width=944 height=368}\n:::\n:::\n\n\n## Approach\n\n**Given** the hyperplane function: $w^{T}x + b$\n\n**Objective Function**: Maximize the margin $M$ subject to the constraints that all data points are classified correctly, formulated as:\n\n$\\max_{w, b} \\; M = \\frac{1}{\\|w\\|}$\n\nSubject to $y_i(w^{T}x + b) \\geq 1$ for all $i$, where $y_i$ are class labels ($+1$ or $-1$)\n\n## Key points\n\n::: incremental\n-   **Margin & Support Vectors**\n\n    -   Distance between hyperplane and nearest points (support vectors) enhances generalization.\n\n-   **Linear Separability**\n\n    -   Assumes perfect separation by a hyperplane; real data may need soft margins or kernels.\n\n-   **Geometric Interpretation**\n\n    -   A line (2D) or plane (higher dimensions) divides feature space, crucial for accuracy.\n\n-   **Computational Aspects**\n\n    -   Finding optimal hyperplane involves quadratic programming to maximize margin.\n:::\n:::\n\n## The non-separable case {.smaller}\n\n::: panel-tabset\n## Visual\n\n> When data cannot be perfectly divided by a linear boundary, SVMs adapt using soft margins and the kernel trick to handle overlaps and complex patterns.\n\n![](images/non-separable.png){width=\"408\"}\n\n## Soft margin\n\n**Definition**: Extends SVM to allow some misclassifications, enhancing flexibility for non-linearly separable data.\n\n**Formula:** minimize $\\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{n}\\xi_i$\n\n**Where:**\n\n::: incremental\n-   $\\xi_i$ are [slack variables](https://en.wikipedia.org/wiki/Slack_variable) indicating misclassification degree\n\n-   $C$ is the regularization parameter\n:::\n\n**Key points:**\n\n::: incremental\n-   Slack variables permit data points to be on the wrong side of the margin.\n\n-   $C$ balances margin width and misclassification penalty, crucial for model performance.\n:::\n\n## Kernel trick\n\n**Definition**: Transforms data into a higher-dimensional space to find a separable hyperplane, enabling non-linear classification.\n\n**Formula**: Uses kernel functions like RBF ($\\exp(-\\gamma\\|x-x'\\|^2)$), Polynomial ($x \\cdot x')^d$), or Sigmoid ($\\tanh(\\alpha x \\cdot x' + r)$).\n\n**Key Points**:\n\n::: incremental\n-   Allows complex decision boundaries without explicitly computing high-dimensional transformations.\n\n-   Selection of kernel and its parameters ($d$, $\\gamma$ etc.) is vital for capturing data structure.\n:::\n\n## Overview\n\n**Handling Non-separable Data**\n\n::: incremental\n-   **Soft Margins**: Introduce flexibility by penalizing misclassifications to a degree controlled by $C$.\n\n-   **Kernel-Based Mapping**: Facilitates classification in cases where linear separation is not feasible in the original feature space.\n\n-   **Parameter Tuning**: Critical for optimizing the trade-off between model complexity and generalization ability, often achieved through cross-validation.\n:::\n:::\n\n## Support vector machines - revisited {.smaller}\n\n::: panel-tabset\n## Visual\n\n> **SVMs** adapt the [Maximal Margin Classifier]{.underline} for linearly non-separable data using slack variables and kernel functions, focusing on [soft margin classification for linear data]{.underline} and the [kernel trick for non-linear cases]{.underline}.\n\n![](images/svm-1.webp){fig-align=\"center\" width=\"450\"}\n\n## Soft margin (linear)\n\n**Objective Function**: Minimize the following objective to find the optimal hyperplane\n\n$\\min_{w, b, \\xi} \\; \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{n}\\xi_i$\n\nSubject to $y_i(w^{T}x + b) \\geq 1 - \\xi_i$ for all $i$, where:\n\n::: incremental\n-   $w$ is the weight factor\n\n-   $b$ is the bias\n\n-   $\\xi_i$ are slack variables representing the degree of misclassification of $x_i$\n\n-   $C$ is the is the regularization parameter controlling the trade-off between margin maximization and classification error.\n:::\n\n## Kernel trick (non-linear)\n\n**Dual Problem Solution**: The SVM optimization problem in its dual form allows the incorporation of kernel functions:\n\n$\\max_{\\alpha} \\; \\sum_{i=1}^{n}\\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{n}\\alpha_i \\alpha_j y_i y_j K(x_i, x_j)$\n\nSubject to $0 \\leq \\alpha_1 \\leq C$ for all $i$ and $\\sum_{i=1}^{n}\\alpha_iy_i = 0$, where:\n\n-   $\\alpha_i$ [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)\n\n-   $K(x_i, x_j)$ is the kernel function evaluating the dot product of $x_i$ and $x_j$ in the transformed feature space.\n\n## Key points\n\n::: incremental\n-   **Slack Variables** $\\xi_i$: Allow for flexibility in classification by permitting data points to be within the margin or incorrectly classified, i.e., the soft margin approach.\n\n-   **Regularization Parameter (**$C$**)** : Balances the trade-off between achieving a wide margin and minimizing the classification error; higher $C$ values lead to less tolerance for misclassification.\n\n-   **Kernel Functions**: Transform the original feature space into a higher-dimensional space, enabling SVMs to find a separating hyperplane in cases where data is not linearly separable. Common kernels include linear, polynomial, RBF, and sigmoid.\n\n-   **Dual Formulation**: Simplifies the problem by focusing on Lagrange multipliers, allowing the use of kernel functions and making the problem solvable even when the feature space is high-dimensional or infinite.\n\n-   **Support Vectors**: Data points corresponding to non-zero $\\alpha_i$​ values; these are the critical elements that define the hyperplane and margin.\n\n-   **Decision Function**: For a new data point $x$, the decision function becomes $\\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b\\right)$, determining the class membership based on the sign of the output.\n:::\n:::\n\n## Back to our data - preprocessing {.smaller}\n\n::: {#8afdb24c .cell execution_count=7}\n``` {.python .cell-code}\n# Selecting columns\nX = laborDiscrim[['education', 'special_skills', 'computer_skills', 'honors', 'sex', 'race']]\ny = laborDiscrim['call']\n\n# Define categorical and numerical features\ncategorical_features = ['sex', 'race']  # Add other categorical feature names as needed\nnumerical_features = ['education', 'special_skills', 'computer_skills', 'honors']  # Add other numerical feature names as needed\n\n# Create transformers for preprocessing\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nnumerical_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\n# Combine transformers into a ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n```\n:::\n\n\n## SVMs (linear kernel) - applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#1d9c6c56 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Splitting dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Define the full pipeline\nsvm_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('svc', SVC())\n])\n\n# Create a custom scorer for cross-validation\naccuracy_scorer = make_scorer(accuracy_score)\n\n# Training the SVM model\nsvm_pipeline.fit(X_train, y_train)\n\n# Predicting the test set results\ny_pred = svm_pipeline.predict(X_test)\n\n# Model Evaluation\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[910   0]\n [ 64   0]]\n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.97       910\n           1       0.00      0.00      0.00        64\n\n    accuracy                           0.93       974\n   macro avg       0.47      0.50      0.48       974\nweighted avg       0.87      0.93      0.90       974\n\n```\n:::\n:::\n\n\n## Metrics\n\n**Columns Explained:**\n\n::: incremental\n-   **Precision**: The ratio of correctly predicted positive observations to the total predicted positives. High precision relates to a low false positive rate.\n\n-   **Recall (Sensitivity)**: The ratio of correctly predicted positive observations to all observations in the actual class. It indicates the model's ability to find all the positive samples.\n\n-   **F1-Score**: The weighted average of Precision and Recall. It takes both false positives and false negatives into account, with 1 being the best score and 0 the worst.\n\n-   **Support**: The number of actual occurrences of the class in the specified dataset.\n:::\n\n## Classes\n\n::: incremental\n-   The report details metrics for two classes, labeled **`0`** and **`1`**, which represent the binary outcomes that the model is predicting.\n\n    **Class 0**:\n\n    -   Precision: 0.93, meaning 93% of the model's predictions for class 0 are correct.\n\n    -   Recall: 1.00, the model correctly identified 100% of all actual class 0s.\n\n    -   F1-Score: 0.97, showing a high balance between precision + recall for class 0.\n\n    -   Support: 910, indicating there are 910 instances of class 0 in the test set.\n\n    **Class 1**:\n\n    -   Precision: 0.00, none of the model's predictions for class 1 were correct.\n\n    -   Recall: 0.00, the model failed to correctly identify any actual class 1 instances.\n\n    -   F1-Score: 0.00, reflecting poor performance for class 1.\n\n    -   Support: 64, showing there are 64 instances of class 1 in the test set.\n:::\n\n## Overall Metrics\n\n::: incremental\n-   **Accuracy**: 93% of the predictions are correct across both classes.\n\n-   **Macro Average**: Shows unweighted averages with 47% precision, 50% recall, and 48% F1-score, reflecting equal consideration for each class irrespective of their size.\n\n-   **Weighted Average**: Indicates averages weighted by class support, resulting in 87% precision, 93% accuracy, and 90% F1-score, highlighting performance adjustments due to class imbalance.\n:::\n:::\n\n## Model tuning: SVM {.smaller}\n\n::: panel-tabset\n## Applied\n\n\n\n::: {#02ccc288 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Defining the parameter grid for Random Search\nparam_grid = {\n    'svc__C': np.logspace(-4, 4, 10),\n    'svc__gamma': np.logspace(-4, 1, 10),\n    'svc__kernel': ['linear', 'rbf']\n}\n\n# Create a custom scorer for cross-validation\naccuracy_scorer = make_scorer(accuracy_score)\n\n# Initialize RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    svm_pipeline,\n    param_distributions=param_grid,\n    n_iter = 20,  # Number of parameter settings sampled\n    scoring=accuracy_scorer,\n    cv = 5,\n    random_state = 42\n)\n\n# Fit the model with RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\n\n# Best parameters from RandomizedSearchCV\nprint(f'Best parameters found: {random_search.best_params_}')\n\n# Evaluate the model with the best found parameters\nbest_model = random_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Calculate and print the accuracy\naccuracy_best = accuracy_score(y_test, y_pred)\nprint(f'Accuracy with best parameters: {accuracy_best:.3f}')\n```\n:::\n\n\n```         \nBest parameters found: {'svc__kernel': 'rbf', 'svc__gamma': 0.774, 'svc__C': 0.359}\nAccuracy with best parameters: 0.934\n```\n\n## Residual analysis\n\n::: {#40ced08e .cell execution_count=11}\n``` {.python .cell-code}\n# Generate classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n```\n:::\n\n\n```         \n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.97       910\n           1       0.00      0.00      0.00        64\n\n    accuracy                           0.93       974\n   macro avg       0.47      0.50      0.48       974\nweighted avg       0.87      0.93      0.90       974\n\n[[910   0]\n [ 64   0]]\n```\n:::\n\n## Aside: SVM Regression {.smaller}\n\n::: panel-tabset\n## Visual\n\nSupport Vector Regression (SVR) adapts SVMs for regression, aiming to predict values within an $\\epsilon$-margin of the actual targets while maintaining simplicity.\n\n::: {#6c10059e .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](10-svm_files/figure-revealjs/cell-13-output-1.png){width=825 height=455}\n:::\n:::\n\n\n## Formula\n\n$f(x) = w^T \\phi(x) + b$\n\n**Where:**\n\n-   $w$ is the weight vector, $\\phi(x)$ represents the high-dimensional feature space mapped from the input vector $x$ via a kernel function, $b$ is the bias term.\n\nThe objective includes an $\\epsilon$-insensitive loss function - errors are tolerated if they are less than $\\epsilon$, leading to the following optimization problem:\n\n**Minimize**:\n\n$\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)$\n\n**Subject to**:\n\n$y_i - w^T \\phi(x_i) - b \\leq \\epsilon + \\xi_i$\n\n$w^T \\phi(x_i) + b - y_i \\leq \\epsilon + \\xi_i^*$\n\n$\\xi_i, \\xi_i^* \\geq 0$\n\n## Key points\n\n::: incremental\n1.  $\\epsilon$**-Insensitive Loss**: SVR predicts within an $\\epsilon$ margin of actual targets, ignoring errors within this range.\n\n2.  **Regularization Parameter (**$C$**)**: Balances prediction simplicity and margin violation tolerance.\n\n3.  **Kernel Trick**: Enables capturing non-linear relationships by mapping inputs into higher-dimensional spaces.\n\n4.  **Support Vectors**: Data points that lie outside the $\\epsilon$-insensitive zone or on the boundary become support vectors, directly influencing the model.\n\n5.  **Versatility**: Suitable for both linear and non-linear regression tasks across various domains.\n:::\n:::\n\n## Conclusions {.smaller}\n\n::: incremental\n1.  **SVM Effectiveness**: SVMs are highly effective for binary classification, showcasing strong generalization with appropriate kernel and hyperparameter selection.\n\n2.  **Hyperparameter Tuning Importance**: Significant improvements in model performance are achievable through precise hyperparameter tuning, utilizing methods like RandomizedSearchCV and GridSearchCV.\n\n3.  **Preprocessing Necessity**: Proper preprocessing, including feature scaling and categorical encoding, is crucial for optimal SVM performance.\n\n4.  **Kernel Trick Benefits**: The kernel trick enables SVMs to handle non-linear problems efficiently by operating in high-dimensional spaces without explicit high-dimension mapping.\n\n5.  **Cross-Validation for Reliability**: Employing cross-validation ensures robust and generalizable hyperparameter selection.\n\n6.  **Balancing Model Complexity**: There's a critical balance between capturing data complexity with models like **`rbf`** kernel SVMs and avoiding overfitting, underscored by careful adjustment of the regularization parameter $C$.\n:::\n\n",
    "supporting": [
      "10-svm_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}