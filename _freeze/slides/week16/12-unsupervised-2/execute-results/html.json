{
  "hash": "ee26f820788cc7188aa6b5bdeb152563",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Unsupervised<br>Learning II\nsubtitle: Lecture 12\nauthor: \"{{< var slides.author >}}\"\ninstitute: \"{{< var slides.institute >}}\"\nfooter: \"{{< var slides.footer >}}\"\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nexecute: \n  warning: false\n  message: false\n  error: false\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   HW 05 is due Fri Apr 26, 11:59pm\n-   RQ 05 is due Wed May 01, 11:59pm\n-   Final Project Presentations are Mon May 06, 1pm\n\n## Setup {.smaller}\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Data Handling and Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.mixture import GaussianMixture\n\n# Machine Learning Models\nfrom sklearn.cluster import KMeans\nfrom sklearn_extra.cluster import KMedoids\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.cluster.hierarchy import fcluster\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the default style for visualization\nsns.set_theme(style = \"white\", palette = \"colorblind\")\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n```\n:::\n\n\n## From last time: question\n\nCan we [identify distinct baseball player groupings]{.underline} based on their **player stats** in 2018?\n\n## Our data: MLB player stats {.smaller}\n\n::: panel-tabset\n## Read + Head\n\n::: {#a442c4d6 .cell execution_count=2}\n``` {.python .cell-code}\nmlb_players_18 = pd.read_csv(\"data/mlb_players_18.csv\", encoding = 'iso-8859-1')\n\nmlb_players_18.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>team</th>\n      <th>position</th>\n      <th>games</th>\n      <th>AB</th>\n      <th>R</th>\n      <th>H</th>\n      <th>doubles</th>\n      <th>triples</th>\n      <th>HR</th>\n      <th>RBI</th>\n      <th>walks</th>\n      <th>strike_outs</th>\n      <th>stolen_bases</th>\n      <th>caught_stealing_base</th>\n      <th>AVG</th>\n      <th>OBP</th>\n      <th>SLG</th>\n      <th>OPS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Allard, K</td>\n      <td>ATL</td>\n      <td>P</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gibson, K</td>\n      <td>MIN</td>\n      <td>P</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Law, D</td>\n      <td>SF</td>\n      <td>P</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Nuno, V</td>\n      <td>TB</td>\n      <td>P</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Romero, E</td>\n      <td>KC</td>\n      <td>P</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Define columns\n\n::: {#7ddfdccd .cell execution_count=3}\n``` {.python .cell-code}\n# Define the columns based on their type for preprocessing\ncategorical_features = ['team', 'position']\nnumerical_features = ['games', 'AB', 'R', 'H', 'doubles', 'triples', 'HR', 'RBI', 'walks', 'strike_outs', 'stolen_bases', 'caught_stealing_base', 'AVG', 'OBP', 'SLG', 'OPS']\n```\n:::\n\n\n## Prepare steps\n\n::: {#b4468151 .cell execution_count=4}\n``` {.python .cell-code}\n# Handling missing values: Impute missing values if any\n# For numerical features, replace missing values with the median of the column\n# For categorical features, replace missing values with the most frequent value of the column\nnumerical_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps = [\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))])\n\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_transformer, numerical_features),\n    ('cat', categorical_transformer, categorical_features)])\n```\n:::\n\n\n## Transformations\n\n::: {#256da58e .cell execution_count=5}\n``` {.python .cell-code}\n# Apply the transformations to the dataset\nmlb_preprocessed = preprocessor.fit_transform(mlb_players_18)\n\n# The result is a NumPy array. To convert it back to a DataFrame:\n# Update the method to get_feature_names_out for compatibility with newer versions of scikit-learn\nfeature_names = list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))\nnew_columns = numerical_features + feature_names\n\nmlb_preprocessed_df = pd.DataFrame(mlb_preprocessed, columns = new_columns)\nmlb_preprocessed_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>games</th>\n      <th>AB</th>\n      <th>R</th>\n      <th>H</th>\n      <th>doubles</th>\n      <th>triples</th>\n      <th>HR</th>\n      <th>RBI</th>\n      <th>walks</th>\n      <th>strike_outs</th>\n      <th>...</th>\n      <th>position_1B</th>\n      <th>position_2B</th>\n      <th>position_3B</th>\n      <th>position_C</th>\n      <th>position_CF</th>\n      <th>position_DH</th>\n      <th>position_LF</th>\n      <th>position_P</th>\n      <th>position_RF</th>\n      <th>position_SS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.904553</td>\n      <td>-0.695768</td>\n      <td>-0.596283</td>\n      <td>-0.633846</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.944603</td>\n      <td>-0.690386</td>\n      <td>-0.559089</td>\n      <td>-0.613594</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.824454</td>\n      <td>-0.695768</td>\n      <td>-0.596283</td>\n      <td>-0.633846</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.944603</td>\n      <td>-0.690386</td>\n      <td>-0.633478</td>\n      <td>-0.613594</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.583894</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.884529</td>\n      <td>-0.695768</td>\n      <td>-0.596283</td>\n      <td>-0.633846</td>\n      <td>-0.525322</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 56 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n# Clustering cont.\n\n## Clustering methods\n\n::: {style=\"text-align: center;\"}\n```{=html}\n<iframe width=\"1200\" height=\"400\" src=\"https://{{< var website.url >}}/tables/model-cheatsheet.html\" frameborder=\"1\" style=\"background:white;\"></iframe>\n```\n:::\n\n## Hierarchical Clustering {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {#edc86fcf .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-7-output-1.png){width=657 height=520}\n:::\n:::\n\n\n## Agglomerative Clustering Process\n\n::: incremental\n-   **Initialization**: Each data point starts as its own cluster ($N$ data points → $N$ clusters).\n\n-   **Find the Closest Pair of Clusters**: Use a distance metric to identify the closest clusters, with options including:\n\n    -   **Single Linkage**: Distance between the nearest points in two clusters.\n\n    -   **Complete Linkage**: Distance between the farthest points in two clusters.\n\n    -   **Average Linkage**: Average distance between all point pairs across two clusters.\n\n    -   **Ward's Method**: Minimizes the increase in total within-cluster variance after merging.\n\n-   **Merge Clusters**: The two nearest clusters are merged into one.\n\n-   **Update Distances**: Recalculate distances between the new cluster and existing clusters.\n\n-   **Repeat**: Continue merging the nearest clusters until reaching a single cluster or a desired number of clusters.\n\n-   **Result**: Visualized as a dendrogram showing merge sequences and distances.\n:::\n\n## Key points\n\n::: incremental\n-   **Initialization**: Begins with each data point as a separate cluster.\n\n-   **Linkage Criteria**: Uses metrics like minimum, maximum, average distance, or Ward's method to measure cluster distances.\n\n-   **Merging Step**: Merges the closest clusters at each step based on the linkage criteria.\n\n-   **Termination**: Ends when all points are in a single cluster or a stop condition is reached.\n\n-   **Objective**: Builds a cluster hierarchy reflective of data structure, not minimizing a specific criterion.\n\n-   **Optimal Clusters**: Chosen by analyzing the dendrogram, without a predefined number.\n\n-   **Sensitivity**: Influenced by linkage method and outliers.\n\n-   **Efficiency**: More computationally intensive than k-means, suitable for detailed hierarchical analysis.\n:::\n:::\n\n## Hierarchical clustering: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#3f4138d1 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Apply hierarchical clustering\nZ = linkage(mlb_preprocessed, 'ward')\n\n# Plotting the dendrogram for visual inspection\nplt.figure(figsize = (10, 4))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\n# To cut the dendrogram at a determined number of clusters and evaluate performance\nmax_d = 25  # for example, distance cutoff to define number of clusters\nclusters_hc = fcluster(Z, max_d, criterion = 'distance')\n\n# Evaluate clustering performance (example using silhouette score)\nsilhouette_avg_hc = silhouette_score(mlb_preprocessed, clusters_hc)\nprint(f\"The average silhouette_score for hierarchical clustering is : {silhouette_avg_hc:.3f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-8-output-1.png){width=814 height=376}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nThe average silhouette_score for hierarchical clustering is : 0.261\n```\n:::\n:::\n\n\n## Visualize results\n\n::: {#c2a0528b .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# PCA for dimensionality reduction\npca = PCA(n_components = 2)\nmlb_pca = pca.fit_transform(mlb_preprocessed)\n\n# Visualize the clusters\nplt.figure(figsize = (9, 5))\nsns.scatterplot(x = mlb_pca[:, 0], y = mlb_pca[:, 1], hue = clusters_hc, alpha = 0.75, palette = \"colorblind\")\nplt.title('MLB Players Clustered (Hierarchical Clustering with PCA-reduced Features)')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend(title = 'Cluster')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-9-output-1.png){width=731 height=445}\n:::\n:::\n\n\n:::\n\n## DBSCAN {.smaller}\n\nDensity-Based Spatial Clustering of Applications with Noise\n\n::: panel-tabset\n## Visual\n\n::: {#dc6a4a16 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-10-output-1.png){width=657 height=519}\n:::\n:::\n\n\n## Key points\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm with the following key points:\n\n1.  **Density-Based:** Identifies clusters based on the density of data points, effectively handling clusters of various shapes and sizes.\n2.  **Noise Sensitivity:** Capable of distinguishing noise (outlier points) from clusters, enhancing its robustness to outliers.\n3.  **Parameters:** Primarily governed by `eps` (the maximum distance between two points for them to be considered as in the same neighborhood) and `min_samples` (the minimum number of points to form a dense region).\n4.  **No Need for Cluster Count:** Unlike K-means, DBSCAN does not require the number of clusters to be specified in advance.\n5.  **Versatility:** Works well on datasets with complex structures and varying densities.\n:::\n\n## DBSCAN: applied {.smaller}\n\n::: panel-tabset\n## Optimal parameters\n\n::: {#f0a38a53 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Selecting relevant features for clustering\nfeatures = mlb_players_18[['AVG', 'OBP', 'SLG', 'OPS']]\n\n# Scaling the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(features)\n\n# Finding an optimal eps value using the nearest neighbors\nneighbors = NearestNeighbors(n_neighbors=4)\nneighbors_fit = neighbors.fit(X_scaled)\ndistances, indices = neighbors_fit.kneighbors(X_scaled)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\n\nplt.figure(figsize = (9, 5))\nplt.plot(distances)\nplt.title('K-Nearest Neighbors: Finding Optimal Epsilon')\nplt.xlabel('Points sorted by distance')\nplt.ylabel('Epsilon distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-11-output-1.png){width=724 height=445}\n:::\n:::\n\n\n## Model summary\n\n::: {#8b2386ea .cell execution_count=11}\n``` {.python .cell-code}\n# Based on the elbow method from the plot, let's pick an eps value and apply DBSCAN\neps_value = 0.5  # This value should be adjusted based on the elbow point observed in the plot\ndbscan = DBSCAN(eps = eps_value, min_samples = 5)\nclusters_dbscan = dbscan.fit_predict(X_scaled)\n\n# Evaluate clustering performance using silhouette score\nsilhouette_avg_dbscan = silhouette_score(X_scaled, clusters_dbscan)\nprint(f\"The average silhouette_score for DBSCAN is : {silhouette_avg_dbscan:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe average silhouette_score for DBSCAN is : 0.060\n```\n:::\n:::\n\n\n## Visualize results\n\n::: {#2df814fc .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# PCA for dimensionality reduction\npca = PCA(n_components = 2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Visualize the clusters\nplt.figure(figsize = (9, 5))\nsns.scatterplot(x = X_pca[:, 0], y = X_pca[:, 1], hue = clusters_dbscan, palette = \"colorblind\", alpha = 0.75, legend = \"full\")\nplt.title('DBSCAN Clustering of MLB Players with PCA-reduced Features')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend(title = 'Cluster')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-13-output-1.png){width=731 height=445}\n:::\n:::\n\n\n:::\n\n# Model-based clustering\n\n## Hierarchical vs. Model-based clustering {.smaller}\n\n::: panel-tabset\n## Hierarchical\n\n::: incremental\n-   **Definition**: Builds clusters by either successively merging or splitting data points, represented as a tree or dendrogram.\n\n-   **Approach**: Does not assume a specific number of clusters a priori. Clusters are formed based on a distance metric and linkage criterion (e.g., single, complete, average linkage).\n\n-   **Flexibility**: Can reveal data structure at different levels of granularity, from many small clusters to a few large ones.\n\n-   **Scalability**: Computationally intensive for large datasets due to the need to compute and update distances between all cluster pairs.\n\n-   **Interpretability**: The dendrogram provides a visual representation of the data hierarchy, making it intuitive to understand cluster relationships.\n\n-   **Use Cases**: Effective for exploratory data analysis, identifying hierarchical structure in data, and datasets where the number of clusters is not known beforehand.\n:::\n\n## Model-based\n\n::: incremental\n-   **Definition**: Assumes data is generated from a mixture of several probabilistic models, typically Gaussian distributions, each representing a cluster.\n\n-   **Approach**: Optimizes the fit between data and model, often using Expectation-Maximization (EM) algorithm to estimate model parameters.\n\n-   **Flexibility**: Can accommodate clusters of different shapes, sizes, and densities due to its probabilistic foundation.\n\n-   **Scalability**: Tends to require more computational resources, especially as the number of dimensions (features) increases.\n\n-   **Interpretability**: Provides statistical measures for model selection (e.g., BIC) and can infer the probability of membership for each data point in each cluster.\n\n-   **Use Cases**: Well-suited for complex datasets where clusters may overlap or have non-spherical shapes.\n:::\n\n## Key differences\n\n::: incremental\n-   **Model Assumptions**: Model-based clustering requires data to fit statistical distributions; hierarchical does not.\n\n-   **Output**: Hierarchical clustering produces a dendrogram showing data hierarchy; model-based identifies a fixed number of clusters.\n\n-   **Scalability**: Model-based clustering is computationally intensive due to model fitting; hierarchical clustering's main demand is in distance computation.\n\n-   **Flexibility and Interpretability**: Hierarchical offers granularity and intuitive dendrograms; model-based allows diverse cluster shapes but requires understanding of probabilistic models.\n:::\n:::\n\n## Multivariate normal distribution {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {#5f5bb4f8 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-14-output-1.png){width=479 height=480}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {#62dc003d .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-15-output-1.png){width=637 height=445}\n:::\n:::\n\n\n:::\n:::\n\n## Formula\n\nThe formula for the probability density function (PDF) of the multivariate normal (Gaussian) distribution for a $d$-dimensional random vector $X = [X_1, X_2,…,X_d]^T$ with mean vector $\\mu = [\\mu_1, \\mu_2,…,\\mu_d]^T$ and covariance $\\Sigma$ (a $d \\times d$ positive definite matrix) is given by:\n\n$f(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu) \\right)$\n\n**Where**:\n\n::: incremental\n-   $x$ is a realization of the random vector $X$\n\n-   $|\\Sigma|$ denotes the determinant of $\\Sigma$\n\n-   $\\Sigma^{-1}$ is the inverse of $\\Sigma$\n\n-   $T$ denotes the transpose of a vector\n:::\n:::\n\n## Soft clustering and mixtures of normals {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {#b3d7040c .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-16-output-1.png){width=644 height=519}\n:::\n:::\n\n\n## Key points\n\n::: incremental\n-   **Soft Clustering**: Allows data points to belong to multiple clusters with varying degrees of membership, expressed probabilistically.\n\n-   **Gaussian Mixture Models (GMMs)**: Assume data originates from a mix of several Gaussian distributions, each representing a cluster.\n\n-   **Probabilistic Membership**: Provides probabilities for each data point's membership in different clusters, rather than assigning them to a single cluster.\n\n-   **Parameters and Estimation**: Utilizes means, covariances, and mixing coefficients to characterize clusters, estimated via the Expectation-Maximization algorithm.\n\n-   **Versatility**: Effective for datasets with overlapping clusters or varying cluster shapes and sizes.\n\n-   **Applications**: Useful in fields like image processing and bioinformatics, where the nuanced understanding of data structure is crucial.\n:::\n:::\n\n## Soft clustering: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {#6ab2dff1 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\n# Fit a Gaussian Mixture Model\ngmm = GaussianMixture(n_components = 3, covariance_type = 'full', random_state = 0)\ngmm.fit(mlb_preprocessed)\n\n# Predict soft assignments\nprobs = gmm.predict_proba(mlb_preprocessed)\n\n# Assign each data point to the cluster with the highest probability\nhard_assignments = np.argmax(probs, axis=1)\n\n# Calculate the silhouette score using these hard assignments\nsilhouette_avg = silhouette_score(mlb_preprocessed, hard_assignments)\n\nprint(f\"The average silhouette score for the GMM clustering is: {silhouette_avg:.3f}\")\n\n# BIC\nbic_score = gmm.bic(mlb_preprocessed)\nprint(f\"BIC Score: {bic_score:.3f}\")\n\n# Convergence\nprint(f\"Converged: {gmm.converged_}\")\n\n# Number of iterations\nprint(f\"Number of iterations: {gmm.n_iter_}\")\n\n# Cluster weights\nprint(f\"Cluster Weights: {gmm.weights_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe average silhouette score for the GMM clustering is: 0.319\nBIC Score: -142996.239\nConverged: True\nNumber of iterations: 10\nCluster Weights: [0.48346465 0.24021972 0.27631563]\n```\n:::\n:::\n\n\n## Visualize results\n\n::: {#c130373f .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\npca = PCA(n_components = 2)\nmlb_pca = pca.fit_transform(mlb_preprocessed)\n\n# Choose a cluster to visualize its probability distribution\nplt.figure(figsize = (9, 5))\nplt.scatter(mlb_pca[:, 0], mlb_pca[:, 1], c = probs[:, 0], cmap = 'coolwarm', edgecolor = 'k', alpha = 0.7)\nplt.colorbar(label = 'Probability of Cluster 1')\nplt.title('Soft Clustering Visualization')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](12-unsupervised-2_files/figure-revealjs/cell-18-output-1.png){width=693 height=445}\n:::\n:::\n\n\n:::\n\n## Conclusions {.smaller}\n\n**Identifying Baseball Player Groupings:**\n\n::: incremental\n-   **Methods:**\n\n    -   Hierarchical clustering (group by similarity)\n\n    -   K-means clustering (fixed number of groups)\n\n    -   Model-based clustering (fit statistical models)\n\n-   **Interpretation:** Analyze clusters for shared player characteristics (e.g., home run hitters, stolen base specialists).\n\n-   **Considerations:**\n\n    -   Desired number of clusters\n\n    -   Player stats used\n\n    -   Distance metric chosen\n\n    -   Computational resources available\n:::\n\n## In-class activity {.smaller}\n\n::: panel-tabset\n## Intro\n\n::: task\nGo to [ex-12](https://{{< var website.url >}}/exercises/ex-12.html) and perform the tasks\n:::\n\n![](images/tSwift.jpeg){fig-align=\"center\" width=\"630\"}\n\n## Visual\n\n![](images/20231017.png){fig-align=\"center\"}\n:::\n\n",
    "supporting": [
      "12-unsupervised-2_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}