{
  "hash": "56673d796e494b4739c902e5a009efda",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Association\nsubtitle: Lecture 13\nauthor: \"{{< var slides.author >}}\"\ninstitute: \"{{< var slides.institute >}}\"\nfooter: \"{{< var slides.footer >}}\"\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nexecute: \n  warning: false\n  message: false\n  error: false\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   RQ 05 is due Today, 11:59pm\n-   Final Project Presentations are Mon May 06, 1pm\n\n## Setup {.smaller}\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Data Handling and Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.mixture import GaussianMixture\n\n# Machine Learning Models\nfrom mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# Stats\nfrom scipy.stats import pearsonr\nimport statsmodels.api as sm\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the default style for visualization\nsns.set_theme(style = \"white\", palette = \"colorblind\")\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n```\n:::\n\n\n## Market Basket Analysis {.smaller}\n\n> Market Basket Analysis (MBA) is a data mining technique used to discover associations between items within large datasets, typically used in retail to find associations between products purchased together.\n\n::: {#2c318659 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nItemsets from records:\n{('milk',), ('eggs',), ('apple', 'milk'), ('apple', 'eggs'), ('apple', 'eggs', 'bread'), ('eggs', 'milk'), ('apple',), ('banana',), ('apple', 'banana'), ('banana', 'milk'), ('apple', 'banana', 'milk'), ('eggs', 'bread'), (), ('bread',), ('apple', 'bread')}\n\n\nItemsets from items:\n[(), ('apple',), ('banana',), ('milk',), ('eggs',), ('bread',), ('apple', 'banana'), ('apple', 'milk'), ('apple', 'eggs'), ('apple', 'bread'), ('banana', 'milk'), ('banana', 'eggs'), ('banana', 'bread'), ('milk', 'eggs'), ('milk', 'bread'), ('eggs', 'bread'), ('apple', 'banana', 'milk'), ('apple', 'banana', 'eggs'), ('apple', 'banana', 'bread'), ('apple', 'milk', 'eggs'), ('apple', 'milk', 'bread'), ('apple', 'eggs', 'bread'), ('banana', 'milk', 'eggs'), ('banana', 'milk', 'bread'), ('banana', 'eggs', 'bread'), ('milk', 'eggs', 'bread'), ('apple', 'banana', 'milk', 'eggs'), ('apple', 'banana', 'milk', 'bread'), ('apple', 'banana', 'eggs', 'bread'), ('apple', 'milk', 'eggs', 'bread'), ('banana', 'milk', 'eggs', 'bread'), ('apple', 'banana', 'milk', 'eggs', 'bread')]\n```\n:::\n:::\n\n\n## Association rules\n\nTo build an association rule, we should have at least one **antecedent** and one **consequent**:\n\n::: incremental\n-   One antecedent and one consequent: if { 🍪 } then { ☕️ }\n\n-   Multi antecedent: if { 🍪, 🍰 } then { ☕️}\n\n-   Multi consequent: if { 🍪 } then { ☕️, 🥛 }\n:::\n\n## Frequency of events {.smaller}\n\n::: {#0aef7091 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nFrequency of the itemsets:\n('apple',), 0.67\n('banana',), 0.33\n('milk',), 0.67\n('apple', 'banana'), 0.33\n('apple', 'milk'), 0.33\n('banana', 'milk'), 0.33\n('apple', 'banana', 'milk'), 0.33\n('eggs',), 0.67\n('bread',), 0.33\n('apple', 'eggs'), 0.33\n('apple', 'bread'), 0.33\n('eggs', 'bread'), 0.33\n('apple', 'eggs', 'bread'), 0.33\n('eggs', 'milk'), 0.33\n```\n:::\n:::\n\n\n## Example data {.smaller}\n\n::: panel-tabset\n## Read + Head\n\n> \"[The Bread Basket](https://www.kaggle.com/datasets/mittalvasu95/the-bread-basket)\" data set that belongs to a bakery located in Edinburgh and includes over 9000 transactions. You can download it from Kaggle.\n\n::: {#7eeb5f24 .cell execution_count=4}\n``` {.python .cell-code}\nbasket = pd.read_csv(\"data/breadBasket.csv\")\nbasket.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Transaction</th>\n      <th>Item</th>\n      <th>date_time</th>\n      <th>period_day</th>\n      <th>weekday_weekend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Bread</td>\n      <td>30-10-2016 09:58</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Scandinavian</td>\n      <td>30-10-2016 10:05</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Scandinavian</td>\n      <td>30-10-2016 10:05</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Hot chocolate</td>\n      <td>30-10-2016 10:07</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Jam</td>\n      <td>30-10-2016 10:07</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Transaction \\#\n\n::: {#a2c7a89b .cell execution_count=5}\n``` {.python .cell-code}\nbasket.loc[basket['Transaction']==3]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Transaction</th>\n      <th>Item</th>\n      <th>date_time</th>\n      <th>period_day</th>\n      <th>weekday_weekend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Hot chocolate</td>\n      <td>30-10-2016 10:07</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Jam</td>\n      <td>30-10-2016 10:07</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>Cookies</td>\n      <td>30-10-2016 10:07</td>\n      <td>morning</td>\n      <td>weekend</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Wrangle\n\n::: {#4af11d2a .cell execution_count=6}\n``` {.python .cell-code}\n# Groupby by the Transaction Ids and count items\nbasket = basket.groupby(\n              by = ['Transaction', \n                  'Item'])['Item'].count().reset_index(name = 'Item_Count')\n\n# Pivot table by the transaction and convert item count to boolean \nbasket = basket.pivot_table(\n              index = 'Transaction', \n              columns = 'Item', \n              values = 'Item_Count', \n              aggfunc = 'sum').fillna(0).astype(bool)\n\nbasket.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Item</th>\n      <th>Adjustment</th>\n      <th>Afternoon with the baker</th>\n      <th>Alfajores</th>\n      <th>Argentina Night</th>\n      <th>Art Tray</th>\n      <th>Bacon</th>\n      <th>Baguette</th>\n      <th>Bakewell</th>\n      <th>Bare Popcorn</th>\n      <th>Basket</th>\n      <th>...</th>\n      <th>The BART</th>\n      <th>The Nomad</th>\n      <th>Tiffin</th>\n      <th>Toast</th>\n      <th>Truffles</th>\n      <th>Tshirt</th>\n      <th>Valentine's card</th>\n      <th>Vegan Feast</th>\n      <th>Vegan mincepie</th>\n      <th>Victorian Sponge</th>\n    </tr>\n    <tr>\n      <th>Transaction</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 94 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Association rule metrics {.smaller}\n\n::: incremental\n-   **Support:** Proportion of transactions including a specific itemset.\n\n-   **Confidence:** Probability that buying item A leads to buying item B.\n\n-   **Lift:** Ratio of observed to expected frequency of A and B together.\n\n-   **Leverage:** Difference in observed versus expected frequency of A and B together.\n\n-   **Conviction:** Ratio indicating dependency of B on A, based on their co-occurrence versus independence.\n:::\n\n## Support {.smaller}\n\n$\\text{Support}(A \\rightarrow B) = \\frac{Freq(A,B)}{N}$\n\n**Pros:**\n\n::: incremental\n-   Easy to understand and calculate.\n\n-   Helps identify the most common itemsets in the dataset, which can be crucial for decision-making.\n:::\n\n**Cons:**\n\n::: incremental\n-   High support may not always imply a useful or interesting rule, as it does not account for the strength or significance of the association.\n\n-   Can miss interesting rules with lower support but high confidence or lift.\n:::\n\n## Confidence {.smaller}\n\n$\\text{Confidence}(A \\rightarrow B) = \\frac{Support(A \\rightarrow B)}{Support(A)}$\n\n**Pros:**\n\n::: incremental\n-   Directly measures the reliability of an association rule.\n\n-   Intuitive interpretation as the conditional probability of occurrence.\n:::\n\n**Cons:**\n\n::: incremental\n-   Can be misleading if the consequent is very common in the dataset, leading to high confidence values that are not necessarily due to a strong association.\n\n-   Does not account for the base popularity of the consequent item.\n:::\n\n## Lift {.smaller}\n\n$\\text{Lift}(A \\rightarrow B) = \\frac{Support(A \\rightarrow B)}{Support(A) \\times Support(B)}$\n\n**Pros:**\n\n::: incremental\n-   Accounts for the base popularity of both antecedent and consequent, providing a more accurate measure of association strength.\n\n-   A lift value greater than 1 indicates a positive association.\n:::\n\n**Cons:**\n\n::: incremental\n-   Can be harder to interpret compared to support and confidence.\n\n-   Does not provide directionality of the rule (i.e., which item is driving the association).\n:::\n\n## Leverage {.smaller}\n\n$\\text{Leverage}(A \\rightarrow B) = Support(A \\rightarrow B) - (Support(A) \\times Support(B))$\n\n**Pros:**\n\n::: incremental\n-   Measures the difference in occurrence frequency of itemsets, providing insight into their co-occurrence beyond chance.\n\n-   Easy to calculate and interpret.\n:::\n\n**Cons:**\n\n::: incremental\n-   Similar to lift, does not indicate the direction of the association.\n\n-   May not differentiate well between items with very high or very low support.\n:::\n\n## Conviction {.smaller}\n\n$\\text{Conviction}(A \\rightarrow B) = \\frac{1 - Support(B)}{1 - Confidence(A \\rightarrow B)}$\n\n**Pros:**\n\n::: incremental\n-   Captures the degree of dependency between antecedent and consequent, with higher values indicating stronger association.\n\n-   Provides a measure of rule interest by comparing the observed frequency of the rule to the frequency expected if the items were independent.\n:::\n\n**Cons:**\n\n::: incremental\n-   Less intuitive to understand and interpret for those new to association mining.\n\n-   Like lift, does not directly indicate the direction of the association.\n:::\n\n# Frequent itemsets mining\n\n## Association rule mining task {.smaller}\n\n::: incremental\n-   Given a set of transactions $T$, the goal of association rule mining is to find all rules having\n\n    -   support ≥ *minsup* threshold\n\n    -   confidence ≥ *minconf* threshold\n\n-   **Brute-force approach**:\n\n    -   List all possible association rules\n\n    -   Compute the support and confidence for each rule\n\n    -   Prune rules that fail the *minsup* and *minconf* thresholds\n\n    -   $\\rightarrow$ Computationally prohibitive!\n:::\n\n## Mining association rules {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n| **TID** | **Items**                 |\n|---------|---------------------------|\n| 1       | Bread, Milk               |\n| 2       | Bread, Diaper, Beer, Eggs |\n| 3       | Milk, Diaper, Beer, Coke  |\n| 4       | Bread, Milk, Diaper, Beer |\n| 5       | Bread, Milk, Diaper, Coke |\n:::\n\n::: {.column width=\"50%\"}\n::: fragment\n{Milk,Diaper} $\\rightarrow$ {Beer} (s=0.4, c=0.67)\n\n{Milk,Beer} $\\rightarrow$ {Diaper} (s=0.4, c=1.0)\n\n{Diaper,Beer} $\\rightarrow$ {Milk} (s=0.4, c=0.67)\n\n{Beer} $\\rightarrow$ {Milk,Diaper} (s=0.4, c=0.67)\n\n{Diaper} $\\rightarrow$ {Milk,Beer} (s=0.4, c=0.5)\n\n{Milk} $\\rightarrow$ {Diaper,Beer} (s=0.4, c=0.5)\n:::\n:::\n:::\n\n**Observations**:\n\n::: incremental\n-   All rules are binary partitions of the itemset {Milk, Diaper, Beer}.\n\n-   Rules from the same itemset share support but differ in confidence.\n\n-   Support and confidence requirements can be separated.\n:::\n\n## Mining association rules {.smaller}\n\n**Two-step approach:**\n\n1.  Frequent Itemset Generation\n    -   Generate all itemsets whose support ≥ minsup\n2.  Rule Generation\n    -   Generate high confidence rules from each frequent itemset\n    -   Each rule is a binary partitioning of a frequent itemset\n\n::: fragment\nFrequent itemset generation is still computationally expensive...\n:::\n\n## Frequent itemset generation {.smaller}\n\nGiven $d$ items, there are $2^d$ itemsets:\n\n![](images/chart-tabset-card.png){fig-align=\"center\" width=\"727\"}\n\n## Apriori method {.smaller}\n\n::: panel-tabset\n## Visual\n\nAll subsets of a frequent itemset must also be frequent.\n\n![](images/apriori.png){width=\"675\"}\n\n## Formula\n\n**Rule generation**\n\n::: incremental\n1.  Calculate support for each itemset to determine which meet the threshold.\n2.  Generate association rules.\n:::\n\n$\\text{Support}(A \\rightarrow B) = \\frac{Freq(A,B)}{N}$\n\n**Evaluate rule strength**\n\n::: incremental\n1.  Assess the likelihood that consequent item is purchased with antecedent.\n2.  Filtering based on a confidence threshold.\n:::\n\n$\\text{Confidence}(A \\rightarrow B) = \\frac{Support(A \\rightarrow B)}{Support(A)}$\n\n**Contribute to decision making**\n\n::: incremental\n1.  Identify strong associations\n2.  Improving predictive accuracy\n:::\n\n## Pros + Cons\n\n**\\\nPros:**\n\n::: incremental\n-   **Simplicity:** Straightforward and easy to understand.\n\n-   **Generality:** Applicable to any transaction dataset without needing domain-specific knowledge.\n\n-   **Widely Used:** A foundational technique that informs many other algorithms.\n:::\n\n**Cons:**\n\n::: incremental\n-   **Performance Issues:** Computationally expensive for large datasets with many items.\n\n-   **Memory Intensive:** Requires significant memory for storing candidates at lower support thresholds.\n\n-   **Redundant Computations:** Generates many candidates, leading to unnecessary computations.\n:::\n:::\n\n## Apriori method: applied {.smaller}\n\n::: {#4173e6b6 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Apply Apriori to find frequent itemsets\nfrequent_itemsets_apriori = apriori(basket, min_support = 0.01, use_colnames = True)\n\n# Generate association rules from Apriori itemsets\nrules_apriori = association_rules(frequent_itemsets_apriori, metric = \"confidence\", min_threshold = 0.5)\nprint(\"Association Rules from Apriori:\\n\", rules_apriori.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAssociation Rules from Apriori:\n        antecedents consequents  antecedent support  consequent support  \\\n0      (Alfajores)    (Coffee)            0.036344            0.478394   \n1           (Cake)    (Coffee)            0.103856            0.478394   \n2        (Cookies)    (Coffee)            0.054411            0.478394   \n3  (Hot chocolate)    (Coffee)            0.058320            0.478394   \n4          (Juice)    (Coffee)            0.038563            0.478394   \n\n    support  confidence      lift  representativity  leverage  conviction  \\\n0  0.019651    0.540698  1.130235               1.0  0.002264    1.135648   \n1  0.054728    0.526958  1.101515               1.0  0.005044    1.102664   \n2  0.028209    0.518447  1.083723               1.0  0.002179    1.083174   \n3  0.029583    0.507246  1.060311               1.0  0.001683    1.058553   \n4  0.020602    0.534247  1.116750               1.0  0.002154    1.119919   \n\n   zhangs_metric   jaccard  certainty  kulczynski  \n0       0.119574  0.039693   0.119446    0.290888  \n1       0.102840  0.103745   0.093105    0.320679  \n2       0.081700  0.055905   0.076787    0.288707  \n3       0.060403  0.058333   0.055314    0.284542  \n4       0.108738  0.041507   0.107078    0.288656  \n```\n:::\n:::\n\n\n## Factors affecting complexity {.smaller}\n\n1.  **Choice of Minimum Support Threshold:**\n\n    -   Lowering the support threshold $\\rightarrow$ a larger number of frequent itemsets\n\n    -   $\\rightarrow$ increase in candidate itemsets & the maximal length of frequent itemsets\n\n2.  **Dimensionality (Number of Items) of the Dataset:**\n\n    -   Greater storage space needed to keep track of each item's support count\n\n    -   If the dataset has a higher number of frequent items, both computational and input/output costs may increase.\n\n3.  **Size of Database:**\n\n    -   The runtime of the Apriori algorithm may increase with the number of transactions since it makes multiple passes over the database.\n\n4.  **Average Transaction Width:**\n\n    -   Wider transactions are typical in denser datasets.\n\n    -   May lead to an increase in the maximal length of frequent itemsets and more traversals of the hash tree, as the number of subsets in a transaction increases with its width.\n\n## Frequent itemset generation: ECLAT {.smaller}\n\nECLAT uses vertical data layout to store a list of transaction ids for each item:\n\n::: columns\n::: {.column width=\"60%\"}\n![](images/eclat.png){width=\"561\"}\n:::\n\n::: {.column width=\"40%\"}\n::: incremental\n-   **Advantage**: very fast support counting\n\n-   **Disadvantage**: intermediate tid-lists may become too large for memory\n:::\n:::\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n::: incremental\n1.  Create the root node (null)\n\n2.  Scan the database, get the frequent itemsets of length 1, and sort these 1-itemsets in decreasing support count.\n\n3.  Read a transaction at a time. Sort items in the transaction acoording to the last step.\n\n4.  For each transaction, insert items to the FP-Tree from the root node and increment occurence record at every inserted node.\n\n5.  Create a new child node if reaching the leaf node before the insersion completes.\n\n6.  If a new child node is created, link it from the last node consisting of the same item.\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 1:** Calculate The Support Count of Each Item in The Dataset\n\n::: panel-tabset\n## Original\n\n|          |                   |\n|----------|-------------------|\n| **Item** | **Support Count** |\n| I1       | 3                 |\n| I2       | 3                 |\n| I3       | 4                 |\n| I4       | 1                 |\n| I5       | 4                 |\n| I6       | 1                 |\n\n## Sorted\n\n|          |                   |\n|----------|-------------------|\n| **Item** | **Support Count** |\n| I3       | 4                 |\n| I5       | 4                 |\n| I1       | 3                 |\n| I2       | 3                 |\n| I4       | 1                 |\n| I6       | 1                 |\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 2:** Reorganize The Items in The Transaction Dataset\n\n|                    |                  |\n|--------------------|------------------|\n| **Transaction ID** | **Items**        |\n| T1                 | `I3, I1, I4`     |\n| T2                 | `I3, I5, I2, I6` |\n| T3                 | `I3, I5, I1, I2` |\n| T4                 | `I5, I2`         |\n| T5                 | `I3, I5, I1`     |\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 3:** Create FP Tree Using the Transaction Dataset\n\nTo construct an FP-Tree for the FP-growth algorithm, perform these steps:\n\n::: incremental\n1.  Initialize a root node labeled as Null.\n\n2.  For each transaction, sorted by item support in descending order:\n\n    -   Start from the root node.\n\n    -   For each item in the transaction:\n\n        -   If a child node with the item exists, increment its count by 1 and proceed to this child.\n\n        -   Otherwise, create a new child node for the item, set its count to 1, and proceed to the new node.\n\n3.  Repeat for all transactions in the dataset.\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T1 to the FP Tree**\n\n::: incremental\n-   Initialize Null root node for the FP-tree.\n\n-   Select first transaction T1: sorted items \\[I3, I1, I4\\].\n\n-   At root, no child for I3 exists; create child node for I3 with count 1.\n:::\n\n::: fragment\n![](images/fp1.png){width=\"361\"}\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T1 to the FP Tree**\n\n::: incremental\n-   Move to node I3 and proceed to next item I1 in transaction.\n\n-   No child node for I1 under I3; create child node for I1 with count 1.\n:::\n\n![](images/fp2.png){width=\"361\"}\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T1 to the FP Tree**\n\n::: incremental\n-   Move to node I1 and next item I4 in transaction.\n\n-   No child node for I4 under I1; create child node for I4 with count 1.\n:::\n\n![](images/fp3.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T2 to the FP Tree**\n\n::: incremental\n-   Complete traversal of T1; move to T2 with items \\[I3, I5, I2, I6\\].\n\n-   At root, find child node with I3; increment its count by 1.\n:::\n\n![](images/fp5.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T2 to the FP Tree**\n\n::: incremental\n-   Move to modified I3 node and next item I5 in T2.\n\n-   No child node for I5 under I3; create child node for I5 with count 1.\n:::\n\n![](images/fp6.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T2 to the FP Tree**\n\n::: incremental\n-   Move to newly created I5 node and next item I2 in transaction.\n\n-   No child node for I2 under I5; create child node for I2 with count 1.\n:::\n\n![](images/fp7.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T2 to the FP Tree**\n\n::: incremental\n-   Move to newly created I2 node and next item I6 in transaction.\n\n-   No child node for I6 under I2; create child node for I6 with count 1.\n:::\n\n![](images/fp8.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T3 to the FP Tree**\n\n::: incremental\n-   Finished traversing T2; start T3 with items \\[I3, I5, I1, I2\\].\n\n-   At root, find I3; increment its count by 1 since it exists.\n:::\n\n![](images/fp9.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T3 to the FP Tree**\n\n::: incremental\n-   Move to modified I3 node and next item I5 in transaction.\n\n-   Find existing child node for I5 under I3; increment I5's count by 1.\n:::\n\n![](images/fp10.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T3 to the FP Tree**\n\n::: incremental\n-   Move to modified I5 node and next item I1 in transaction.\n\n-   No child node for I1 under I5; create child node for I1 with count 1.\n:::\n\n![](images/fp11.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items from Transaction T3 to the FP Tree**\n\n::: incremental\n-   Move to newly created I1 node and next item I2 in transaction.\n\n-   No child node for I2 under I1; create child node for I2 with count 1.\n:::\n\n![](images/fp12.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items From Transaction T4 to the FP Tree**\n\n::: incremental\n-   Starting with I5 at root, check for child node with I5.\n\n-   No child node for I5; create new child node for I5 with count 1 at root.\n:::\n\n![](images/fp13.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Add Items From Transaction T4 to the FP Tree**\n\n::: incremental\n-   Starting with I5 at root, check for child node with I5.\n\n-   No child node for I5; create new child node for I5 with count 1 at root.\n:::\n\n![](images/fp14.png)\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 4**: Create a Pattern Base For All The Items Using The FP-Tree\n\nTo construct pattern bases using the FP-growth algorithm, we analyze the FP-tree to track paths leading to each item, summarizing the findings:\n\n::: incremental\n-   **Pattern Base for I1:** **`{I3}:1`**, **`{I3, I5}:2`**. This reflects paths from I3 to I1 (count 1) and I3 through I5 to I1 (count 2).\n\n-   **Pattern Base for I2:** **`{I3, I5}:1`**, **`{I3, I5, I1}:1`**, **`{I5}:1`**. Represents paths I3-\\>I5-\\>I2, I3-\\>I5-\\>I1-\\>I2 (each with count 1), and directly I5-\\>I2 (count 1).\n\n-   **Pattern Base for I3:** None, as I3 is directly connected to the root.\n\n-   **Pattern Base for I5:** **`{I3}:3`**, indicating the path I3-\\>I5 with count 3.\n\n-   **Pattern Base for I4 and I6:** Not applicable, as their support does not meet the threshold for inclusion in this analysis.\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 4**: Create a Pattern Base For All The Items Using The FP-Tree\n\nThe pattern base for each item in the dataset is tabulated below.\n\n|          |                                    |\n|----------|------------------------------------|\n| **Item** | **Pattern Base**                   |\n| `I1`     | `{I3}:1,{I3, I5}:2`                |\n| `I2`     | `{I3, I5}:1,{I3, I5, I1}:1,{I5}:1` |\n| `I3`     | `{}`                               |\n| `I5`     | `{I3}:3`                           |\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 5:** Create a Conditional FP Tree For Each Frequent Item\n\n-   **Item I1**: In its pattern base, I3 appears with total count 3 ({I3} with count 1 and {I3,I5} with count 2); I5 appears in {I3, I5} with count 2. Conditional FP-tree: {I3:3, I5:2}.\n\n-   **Item I2**: Pattern base shows I3 (total count 2) and I5 (total count 3); I1 (count 1) is dropped for being below minimum support. Conditional FP-tree: {I3:2, I5:3}.\n\n-   **Item I5**: Pattern base shows I3 with count 3. Conditional FP-tree: {I3:3}.\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 5:** Create a Conditional FP Tree For Each Frequent Item\n\nThe conditional fp-tree for each item is tabulated below.\n\n|          |                                    |                         |\n|----------|------------------------------------|-------------------------|\n| **Item** | **Pattern Base**                   | **Conditional FP Tree** |\n| `I1`     | `{I3}:1,{I3, I5}:2`                | `{I3:3, I5:2}`          |\n| `I2`     | `{I3, I5}:1,{I3, I5, I1}:1,{I5}:1` | `{I3:2, I5:3}`          |\n| `I3`     | `{}`                               |                         |\n| `I5`     | `{I3}:3`                           | `{I3:3}`                |\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 6:** Generate Frequent Itemsets From Conditional FP-Trees\n\nAfter constructing the conditional FP-tree, we generate frequent itemsets for each item by combining items in the conditional FP-tree with their counts:\n\n::: incremental\n-   **For I1:** Frequent itemsets are `{I1, I3}:3`, `{I1, I5}:2`, and `{I1, I3, I5}:2`. The support counts are determined by the maximum support of items in the conditional FP-tree, with `{I1, I5}` and `{I1, I3, I5}` limited to 2 due to `I5`'s maximum support count.\n\n-   **For I2:** Frequent itemsets include `{I2, I3}:2`, `{I2, I5}:3`, and `{I2, I3, I5}:2`. Here, the support count of `{I2, I3}` and `{I2, I3, I5}` is restricted to 2 due to `I3`'s support count, while `{I2, I5}` reaches 3, reflecting `I5`'s maximum support in the conditional FP-tree.\n\n-   **For I5:** The only frequent itemset is `{I3, I5}` with a count of 3, as derived from the conditional FP-tree's support counts.\n:::\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n#### **Step 6:** Generate Frequent Itemsets From Conditional FP-Trees\n\nAll the frequent itemsets derived from the conditional fp-trees have been tabulated below.\n\n|          |                                    |                         |                                         |\n|----------|------------------------------------|-------------------------|-----------------------------------------|\n| **Item** | **Pattern Base**                   | **Conditional FP Tree** | **Frequent itemsets**                   |\n| `I1`     | `{I3}:1,{I3, I5}:2`                | `{I3:3, I5:2}`          | `{I1, I3}:3,{I1, I5}:2,{I1, I3, I5}:2`  |\n| `I2`     | `{I3, I5}:1,{I3, I5, I1}:1,{I5}:1` | `{I3:2, I5:3}`          | `{I2, I3}:2, {I2, I5}:3,{I2, I3, I5}:2` |\n| `I5`     | `{I3}:3`                           | `{I3:3}`                | `{I3, I5}:3`                            |\n\n## Frequent itemset generation: FP-Growth {.smaller}\n\n**Step 7:** Generate Association Rules, e.g.:\n\n|                |                                                                                                  |\n|----------------|--------------------------------------------------------------------------------------------------|\n| **Itemset**    | **Association Rules**                                                                            |\n| `{I1}`         | `x`                                                                                              |\n| `{I2}`         | `x`                                                                                              |\n| `{I3}`         | `x`                                                                                              |\n| `{I5}`         | `x`                                                                                              |\n| `{I1,I3}`      | `{I1}->{I3}, {I3}->{I1}`                                                                         |\n| `{I1,I5}`      | `{I1}->{I5}, {I5}->{I1}`                                                                         |\n| `{I2,I3}`      | `{I2}->{I3}, {I3}->{I2}`                                                                         |\n| `{I2,I5}`      | `{I2}->{I5}, {I5}->{I2}`                                                                         |\n| `{I3,I5}`      | `{I3}->{I5}, {I5}->{I3}`                                                                         |\n| `{I2, I3, I5}` | `{I2}->{I3, I5}, {I3}->{I2, I5}, {I5}->{I2, I3}, {I2, I3}->{I5}, {I2, I5}->{I3}, {I3, I5}->{I2}` |\n| `{I1, I3, I5}` | `{I1}->{I3, I5}, {I3}->{I1, I5}, {I5}->{I1, I3}, {I1, I3}->{I5}, {I1, I5}->{I3}, {I3, I5}->{I1}` |\n\n## FP-Growth: applied {.smaller}\n\n::: {#46213587 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Apply FP-Growth to find frequent itemsets\nfrequent_itemsets_fpgrowth = fpgrowth(basket, min_support = 0.01, use_colnames = True)\n\n# The frequent itemsets found by FP-Growth will be the same as those found by Apriori\n# but potentially in a different order and possibly faster depending on the dataset\nprint(\"\\nFrequent Itemsets from FP-Growth:\\n\", frequent_itemsets_fpgrowth.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFrequent Itemsets from FP-Growth:\n     support         itemsets\n0  0.327205          (Bread)\n1  0.029054   (Scandinavian)\n2  0.058320  (Hot chocolate)\n3  0.054411        (Cookies)\n4  0.015003            (Jam)\n```\n:::\n:::\n\n\n## Maximal frequent itemsets {.smaller}\n\nAn itemset is maximal frequent if none of its immediate supersets is frequent:\n\n![](images/max-frequent.png){width=\"712\"}\n\n## Closed itemsets {.smaller}\n\nNone of an itemset's its immediate supersets has the same support as the itemset\n\n-   Can only have smaller support $\\rightarrow$ *see APRIORI principle above*\n\n![](images/closed.png)\n\n## Closed vs. maximal itemsets\n\n![](images/closed-max.png){width=\"980\"}\n\n## Closed & maximal itemsets: applied {.smaller}\n\n::: panel-tabset\n## Setup\n\n::: {#a972e5ac .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nimport time\n\n# Task 1: Compute Frequent Item Set using mlxtend.frequent_patterns\nstart_time = time.time()\nfrequent_itemsets = fpgrowth(basket, min_support = 0.001, use_colnames = True)\nprint('Time to find frequent itemset:', time.time() - start_time, 'seconds')\n\n# Task 2 & 3: Find closed/max frequent itemset using the frequent itemset found in Task 1\n# Initialize lists to store closed and max itemsets\nclosed_itemsets = []\nmax_itemsets = []\n\n# Get all unique support counts\nunique_supports = frequent_itemsets['support'].unique()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime to find frequent itemset: 4.540992021560669 seconds\n```\n:::\n:::\n\n\n## Closed itemsets\n\n::: {#1aa95c23 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Find closed itemsets\nfor support in unique_supports:\n    itemsets_at_support = frequent_itemsets[frequent_itemsets['support'] == support]['itemsets']\n    for itemset in itemsets_at_support:\n        is_closed = not any((itemset < other_itemset) and (itemset != other_itemset) for other_itemset in itemsets_at_support)\n        if is_closed:\n            closed_itemsets.append(itemset)\n            \nprint(f'Found {len(closed_itemsets)} closed itemsets')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 471 closed itemsets\n```\n:::\n:::\n\n\n## Maximal itemsets\n\n::: {#6d38de14 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# Find max itemsets\nfor support in sorted(unique_supports, reverse=True):\n    itemsets_at_or_above_support = frequent_itemsets[frequent_itemsets['support'] >= support]['itemsets']\n    for itemset in itemsets_at_or_above_support:\n        is_max = not any(itemset < other_itemset for other_itemset in itemsets_at_or_above_support if itemset != other_itemset)\n        if is_max and itemset not in max_itemsets:\n            max_itemsets.append(itemset)\n\nprint(f'Found {len(max_itemsets)} max itemsets')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 471 max itemsets\n```\n:::\n:::\n\n\n:::\n\n## Correlations from association {.smaller}\n\n::: {#2b1332ee .cell execution_count=12}\n``` {.python .cell-code}\n# Convert boolean values to integers for correlation analysis\ndf_numeric = basket.astype(int)\n\n# Example calculation (assuming you want to compare 'Cake' and 'Coffee')\ncorr_coefficient, p_value = pearsonr(df_numeric['Cake'], df_numeric['Coffee'])\n\nprint(f\"Pearson Correlation Coefficient between Cake and Coffee: {corr_coefficient:.3f}\")\nprint(f\"P-Value: {p_value:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPearson Correlation Coefficient between Cake and Coffee: 0.033\nP-Value: 0.001\n```\n:::\n:::\n\n\n## Regression analysis {.smaller}\n\n::: {#04c3f4d8 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\n# Prepare the data\nX = df_numeric['Cake']\ny = df_numeric['Coffee']\nX = sm.add_constant(X)  # Adds a constant term to the predictor\n\n# Fit a logistic regression model\nmodel = sm.Logit(y, X).fit()\n\n# Print the model summary\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.691666\n         Iterations 3\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 Coffee   No. Observations:                 9465\nModel:                          Logit   Df Residuals:                     9463\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 20 Aug 2025   Pseudo R-squ.:               0.0007904\nTime:                        11:35:27   Log-Likelihood:                -6546.6\nconverged:                       True   LL-Null:                       -6551.8\nCovariance Type:            nonrobust   LLR p-value:                  0.001290\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.1090      0.022     -5.014      0.000      -0.152      -0.066\nCake           0.2170      0.067      3.215      0.001       0.085       0.349\n==============================================================================\n```\n:::\n:::\n\n\n## Conclusions {.smaller}\n\n::: incremental\n1.  **Association Rule Mining's Versatility**: It uncovers relationships in datasets, useful across retail, bioinformatics, and web mining.\n\n2.  **Strength ≠ Usefulness**: High support and confidence don't guarantee an association rule's relevance; evaluation of novelty and usefulness is critical.\n\n3.  **Advanced Measures for Depth**: Beyond support and confidence, lift, leverage, and conviction offer deeper insights into rule significance.\n\n4.  **Reducing Redundancy**: Mining closed and maximal itemsets streamlines pattern discovery, highlighting the most pertinent itemsets.\n\n5.  **Beyond Co-occurrence**: Correlation analysis reveals the influence between items, adding a layer of understanding to association rules.\n\n6.  **Crucial Data Preparation**: Effective data format and preparation are key to successful mining and statistical analysis.\n\n7.  **Significance vs. Usefulness**: Distinguishing statistical from practical significance is essential; not all statistically significant rules are practically useful.\n\n8.  **Interdisciplinary Insights**: Combining data mining, statistics, and domain knowledge enhances the interpretation and application of findings.\n:::\n\n## Conclusions cont...\n\nThe semester is over!!! 😭\n\n::: fragment\n...but actually 🥳\n:::\n\n::: fragment\n✌🏻\n:::\n\n## In-class Exercise\n\n::: task\nGo to [ex-13](https://{{< var website.url >}}/exercises/ex-13.html) and perform the tasks\n:::\n\n",
    "supporting": [
      "13-association-1_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}