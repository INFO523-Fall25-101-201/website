{
  "hash": "f53fd6665db10d9eb0df63f2ab428b9b",
  "result": {
    "markdown": "---\ntitle: Regressions III\nsubtitle: Lecture 9\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   Final project proposal is due Mon Apr 01, 11:59pm\n\n-   RQ 04 is due Mon Apr 03, 11:59pm\n\n## Setup {.smaller}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code}\n# Import all required libraries\n# Data handling and manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning and preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.kernel_approximation import RBFSampler\n\n# Model selection\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n\n# Statistical models\nimport statsmodels.api as sm\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nimport operator\n\n# Scientific computing\nfrom scipy.interpolate import UnivariateSpline, interp1d, CubicSpline, make_interp_spline\n\n# Design matrices\nfrom patsy import dmatrix\n\n# Generalized Additive Models\nfrom pygam import LinearGAM, s, f\n\n# Set the default style for visualization\nsns.set_theme(style = \"whitegrid\", palette = \"colorblind\")\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"white\")\n```\n:::\n\n\n# Regressions III\n\n## Our data: Tucson daily average temps {.smaller}\n\n::: panel-tabset\n## Read + head\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntucsonTemp = pd.read_csv(\"data/tucsonWeather.csv\", encoding = 'iso-8859-1')[[\"STATION\", \"NAME\", \"DATE\", \"TAVG\"]]\ntucsonTemp['DATE'] = pd.to_datetime(tucsonTemp['DATE'])\ntucsonTemp['DATE_NUMERIC'] = (tucsonTemp['DATE'] - tucsonTemp['DATE'].min()).dt.days\ntucsonTemp.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATION</th>\n      <th>NAME</th>\n      <th>DATE</th>\n      <th>TAVG</th>\n      <th>DATE_NUMERIC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>US1AZPM0322</td>\n      <td>CATALINA 1.6 S, AZ US</td>\n      <td>2023-01-01</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>US1AZPM0322</td>\n      <td>CATALINA 1.6 S, AZ US</td>\n      <td>2023-01-02</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>US1AZPM0322</td>\n      <td>CATALINA 1.6 S, AZ US</td>\n      <td>2023-01-03</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>US1AZPM0322</td>\n      <td>CATALINA 1.6 S, AZ US</td>\n      <td>2023-01-04</td>\n      <td>NaN</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>US1AZPM0322</td>\n      <td>CATALINA 1.6 S, AZ US</td>\n      <td>2023-01-05</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Metadata\n\n| **variable**   | **class** | **description**                      |\n|----------------|-----------|--------------------------------------|\n| `STATION`      | character | Station ID                           |\n| `NAME`         | character | Station name                         |\n| `DATE`         | date      | Date the reading was collected       |\n| `TAVG`         | float     | Daily average temperature            |\n| `DATE_NUMERIC` | int       | Numerical representation of the date |\n\n## Info\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntucsonTemp.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 47731 entries, 0 to 47730\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   STATION       47731 non-null  object        \n 1   NAME          47731 non-null  object        \n 2   DATE          47731 non-null  datetime64[ns]\n 3   TAVG          718 non-null    float64       \n 4   DATE_NUMERIC  47731 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 1.8+ MB\n```\n:::\n:::\n\n\n## Describe\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntucsonTemp.describe()\ntucsonTemp = tucsonTemp[tucsonTemp['TAVG'] > 0]\n```\n:::\n\n\n## Missing values\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntusconTemp = tucsonTemp.dropna()\ntusconTemp.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 718 entries, 6831 to 45728\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   STATION       718 non-null    object        \n 1   NAME          718 non-null    object        \n 2   DATE          718 non-null    datetime64[ns]\n 3   TAVG          718 non-null    float64       \n 4   DATE_NUMERIC  718 non-null    int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 33.7+ KB\n```\n:::\n:::\n\n\n## Visualize\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-7-output-1.png){width=752 height=559}\n:::\n:::\n\n\n:::\n\n## Review: linear models\n\n$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$\n\n::: incremental\n-   $Y_i$: Dependent/response variable\n\n-   $X_i$: Independent/predictor variable\n\n-   $\\beta_0$: y-intercept\n\n-   $\\beta_1$: Slope\n\n-   $\\epsilon_i$: Random error term, deviation of the real value from predicted\n\n-   **Advantage**: Simplicity, interpretability, ease of fitting.\n:::\n\n## Limitations of linear models\n\n::: incremental\n-   **Linearity Assumption:** Assumes a linear relationship between predictors and response.\n\n-   **Lacks Flexibility:** Struggles with non-linear relationships and interactions without explicit feature engineering.\n\n-   **Potential for Underfitting:** May not capture complex data patterns, leading to poor model performance.\n:::\n\n## Motivation for moving beyond linearity\n\n::: incremental\n-   **Improved Accuracy:** Captures non-linear relationships for better predictions.\n\n-   **Complex Data Handling:** Models complex patterns in real-world data.\n\n-   **Interpretability with Non-linearity:** Offers structured ways to understand non-linear effects of predictors.\n\n-   **Balanced Flexibility:** Provides a middle ground between simplicity and the complexity of machine learning models.\n:::\n\n## Polynomial regression {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-8-output-1.png){width=676 height=529}\n:::\n:::\n\n\n## Formula\n\n$Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\dots + \\beta_nX^n + \\epsilon$\n\n**Where**:\n\n::: incremental\n-   $Y$: Dependent/response variable\n\n-   $X$: Independent/predictor variable\n\n-   $X^n$: the $n$th degree polynomial term of $X$.\n\n-   $\\beta_0, \\beta_1, \\beta_2, ... \\beta_n$: the coefficients that the model will estimate\n\n-   $\\epsilon_i$: Random error term, deviation of the real value from predicted\n:::\n\n## Key points\n\n::: incremental\n-   **Non-linear Relationship Modeling:** Incorporates polynomial terms of predictors, enabling the modeling of complex, non-linear relationships between variables.\n\n-   **Flexibility:** The degree of the polynomial ($n$) determines the model's flexibility, with higher degrees allowing for more complex curves.\n\n-   **Overfitting Risk:** Higher-degree polynomials can lead to overfitting, capturing noise rather than the underlying data pattern.\n\n-   **Interpretation:** While polynomial regression can model non-linear relationships, interpreting the coefficients becomes more challenging as the degree of the polynomial increases.\n\n-   **Use Cases:** Suitable for modeling phenomena where the relationship between predictors and response variable is known to be non-linear, such as in biological, agricultural, or environmental data.\n:::\n:::\n\n## Polynomial regression: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nX = tucsonTemp[['DATE_NUMERIC']].values\ny = tucsonTemp['TAVG'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Transforming data for polynomial regression\npoly_features = PolynomialFeatures(degree = 2)  # Adjust degree as necessary\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.transform(X_test)\n\n# Initialize the Linear Regression model\nlinear_reg = LinearRegression()\n\n# Fit the model with polynomial features\nlinear_reg.fit(X_train_poly, y_train)\n\n# Predict on the testing set\ny_pred = linear_reg.predict(X_test_poly)\n\n# Calculate and print the MSE and R-squared\nmse_initial = mean_squared_error(y_test, y_pred)\nr2_initial = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse_initial.round(2)}')\nprint(f'R-squared: {r2_initial.round(2)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 55.6\nR-squared: 0.76\n```\n:::\n:::\n\n\n## Polynomial fit\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\n# Scatter plot of the actual data points\nplt.scatter(X_test, y_test, color='lightgray', label='Actual data')\n\n# To plot the polynomial curve, we need to sort the values because the line plot needs to follow the order of X\n# Create a sequence of values from the minimum to the maximum X values for plotting the curve\nX_plot = np.linspace(np.min(X), np.max(X), 100).reshape(-1, 1)\n\n# Transform the plot data for the polynomial model\nX_plot_poly = poly_features.transform(X_plot)\n\n# Predict y values for the plot data\ny_plot = linear_reg.predict(X_plot_poly)\n\n# Plot the polynomial curve\nplt.plot(X_plot, y_plot, color='red', label='Polynomial fit')\n\n# Labeling the plot\nplt.xlabel('Date Numeric')\nplt.ylabel('Daily Average Temperature')\nplt.title('2nd Degree Polynomial Fit to Tucson Temperature Data')\nplt.legend()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-10-output-1.png){width=820 height=455}\n:::\n:::\n\n\n## Residual plot\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals = y_test - y_pred\nsns.residplot(x = y_pred.ravel(), y = residuals.ravel(), lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Polynomial Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-11-output-1.png){width=821 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning: Polynomial regression {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Exploring different polynomial degrees to find the best fit\ndegrees = [1, 2, 3, 4, 5]\nmse_scores = []\nr2_scores = []\n\nfor degree in degrees:\n    poly_features = PolynomialFeatures(degree = degree)\n    X_train_poly = poly_features.fit_transform(X_train)\n    X_test_poly = poly_features.transform(X_test)\n    \n    model = LinearRegression()\n    model.fit(X_train_poly, y_train)\n    \n    y_pred = model.predict(X_test_poly)\n    mse_scores.append(mean_squared_error(y_test, y_pred))\n    r2_scores.append(r2_score(y_test, y_pred))\n\n# Display the MSE and R-squared for each degree\nfor degree, mse, r2 in zip(degrees, mse_scores, r2_scores):\n    print(f'Degree: {degree}, MSE: {mse.round(3)}, R-squared: {r2.round(4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDegree: 1, MSE: 222.653, R-squared: 0.0444\nDegree: 2, MSE: 55.597, R-squared: 0.7614\nDegree: 3, MSE: 38.106, R-squared: 0.8365\nDegree: 4, MSE: 26.386, R-squared: 0.8868\nDegree: 5, MSE: 26.067, R-squared: 0.8881\n```\n:::\n:::\n\n\n## Re-evaluate model\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Selecting the best degree based on previous step\nbest_degree = degrees[np.argmin(mse_scores)]\n\n# Transforming data with the best degree\npoly_features_best = PolynomialFeatures(degree=best_degree)\nX_train_poly_best = poly_features_best.fit_transform(X_train)\nX_test_poly_best = poly_features_best.transform(X_test)\n\n# Fitting the model again\nbest_model = LinearRegression()\nbest_model.fit(X_train_poly_best, y_train)\n\n# New predictions\ny_pred_best = best_model.predict(X_test_poly_best)\n\n# Calculate new MSE and R-squared\nmse_best = mean_squared_error(y_test, y_pred_best)\nr2_best = r2_score(y_test, y_pred_best)\nprint(f'Best Degree: {best_degree}, MSE: {mse_best.round(3)}, R-squared: {r2_best.round(4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Degree: 5, MSE: 26.067, R-squared: 0.8881\n```\n:::\n:::\n\n\n## Polynomial fits\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\n# Generate a sequence of X values for plotting\nX_range = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)\n\n# Plot the actual data\nplt.scatter(X_test, y_test, color='gray', alpha=0.5, label='Actual data')\n\ncolors = ['blue', 'green', 'red', 'purple', 'orange']\nlabels = ['1st degree', '2nd degree', '3rd degree', '4th degree', '5th degree']\n\nfor i, degree in enumerate(degrees):\n    # Create polynomial features for the current degree\n    poly_features = PolynomialFeatures(degree=degree)\n    X_train_poly = poly_features.fit_transform(X_train)\n    X_test_poly = poly_features.transform(X_test)\n    X_range_poly = poly_features.transform(X_range)\n    \n    # Fit the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train)\n    \n    # Predict over the generated range of X values\n    y_range_pred = model.predict(X_range_poly)\n    \n    # Plot\n    plt.plot(X_range, y_range_pred, color=colors[i], label=f'Polynomial fit degree {degree}')\n\n# Enhancing the plot\nplt.xlabel('Date Numeric')\nplt.ylabel('Daily Average Temperature')\nplt.title('Comparing Polynomial Fits of Different Degrees')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-14-output-1.png){width=820 height=455}\n:::\n:::\n\n\n## Residuals\n\n::: {.cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_best = y_test - y_pred_best\nplt.scatter(y_pred_best, residuals_best, alpha = 0.5)\nplt.axhline(y = 0, color = 'r', linestyle = '--')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residual Plot with Best Degree')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-15-output-1.png){width=821 height=455}\n:::\n:::\n\n\n:::\n\n## Step functions {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-16-output-1.png){width=820 height=529}\n:::\n:::\n\n\n## Formula\n\n$f(x) = \\beta_0 + \\beta_1 I(x \\in C_1) + \\beta_2 I(x \\in C_2) + \\dots + \\beta_k I(x \\in C_k)$\n\n**Where**\n\n::: incremental\n-   $I$: indicator function that returns 1 if $x$ is within the interval $C_i$ and 0\n\n-   $\\beta_0, \\beta_1, \\beta_2, … \\beta_k$: value of the response variable $Y$ within interval $C_i$\n:::\n\n## Key points\n\n::: incremental\n-   **Model Non-linearity:** Efficiently captures non-linear relationships by assigning constant values within specific intervals of the predictor variable.\n\n-   **Simple Interpretation:** Each step's effect is straightforward, with a constant response value within each interval.\n\n-   **Adjustable Steps:** Flexibility in setting the number and boundaries of steps, although optimal placement may require exploratory data analysis or domain knowledge.\n\n-   **Discontinuity Handling:** Can model sudden jumps in the response variable, a feature not readily handled by smooth models like polynomials or splines.\n:::\n:::\n\n## Step functions: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\n# Define the step function intervals\nstep_intervals = [0, 100, 200, 300, 400]\ntucsonTemp['step_bins'] = pd.cut(tucsonTemp['DATE_NUMERIC'], bins=step_intervals, labels=False)\n\n# Prepare the data for step function fitting\nX = pd.get_dummies(tucsonTemp['step_bins'], drop_first=True).values\ny = tucsonTemp['TAVG'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the step function model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Calculate and print the MSE and R-squared\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 53.23\nR-squared: 0.77\n```\n:::\n:::\n\n\n## Step function fit\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# Plotting the step function fit\nsns.lineplot(x = tucsonTemp['DATE_NUMERIC'], y = tucsonTemp['TAVG'], label='Actual Data', alpha = 0.6)\n\n# Generate the step function values for the plot\nfor i, (lower, upper) in enumerate(zip(step_intervals[:-1], step_intervals[1:])):\n    mask = (tucsonTemp['DATE_NUMERIC'] >= lower) & (tucsonTemp['DATE_NUMERIC'] < upper)\n    plt.hlines(y[mask].mean(), lower, upper, colors='red', label=f'Step {i+1}')\n\nplt.xlabel('Date Numeric')\nplt.ylabel('Daily Average Temperature')\nplt.title('Step Function Fit to Tucson Temperature Data')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-18-output-1.png){width=820 height=455}\n:::\n:::\n\n\n:::\n\n## Basis functions {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-19-output-1.png){width=676 height=529}\n:::\n:::\n\n\n## Formula\n\n$y(x) = \\sum_{j=0}^{M} w_j \\phi_j(x)$\n\n**Where**:\n\n::: incremental\n-   $y(x)$: the predicted value\n\n-   $\\phi_j(x)$: the $j$th basis function\n\n-   $w_j$: weight (or coefficient) for the $j$th basis function\n\n-   $M$: number of basis functions used in the model\n:::\n\nFor Gaussian basis functions, each $\\phi_j(x)$ could be defined as:\\\n$\\phi_j(x) = \\exp\\left(-\\frac{(x - \\mu_j)^2}{2s^2}\\right)$\n\n**Where**:\n\n::: incremental\n-   $\\mu_j$ is the center of the $j$th basis function\n\n-   $s$ is the width\n:::\n\n## Key points\n\n::: incremental\n-   **Flexibility:** Basis functions, such as polynomials, splines, or Gaussians, allow modeling of complex non-linear relationships.\n\n-   **Transformations:** They transform the input data into a new space where linear regression techniques can be applied.\n\n-   **Customizability:** The choice and parameters of the basis functions can be adapted to the problem, often requiring domain knowledge or model selection techniques.\n\n-   **Overfitting Potential:** More basis functions can lead to greater model complexity, which increases the risk of overfitting, hence the need for regularization.\n\n-   **Computational Efficiency:** While basis functions can provide powerful models, they may introduce computational complexity, especially when the number of basis functions is large.\n:::\n:::\n\n## Basis function regression: Applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\n# Splitting the dataset\nX = tucsonTemp[['DATE_NUMERIC']].values\ny = tucsonTemp['TAVG'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Create the basis function model\nmodel = make_pipeline(RBFSampler(gamma = 1.0, n_components=50, random_state=42), LinearRegression())\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Predict on the testing set\ny_pred = model.predict(X_test)\n\n# Calculate and print the MSE and R-squared\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 93.76\nR-squared: 0.60\n```\n:::\n:::\n\n\n## Basis function fit\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\n# Predict on a grid for a smooth line\nnp.random.seed(42)\nnum_samples = 365\nx_grid = np.linspace(X.min(), X.max(), num_samples).reshape(-1, 1)\ny_grid_pred = model.predict(x_grid)\n\n# Plotting the actual data points and the basis function fit\nsns.lineplot(x = X_train.squeeze(), y = y_train, color = 'gray', label = 'Training data', alpha = 0.5)\nsns.lineplot(x = X_test.squeeze(), y = y_test, color = 'blue', label = 'Testing data', alpha = 0.5)\nsns.lineplot(x = x_grid.squeeze(), y = y_grid_pred, color = 'red', label = 'Basis Function Fit')\n\n# Labeling the plot\nplt.xlabel('Normalized Date Numeric')\nplt.ylabel('Daily Average Temperature')\nplt.title('Basis Function Fit to Tucson Temperature Data')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-21-output-1.png){width=820 height=455}\n:::\n:::\n\n\n## Residual plot\n\n::: {.cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\n# Calculating residuals\nresiduals = y_test - y_pred\n\n# Plotting residuals\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Basis Function Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-22-output-1.png){width=821 height=455}\n:::\n:::\n\n\n:::\n\n## Aside: Grid search cross-validation {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/single-grid.jpg){fig-align=\"center\" width=\"364\"}\n\n## Formula\n\n$CV_{\\text{score}} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{score}_i$\n\n**Where**:\n\n-   $k$ is the number of folds in the cross-validation.\n\n-   $score_i$​ is the score obtained from the �i-th fold.\n\n-   $CV_{score}$​ is the cross-validation score, which is the average of all the individual fold scores.\n\n## Pros + cons\n\n**Pros**\n\n::: incremental\n-   **Systematic Exploration:** It ensures that every combination in the specified parameter range is evaluated.\n\n-   **Reproducibility:** The search is deterministic, meaning it will produce the same result each time for the same dataset and parameters.\n:::\n\n**Cons**\n\n::: incremental\n-   **Computational Cost:** It can be very computationally expensive, especially with a large number of hyperparameters or when the range of values for each hyperparameter is large.\n\n-   **Dimensionality:** The time required increases exponentially with the addition of more parameters (known as the curse of dimensionality).\n:::\n\n## Also...\n\nHere, we identify an area where the model performs well, then launch a second grid:\n\n![](images/grid-cv.jpeg){fig-align=\"center\" width=\"708\"}\n:::\n\n## Aside: random search {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/single-rand.jpeg){fig-align=\"center\" width=\"528\"}\n\n## Formula\n\n$\\text{Given:} \\quad \\text{Hyperparameters Space} = \\{ H_1, H_2, …, H_n \\}$\n\n$\\text{Randomly sample } p \\text{ sets of hyperparameters: } \\{ h_{1_p}, h_{2_p}, …, h_{n_p} \\}$\n\n$\\text{For each set of hyperparameters, compute:} \\quad CV_{\\text{score}_p} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{score}_{i_p}$\n\n$\\text{Select the set with the best } CV_{\\text{score}}.$\n\n::: incremental\n-   $H_1, H_2, … H_n$ ​ represent the range of hyperparameters being considered.\n\n-   $p$ is the number of parameter sets sampled in the random search.\n\n-   $h_{1_p}, h_{2_p}, ... h_{n_p}$​​ is one of the randomly sampled sets of hyperparameters.\n\n-   $k$ is the number of folds in cross-validation.\n\n-   $score_{1_p}$ is the evaluation score of the $i$-th fold using the $p$-th set of hyperparameters.\n\n-   $CV_{score_p}$ ​​ is the cross-validation score for the $p$-th set of hyperparameters.\n:::\n\n## Pros + cons\n\n**Pros:**\n\n::: incremental\n-   **Computational Efficiency:** Less resource-intensive than exhaustive grid search.\n\n-   **Exploratory:** Can discover good hyperparameters without testing every possible combination.\n\n-   **Diverse Sampling:** May explore unexpected areas of the hyperparameter space.\n:::\n\n**Cons:**\n\n::: incremental\n-   **Potentially Incomplete:** Might miss the optimal hyperparameters due to non-exhaustive sampling.\n\n-   **Less Reproducible:** Results can vary with different random seeds.\n\n-   **Less Consistent:** Might require more iterations to match the precision of grid search.\n:::\n:::\n\n## Best of both worlds? {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/random-grid.jpeg){fig-align=\"center\" width=\"661\"}\n\n## Pros + cons\n\n**Pros**\n\n::: incremental\n-   **Quick Exploration**: Random search identifies promising hyperparameter regions rapidly.\n\n-   **Targeted Refinement**: Grid search then efficiently fine-tunes within these regions.\n\n-   **Computational Savings**: Less resource-intensive than a full grid search.\n\n-   **Strategic Balance**: Combines broad exploration with detailed exploitation.\n:::\n\n**Cons**\n\n::: incremental\n-   **Complex Setup**: More steps involved than using a single search method.\n\n-   **Potentially Time-Consuming**: Can be slower than random search alone.\n\n-   **Missed Global Optima**: Initial random search may overlook the best hyperparameter areas.\n\n-   **Resource Management**: Needs careful distribution of computational effort.\n:::\n:::\n\n## Model tuning: Basis function regression {.smaller}\n\n::: panel-tabset\n## Applied\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\n# First search: random Search to narrow down the range for hyperparameters\nrandom_param_grid = {\n    'rbfsampler__gamma': np.logspace(-3, 0, 4),  # Wider range for gamma\n    'rbfsampler__n_components': np.linspace(50, 500, 10).astype(int)  # Wider range for n_components\n}\n\n# Create a custom scorer for cross-validation\nmse_scorer = make_scorer(mean_squared_error, greater_is_better = False)\n\n\n# Initialize RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    model,\n    param_distributions = random_param_grid,\n    n_iter = 10,  # Number of parameter settings that are sampled\n    scoring = mse_scorer,\n    cv = 5,\n    random_state = 42\n)\n\n# Fit the model\nrandom_search.fit(X_train, y_train)\n\n# Second search: grid Search to fine-tune the hyperparameters\n# Use best parameters from random search as the center point for the grid search\nbest_gamma = random_search.best_params_['rbfsampler__gamma']\nbest_n_components = random_search.best_params_['rbfsampler__n_components']\nrefined_param_grid = {\n    'rbfsampler__gamma': np.linspace(best_gamma / 2, best_gamma * 2, 5),\n    'rbfsampler__n_components': [best_n_components - 50, best_n_components, best_n_components + 50]\n}\n\n# Initialize GridSearchCV with the refined grid\ngrid_search = GridSearchCV(\n    model,\n    param_grid = refined_param_grid,\n    scoring = mse_scorer,\n    cv = 5\n)\n\n# Fit the model using GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Best parameters from grid search\nprint(f'Best parameters after Grid Search: {grid_search.best_params_}')\n\n# Re-initialize and fit the model with the best parameters from grid search\nbest_basis_model = make_pipeline(\n    RBFSampler(\n        gamma=grid_search.best_params_['rbfsampler__gamma'],\n        n_components = grid_search.best_params_['rbfsampler__n_components'],\n        random_state = 42\n    ),\n    LinearRegression()\n)\nbest_basis_model.fit(X_train, y_train)\n\n# Make new predictions with the best model\ny_pred_best = best_basis_model.predict(X_test)\n\n# Calculate R-squared and Mean Squared Error (MSE) with the best model\nr2_best = r2_score(y_test, y_pred_best)\nmse_best = mean_squared_error(y_test, y_pred_best)\nprint(f'Mean Squared Error: {mse_best.round(3)}')\nprint(f'R-squared: {r2_best.round(4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters after Grid Search: {'rbfsampler__gamma': 0.002, 'rbfsampler__n_components': 250}\nMean Squared Error: 20.303\nR-squared: 0.9129\n```\n:::\n:::\n\n\n## Compare with previous model\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# Calculate the original MSE and r-squared scored\nmse_initial = mean_squared_error(y_test, y_pred)\nr2_initial = r2_score(y_test, y_pred)\n\n# Print comparison\nprint(f'Initial MSE: {mse_initial.round(3)}, Best Parameters MSE: {mse_best.round(3)}')\nprint(f'Initial R-squared: {r2_initial.round(4)}, Best Parameters R-squared: {r2_best.round(5)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial MSE: 93.762, Best Parameters MSE: 20.303\nInitial R-squared: 0.5976, Best Parameters R-squared: 0.91287\n```\n:::\n:::\n\n\n## Residuals\n\n::: {.cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\nresiduals_best = y_test - y_pred_best\nsns.residplot(x = y_pred_best, y = residuals_best, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residual Plot with Best Parameters')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-25-output-1.png){width=821 height=455}\n:::\n:::\n\n\n:::\n\n## Piecewise polynomials {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=25}\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-26-output-1.png){width=812 height=529}\n:::\n:::\n\n\n## Formula\n\n$$\ny(x) = \\begin{cases} a_0 + a_1 x + a_2 x^2 + \\dots + a_n x^n & \\text{for } x \\in [x_0, x_1) \\\\b_0 + b_1 x + b_2 x^2 + \\dots + b_m x^m & \\text{for } x \\in [x_1, x_2) \\\\\\vdots \\\\z_0 + z_1 x + z_2 x^2 + \\dots + z_p x^p & \\text{for } x \\in [x_{k-1}, x_k]\\end{cases}\n$$\n\n**Where:**\n\n::: incremental\n-   $y(x)$: the output of the piecewise polynomial function at input $x$\n\n-   $a_0, a_1, \\dots, a_n, b_0, b_1, \\dots, b_n, ..., z_0, z_1, \\dots, z_n$: coefficients specific to each polynomial piece within its respective interval\n\n-   $[x_0, x_1), [x_1, x_2),...,[x_{k-1}, x_k]$: intervals that divide the range $x$ into segments, each with a specific polynomial.\n\n    -   Defined by knots $x_0, x_1,...,x_k$, or points where the polynomial changes\n:::\n\n## Key points\n\n::: incremental\n-   **Flexibility**: Allows for modeling different behaviors of the response variable across the range of the predictor variable.\n\n-   **Customizable**: The number and location of knots (points where the polynomial changes) can be adjusted based on the data or domain knowledge.\n\n-   **Continuity**: While the model can change form at each knot, continuity can be enforced to ensure the function is smooth at the transitions.\n\n-   **Complexity Control**: The degree of the polynomial for each segment can be chosen to balance the model's flexibility with the risk of overfitting.\n\n-   **Interpretation**: While more complex than a single polynomial model, piecewise polynomials can offer intuitive insights into changes in trends or behaviors within different regions of the data.\n:::\n:::\n\n## Piecewise polynomial regression: Applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\n# Assign variables\nX = tucsonTemp[['DATE_NUMERIC']].values\ny = tucsonTemp['TAVG'].values\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Ensure X_train and y_train are sorted by X_train\nsorted_indices = np.argsort(X_train.squeeze())\nX_train_sorted = X_train[sorted_indices].squeeze()  # Convert to 1D\ny_train_sorted = y_train[sorted_indices]\n\n# Define initial knots based on domain knowledge or quantiles\ninitial_knots = np.quantile(X_train_sorted, [0.25, 0.5, 0.75])\n\n# Adjust knots to include the full range of X_train_sorted\nknots = np.concatenate(([X_train_sorted.min()], initial_knots, [X_train_sorted.max()]))\n\n# Function to handle duplicates by averaging y values for duplicate x values\ndef unique_with_average(X, Y):\n    unique_X, indices = np.unique(X, return_inverse=True)\n    avg_Y = np.array([Y[indices == i].mean() for i in range(len(unique_X))])\n    return unique_X, avg_Y\n\nsplines = []\n\nfor i in range(len(knots) - 1):\n    mask = (X_train_sorted >= knots[i]) & (X_train_sorted < knots[i+1])\n    X_segment = X_train_sorted[mask]\n    y_segment = y_train_sorted[mask]\n    \n    # Ensure X_segment is strictly increasing by handling duplicates\n    X_segment_unique, y_segment_avg = unique_with_average(X_segment, y_segment)\n    \n    if len(X_segment_unique) >= 4:  # Ensuring there are enough points\n        spline = CubicSpline(X_segment_unique, y_segment_avg, bc_type='natural')\n        splines.append((knots[i], knots[i+1], spline))\n\n# Predict function needs updating to loop over splines correctly\ndef predict_with_splines(X, splines, knots):\n    y_pred = np.zeros_like(X)\n    for i, x_val in enumerate(X):\n        for start, end, spline in splines:\n            if start <= x_val < end:\n                y_pred[i] = spline(x_val)\n                break\n    return y_pred\n\n# Predictions and evaluations\ny_pred = predict_with_splines(X_test.squeeze(), splines, knots)  # Ensure X_test is 1D for the function\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R-squared: {r2:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 16.80\nR-squared: 0.93\n```\n:::\n:::\n\n\n## Piecewise polynomial fit\n\n::: {.cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\n# Visualization of the training and testing data\nsns.scatterplot(x = X_train.squeeze(), y = y_train, color = 'gray', label = 'Training data', alpha = 0.5)\nsns.scatterplot(x = X_test.squeeze(), y = y_test, color = 'blue', label = 'Testing data', alpha = 0.5)\n\n# Assuming splines and knots are correctly calculated as per previous steps\nx_range = np.linspace(X.min(), X.max(), 400).squeeze()\ny_range_pred = predict_with_splines(x_range, splines, knots)\n\nsns.lineplot(x = x_range, y = y_range_pred, color = 'red', label = 'Piecewise Polynomial Fit')\nplt.xlabel('Date Numeric')\nplt.ylabel('Daily Average Temperature')\nplt.title('Piecewise Polynomial Fit to Temperature Data')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-28-output-1.png){width=820 height=455}\n:::\n:::\n\n\n## Residual plot\n\n::: {.cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\n# Calculating residuals\nresiduals = y_test - predict_with_splines(X_test.squeeze(), splines, knots)\n\n# Plotting residuals\nsns.residplot(x = y_pred, y = residuals, lowess = True, line_kws = {'color': 'red', 'lw': 1})\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Piecewise Polynomial Regression')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-regn-3_files/figure-revealjs/cell-29-output-1.png){width=825 height=455}\n:::\n:::\n\n\n:::\n\n## Model tuning\n\nPretty advanced, so we will skip this one.\n\n**Methods include:**\n\n::: incremental\n1.  **Understanding data + domain knowledge**\n2.  **Adaptive algorithms**\n    1.  Recursive Partitioning and Amalgamation (RECPAM)\n    2.  Cross-validation\n3.  **Regularization**\n    1.  Ridge / Lasso regression\n4.  **Gradient descent for fine-tuning**\n:::\n\n## Other regressions {.smaller}\n\n::: incremental\n-   **Local regression (LOESS/LOWESS)**\n\n    -   **Adaptive and Robust:** Captures variable trends with minimal assumptions, adapting to data changes and resistant to outliers.\n\n-   **Generalized Additive Models (GAMs)**\n\n    -   **Flexible and Interpretable:** Extends GLMs to include non-linear trends through additive, interpretable components, supporting various distributions.\n:::\n\n## Tree-based regressions {.smaller}\n\n**See** [ISL Chapter 8.2](/readings/ISL_chp8.2.pdf)\n\n-   **Bagging**\n\n    ::: incremental\n    -   **Robust and Reducing Overfitting:** Effectively lowers variance and enhances stability in models prone to overfitting.\n    :::\n\n-   **Random Forests**\n\n    -   **Feature Insight and Stability:** Enhances prediction stability and offers valuable insights into feature importance.\n\n-   **Boosting**\n\n    -   **Precision and Performance:** Amplifies the accuracy of weak learners, significantly boosting overall model performance.\n\n-   **Bayesian Additive Regression Trees (BART)**\n\n    -   **Probabilistic and Nuanced:** Delivers detailed probabilistic interpretations, suitable for intricate modeling challenges.\n\n## Conclusions {.smaller}\n\n## Table\n\n| Model                                          | MSE       | $R^{2}$  |\n|------------------------------------------------|-----------|----------|\n| Polynomial regression (best degree)            | 26.067    | 0.8881   |\n| Step functions                                 | 53.23     | 0.77     |\n| Basis functions (best $\\gamma$, \\# components) | 20.303    | 0.9129   |\n| **Piecewise polynomial (untuned)**             | **16.80** | **0.93** |\n\n::: fragment\n**Occam's Razor loses!..**\n:::\n\n",
    "supporting": [
      "09-regn-3_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}