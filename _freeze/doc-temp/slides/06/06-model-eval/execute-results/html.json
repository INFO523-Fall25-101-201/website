{
  "hash": "f00b15cc05e4f05f0ea1d1ed78871c59",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Classification II + Model Evaluation\nsubtitle: Lecture 6\nauthor: \"{{< var slides.author >}}\"\ninstitute: \"{{< var slides.institute >}}\"\nfooter: \"{{< var slides.footer >}}\"\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   HW 03 is due Wed, Mar 20, 11:59pm\n\n-   RQ 3 is due Wed, Mar 20, 11:59pm\n\n## Setup {.smaller}\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Import all required libraries\n# Data handling and manipulation\nimport pandas as pd\nimport numpy as np\n\n# Machine learning models\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nimport mord as m\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\n# Model evaluation and validation methods\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import resample\n\n# Metrics for model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\n# Utility for data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import label_binarize\n\n# For advanced visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Set Seaborn theme\nsns.set_theme(style = \"white\")\n```\n:::\n\n\n# Model Evaluation\n\n## Looking back\n\n**Can we [predict high-risk credit individuals]{.underline} by their [financial traits]{.underline}?**\n\n![](images/credit-risk.jpeg){fig-align=\"center\" width=\"1200\"}\n\n::: aside\nSource: [Lending Club](https://www.lendingclub.com/info/statistics.action)\n:::\n\n## What did we do?\n\n::: incremental\n-   Used several models\n\n-   Compared and contrasted efficacy\n\n-   Concluded that we couldn't predict credit risk\n:::\n\n## Refresher on our data (with ordinal risk) {.smaller}\n\n::: {#4d5bd3c3 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nloans_class = pd.read_csv(\"data/loans_class.csv\")\n\nloans_class.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emp_title</th>\n      <th>emp_length</th>\n      <th>state</th>\n      <th>homeownership</th>\n      <th>annual_income</th>\n      <th>verified_income</th>\n      <th>debt_to_income</th>\n      <th>delinq_2y</th>\n      <th>earliest_credit_line</th>\n      <th>inquiries_last_12m</th>\n      <th>...</th>\n      <th>issue_month</th>\n      <th>loan_status</th>\n      <th>initial_listing_status</th>\n      <th>disbursement_method</th>\n      <th>balance</th>\n      <th>paid_total</th>\n      <th>paid_principal</th>\n      <th>paid_interest</th>\n      <th>paid_late_fees</th>\n      <th>risk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>global config engineer</td>\n      <td>3.0</td>\n      <td>NJ</td>\n      <td>MORTGAGE</td>\n      <td>90000.0</td>\n      <td>Verified</td>\n      <td>18.01</td>\n      <td>0</td>\n      <td>2001</td>\n      <td>6</td>\n      <td>...</td>\n      <td>Mar-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>27015.86</td>\n      <td>1999.33</td>\n      <td>984.14</td>\n      <td>1015.19</td>\n      <td>0.0</td>\n      <td>Medium Risk</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>warehouse office clerk</td>\n      <td>10.0</td>\n      <td>HI</td>\n      <td>RENT</td>\n      <td>40000.0</td>\n      <td>Not Verified</td>\n      <td>5.04</td>\n      <td>0</td>\n      <td>1996</td>\n      <td>1</td>\n      <td>...</td>\n      <td>Feb-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>4651.37</td>\n      <td>499.12</td>\n      <td>348.63</td>\n      <td>150.49</td>\n      <td>0.0</td>\n      <td>Medium Risk</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>assembly</td>\n      <td>3.0</td>\n      <td>WI</td>\n      <td>RENT</td>\n      <td>40000.0</td>\n      <td>Source Verified</td>\n      <td>21.15</td>\n      <td>0</td>\n      <td>2006</td>\n      <td>4</td>\n      <td>...</td>\n      <td>Feb-2018</td>\n      <td>Current</td>\n      <td>fractional</td>\n      <td>Cash</td>\n      <td>1824.63</td>\n      <td>281.80</td>\n      <td>175.37</td>\n      <td>106.43</td>\n      <td>0.0</td>\n      <td>Medium Risk</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>customer service</td>\n      <td>1.0</td>\n      <td>PA</td>\n      <td>RENT</td>\n      <td>30000.0</td>\n      <td>Not Verified</td>\n      <td>10.16</td>\n      <td>0</td>\n      <td>2007</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Jan-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>18853.26</td>\n      <td>3312.89</td>\n      <td>2746.74</td>\n      <td>566.15</td>\n      <td>0.0</td>\n      <td>Low Risk</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>security supervisor</td>\n      <td>10.0</td>\n      <td>CA</td>\n      <td>RENT</td>\n      <td>35000.0</td>\n      <td>Verified</td>\n      <td>57.96</td>\n      <td>0</td>\n      <td>2008</td>\n      <td>7</td>\n      <td>...</td>\n      <td>Mar-2018</td>\n      <td>Current</td>\n      <td>whole</td>\n      <td>Cash</td>\n      <td>21430.15</td>\n      <td>2324.65</td>\n      <td>1569.85</td>\n      <td>754.80</td>\n      <td>0.0</td>\n      <td>Medium Risk</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 51 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Our preprocessing\n\n::: {#8a4a7e69 .cell execution_count=3}\n``` {.python .cell-code}\n# Encode categorical variables\ncategorical_columns = loans_class.select_dtypes(include = ['object', 'category']).columns.tolist()\n\n# Encode categorical variables\nlabel_encoders = {col: LabelEncoder() for col in categorical_columns}\nfor col in categorical_columns:\n    loans_class[col] = label_encoders[col].fit_transform(loans_class[col])\n    \n# Define features and target\nX = loans_class.drop('risk', axis = 1)\ny = loans_class['risk']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Reduce dimensionality to prevent overfitting\npca = PCA(n_components = 2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n```\n:::\n\n\n## Reminder: decision boundary\n\n::: {#14ab440c .cell execution_count=4}\n``` {.python .cell-code}\ndef decisionplot(model, X, y, resolution=216):\n    # Split the data into features (X) and the class variable (y)\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n                         np.linspace(y_min, y_max, resolution))\n\n    # Predict outcomes for each point on the grid\n    if isinstance(model, LinearDiscriminantAnalysis):\n        # For LDA, we need to use the decision_function method\n        Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    else:\n        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    \n    if isinstance(model, LinearDiscriminantAnalysis):\n    # Reshape LDA decision function output appropriately\n        Z = Z.reshape(-1, 1)\n    else:\n        Z = Z.reshape(xx.shape)\n\n    # Plot the actual data points\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='k', s=20)\n\n    # Overlay the decision boundary\n    plt.contourf(xx, yy, Z, alpha = 0.5)\n    \n    # Calculate the accuracy\n    predictions = model.predict(X)\n    acc = accuracy_score(y, predictions)\n    \n  \n    # Set labels for axes\n    plt.xlabel(X.columns[0])\n    plt.ylabel(X.columns[1])\n\n    plt.show()\n```\n:::\n\n\n## What models will we reuse? {.smaller}\n\n::: panel-tabset\n## Ordinal Logistic Regression\n\n::: {#f7fd4807 .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\nOrdinal Logistic Regression Accuracy: 0.549\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06-model-eval_files/figure-revealjs/cell-6-output-2.png){width=825 height=450}\n:::\n:::\n\n\n## K-Nearest Neighbors\n\n::: {#160653fa .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](06-model-eval_files/figure-revealjs/cell-7-output-1.png){width=825 height=450}\n:::\n:::\n\n\n## Decision Trees\n\n::: {#e0ff0491 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](06-model-eval_files/figure-revealjs/cell-8-output-1.png){width=825 height=450}\n:::\n:::\n\n\n:::\n\n## Model Evaluation {.smaller}\n\n> **Model selection** is the task of selecting a model from among various candidates on the basis of performance criterion to choose the best one. In the context of [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") and more generally [statistical analysis](https://en.wikipedia.org/wiki/Statistical_analysis \"Statistical analysis\"), this may be the selection of a [statistical model](https://en.wikipedia.org/wiki/Statistical_model \"Statistical model\") from a set of candidate models, given data. Typically, [Occam's Razor](https://en.wikipedia.org/wiki/Occam%E2%80%99s_Razor#Science_and_the_scientific_method) is the best approach\n\n::: fragment\n#### Broadly we will focus on two categories\n:::\n\n::: fragment\n**Evaluating performance**\n\n-   The process of assessing the performance of a machine learning model using various metrics, such as accuracy, precision, recall, and F1 score, to determine how effectively it makes predictions on new, unseen data.\n:::\n\n::: fragment\n**Validation methods**\n\n-   The technique of verifying a machine learning model's performance and reliability on a separate dataset (validation set) that was not used during the model's training, to ensure that it generalizes well to new data.\n:::\n\n## Evaluating performance\n\n**Back to Confusion Matrices**\n\n![](images/confusion.jpeg){fig-align=\"center\" width=\"1067\"}\n\n## Accuracy {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/accuracy.png){fig-align=\"left\" width=\"612\"}\n\n**Error Rate** $= \\frac{FP + FN}{TP + TN + FP + FN}$\n\n## Pros + Cons\n\n::: fragment\n**Definition**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n:::\n\n::: fragment\n**Use Case**: Best for balanced datasets where each class is approximately equally represented.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Intuitive and easy to understand.\n\n-   Useful when the costs of false positives and false negatives are similar.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Can be misleading in imbalanced datasets (where one class significantly outnumbers the other).\n-   Doesn't consider the type of errors (false positives vs. false negatives).\n:::\n:::\n:::\n\n## Precision {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/precision.png){fig-align=\"left\" width=\"612\"}\n\n**Error Rate** $= \\frac{TP}{TP + FP}$\n\n## Pros + Cons\n\n::: fragment\n**Definition**: The ratio of true positives to the sum of true and false positives. It's a measure of a classifier's exactness.\n:::\n\n::: fragment\n**Use Case**: Important when the cost of false positives is high (e.g., in spam detection).\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Focuses on the positive class's predictive power.\n\n-   Useful in imbalanced datasets to evaluate the performance of the minority class.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Does not consider false negatives (i.e., how many positive cases were missed).\n:::\n:::\n:::\n\n## Recall (Sensitivity) {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/recall.png){fig-align=\"left\" width=\"612\"}\n\n**Error Rate** $= \\frac{TP}{TP + FN}$\n\n## Pros + Cons\n\n::: fragment\n**Definition**: The ratio of true positives to the sum of true positives and false negatives. It's a measure of a classifier's completeness.\n:::\n\n::: fragment\n**Use Case**: Crucial when the cost of false negatives is high (e.g., in disease screening).\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Ensures that most positive examples are correctly recognized.\n\n-   Very useful in imbalanced datasets for evaluating how well the minority class is being predicted\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Does not consider false positives (i.e., can lead to many false alarms).\n:::\n:::\n:::\n\n## F1 Score {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/f1-score.png){fig-align=\"left\" width=\"612\"}\n\n**Error Rate** $= 2\\times\\frac{precision \\times recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}$\n\n## Pros + Cons\n\n::: fragment\n**Definition**: The harmonic mean of precision and recall.\n:::\n\n::: fragment\n**Use Case**: When you need a balance between precision and recall.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Combines precision and recall into a single metric.\n\n-   Useful in imbalanced datasets or when both types of errors are equally costly.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Not as intuitive as accuracy.\n-   Harmonic mean can be influenced heavily by lower values of either precision or recall.\n:::\n:::\n:::\n\n## **AUC-ROC** {.smaller}\n\n::: panel-tabset\n## +/- Positives\n\n![](images/trueFalsePositives.png){fig-align=\"center\" width=\"888\"}\n\n## E.g., Classifiers\n\n![](images/classifiers.png){fig-align=\"center\" width=\"888\"}\n\n## Trade-off\n\n![](images/classifierMetrics.png){fig-align=\"center\" width=\"915\"}\n\n## ROC\n\n::: columns\n::: {.column width=\"53%\"}\n![](images/roc.png){fig-align=\"center\" width=\"838\"}\n:::\n\n::: {.column width=\"47%\"}\n![](images/roc-1.png){fig-align=\"center\" width=\"635\"}\n:::\n:::\n\n::: fragment\n::: incremental\n-   **Left side** = more \"confident\" thresholds: lower recall and fewer false positive errors\n\n-   **Right side** = \"less strict\" scenarios when the threshold is low: both recall and False Positive rates are higher, ultimately reaching 100%\n:::\n:::\n\n## ROC AUC\n\n![](images/roc-auc.png){fig-align=\"center\" width=\"888\"}\n\n::: incremental\n-   Single metric to summarize the performance of models\n\n-   Common calculation - [trapezoid rule](https://en.wikipedia.org/wiki/Trapezoidal_rule)\n:::\n\n## Pros + Cons\n\n::: fragment\n**Definition**: Measures the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n:::\n\n::: fragment\n**Use Case**: Effective for balanced and imbalanced datasets, especially for binary classification problems.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Performance measurement for the classification model at various thresholds settings.\n\n-   AUC-ROC near 1 indicates a good ability to distinguish between positive and negative classes.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Can be overly optimistic in imbalanced datasets.\n\n-   Does not distinguish between types of errors.\n:::\n:::\n:::\n\n## Applying model evaluation {.smaller}\n\nUsing the decision tree model\n\n\n\n## Applying model evaluation {.smaller}\n\n::: panel-tabset\n## Accuracy\n\n::: {#abeb4855 .cell execution_count=9}\n``` {.python .cell-code}\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.48\n```\n:::\n:::\n\n\n## Precision\n\n::: {#41ab513a .cell execution_count=10}\n``` {.python .cell-code}\nprecision = precision_score(y_test, predictions, average = 'weighted') # Use 'weighted' for multiclass\nprint(f\"Precision: {precision:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.48\n```\n:::\n:::\n\n\n## Recall\n\n::: {#4d7dde0b .cell execution_count=11}\n``` {.python .cell-code}\nrecall = recall_score(y_test, predictions, average = 'weighted') # Use 'weighted' for multiclass\nprint(f\"Recall: {recall:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall: 0.48\n```\n:::\n:::\n\n\n## F1 Score\n\n::: {#216db207 .cell execution_count=12}\n``` {.python .cell-code}\nf1 = f1_score(y_test, predictions, average = 'weighted') # Use 'weighted' for multiclass\nprint(f\"F1 Score: {f1:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1 Score: 0.48\n```\n:::\n:::\n\n\n## AUC-ROC\n\n::: {#37c8269d .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\n# Binarize the output for multiclass\ny_test_binarized = label_binarize(y_test, classes = np.unique(y_train))\nn_classes = y_test_binarized.shape[1]\n\n# Get the probability predictions for each class\ny_score = dtree.predict_proba(X_test_pca)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Calculate macro-average ROC-AUC\nroc_auc_macro = np.mean(list(roc_auc.values()))\nprint(f\"Macro-average ROC-AUC: {roc_auc_macro:.2f}\")\n\n# Calculate micro-average ROC-AUC\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_score.ravel())\nroc_auc_micro = auc(fpr[\"micro\"], tpr[\"micro\"])\nprint(f\"Micro-average ROC-AUC: {roc_auc_micro:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMacro-average ROC-AUC: 0.52\nMicro-average ROC-AUC: 0.61\n```\n:::\n:::\n\n\n:::\n\n::: incremental\n-   **Binarization**: `label_binarize` is used to binarize the labels in a one-vs-all fashion which is necessary for multiclass ROC calculation.\n\n-   **Macro-average**: computes the metric independently for each class and then takes the average (treating all classes equally).\n\n-   **Micro-average**:aggregates the contributions of all classes to compute the average metric.\n:::\n\n## Conclusions\n\n::: incremental\n-   All values are relatively the same (\\~50%)\n\n-   Our risk column is unbalanced, so precision and recall are useful\n\n-   The F1 Score is best to analyze our data (balance precision and recall)\n\n-   ROC-AUC is effective in distinguishing classes\n:::\n\n## Cross validation {.smaller}\n\n::: fragment\n> A resampling method that evaluates machine learning models on a limited data sample. It involves partitioning a dataset into complementary subsets, performing the analysis on one subset (training set), and validating the analysis on the other subset (validation set).\n:::\n\n::: fragment\n**Use Case**: Widely used for assessing the effectiveness of predictive models, helping to safeguard against overfitting.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Provides a more accurate measure of a model's predictive performance compared to a simple train/test split.\n\n-   Utilizes the data efficiently as every observation is used for both training and validation.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Computationally intensive, especially for large datasets.\n\n-   Results can vary depending on how the data is divided.\n:::\n:::\n\n## Cross validation methods\n\n::: incremental\n-   Leave-One-Out Cross-Validation\n\n-   k-Fold Cross-Validation\n:::\n\n## Leave-One-Out Cross-Validation (LOOCV) {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"60%\"}\n![](images/loocv.webp){fig-align=\"left\" width=\"653\"}\n:::\n\n::: {.column width=\"40%\"}\n**Overall score**:\n\n$\\frac{score_1 + score_2 + score_3 + score_4 + score_5 + score_6}{6}$\n\n::: fragment\nFor classification the score is:\n:::\n\n::: fragment\n-   Accuracy\n-   Precision, Recall, F1-Score\n-   AUC-ROC\n:::\n:::\n:::\n\n## Pros + Cons\n\n::: fragment\n**Definition**: The number of folds equals the number of instances in the dataset. Each model is trained on all data points except one, which is used as the test set.\n:::\n\n::: fragment\n**Use Case**: Useful for small datasets where maximizing the training data is important.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Utilizes the data to its maximum extent.\n\n-   Reduces bias as each data point gets to be in the test set exactly once.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Highly computationally expensive with large datasets.\n-   High variance in the estimate of model performance as the evaluation can be highly dependent on the data points chosen as the test set.\n:::\n:::\n:::\n\n## k-Fold Cross-Validation (k-Fold) {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: columns\n::: {.column width=\"60%\"}\n![](images/kfold.webp){fig-align=\"left\" width=\"653\"}\n:::\n\n::: {.column width=\"40%\"}\n**Overall score**:\n\n$\\frac{score_1 + score_2 + score_3}{3}$\n\n::: fragment\nFor classification the score is:\n:::\n\n::: fragment\n-   Accuracy\n-   Precision, Recall, F1-Score\n-   AUC-ROC\n:::\n:::\n:::\n\n## Pros + Cons\n\n::: fragment\n**Definition**: The dataset is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set.\n:::\n\n::: fragment\n**Use Case**: Ideal for both small and medium-sized datasets and when the balance between bias and variance is crucial.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   Reduces the variance of a single trial of train/test split.\n\n-   More reliable estimate of out-of-sample performance than LOOCV.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   Still computationally intensive, especially with large k.\n-   Results can be dependent on the random division of the data into folds.\n:::\n:::\n\n## Bias-Variance Trade-off\n\n::: fragment\n**Definition**: Refers to managing the trade-off between the bias of the model (error due to overly simplistic assumptions) and its variance (error due to sensitivity to small fluctuations in the training set).\n:::\n:::\n\n## The bootstrap method {.smaller}\n\n::: panel-tabset\n## Visual\n\n![](images/bootstrap.png){fig-align=\"center\" width=\"1644\"}\n\n::: incremental\n1.  **Sample creation**\n\n2.  **Model training + evaluation**\n\n3.  **Performance aggregation**\n:::\n\n## Pros + Cons\n\n::: fragment\n**Definition**: A statistical technique using repeated sampling with replacement from a dataset to train and evaluate model performance.\n:::\n\n::: fragment\n**Use Case**: Ideal for small datasets or when assessing model performance variability. Commonly used for accuracy estimation and model validation.\n:::\n\n::: fragment\n**Pros**:\n\n::: incremental\n-   **Variability estimation**: Offers insights into the model's performance variability.\n\n-   **Flexible**: Non-parametric and adaptable to various data distributions.\n:::\n:::\n\n::: fragment\n**Cons**:\n\n::: incremental\n-   **Computationally demanding**: High computational cost due to repeated resampling and training.\n\n-   **Overfitting bias**: Can be overly optimistic for overfitting models.\n:::\n:::\n:::\n\n## Applying resampling methods {.smaller}\n\n::: panel-tabset\n## LOOCV\n\n::: {#8672c434 .cell execution_count=14}\n``` {.python .cell-code}\nimport time\nstart_time = time.time()\n\ny_train = y_train.to_numpy()\n\nloo = LeaveOneOut()\nloo_f1_scores = []\n\nfor train_index, test_index in loo.split(X_train_pca):\n    X_train_fold, X_test_fold = X_train_pca[train_index], X_train_pca[test_index]\n    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n\n    dtree.fit(X_train_fold, y_train_fold)\n    prediction = dtree.predict(X_test_fold)\n    f1 = f1_score(y_test_fold, prediction, average = 'weighted')\n    loo_f1_scores.append(f1)\n```\n:::\n\n\n::: {#9adec9aa .cell execution_count=15}\n``` {.python .cell-code}\nloo_f1_average = np.mean(loo_f1_scores)\nprint(f\"LOOCV F1-Score: {loo_f1_average:.2f}\")\nloocv_time = time.time() - start_time\nprint(f\"LOOCV took {loocv_time:.2f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLOOCV F1-Score: 0.48\nLOOCV took 244.76 seconds\n```\n:::\n:::\n\n\n## k-Fold\n\n::: {#2eb64f08 .cell execution_count=16}\n``` {.python .cell-code}\nstart_time = time.time()\n\nkf = KFold(n_splits = 5) \nkf_f1_scores = cross_val_score(dtree, X_train_pca, y_train, cv = kf, scoring = 'f1_weighted')\n```\n:::\n\n\n::: {#41f9039b .cell execution_count=17}\n``` {.python .cell-code}\nkf_f1_average = np.mean(kf_f1_scores)\nprint(f\"k-Fold F1-Score: {kf_f1_average:.2f}\")\nkfold_time = time.time() - start_time\nprint(f\"k-Fold CV took {kfold_time:.2f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-Fold F1-Score: 0.48\nk-Fold CV took 0.13 seconds\n```\n:::\n:::\n\n\n## Bootstrap\n\n::: {#fd52cba1 .cell execution_count=18}\n``` {.python .cell-code}\nstart_time = time.time()\nn_iterations = 100  \nn_size = int(len(X_train_pca) * 0.50)  \nbootstrap_f1_scores = []\n\nfor _ in range(n_iterations):\n    X_sample, y_sample = resample(X_train_pca, y_train, n_samples=n_size)\n    dtree.fit(X_sample, y_sample)\n\n    predictions = dtree.predict(X_test_pca)\n    f1 = f1_score(y_test, predictions, average='weighted')\n    bootstrap_f1_scores.append(f1)\n```\n:::\n\n\n::: {#f07efe3f .cell execution_count=19}\n``` {.python .cell-code}\nbootstrap_f1_average = np.mean(bootstrap_f1_scores)\nprint(f\"Bootstrap F1-Score: {bootstrap_f1_average:.2f}\")\nbootstrap_time = time.time() - start_time\nprint(f\"Bootstrap Method took {bootstrap_time:.2f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBootstrap F1-Score: 0.48\nBootstrap Method took 1.23 seconds\n```\n:::\n:::\n\n\n## Time spent\n\n::: {#bc5c3477 .cell execution_count=20}\n``` {.python .cell-code}\nratio = kfold_time / loocv_time\nprint(f\"Ratio of k-Fold time to LOOCV time: {ratio:.4f}\")\nratio = bootstrap_time / loocv_time\nprint(f\"Ratio of Bootstrap time to LOOCV time: {ratio:.4f}\")\nratio = kfold_time / bootstrap_time\nprint(f\"Ratio of k-Fold time to Bootstrap time: {ratio:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRatio of k-Fold time to LOOCV time: 0.0005\nRatio of Bootstrap time to LOOCV time: 0.0050\nRatio of k-Fold time to Bootstrap time: 0.1038\n```\n:::\n:::\n\n\n:::\n\n## Comparing classification methods II\n\n**Model evaluation method**:\n\n-   F1-Score\n\n**Subset method**:\n\n-   k-Fold Cross Validation\n\n## Comparing classification methods II\n\n::: panel-tabset\n## Define models\n\n::: {#0288a588 .cell execution_count=21}\n``` {.python .cell-code}\n# Logistic Regression\nord_log_reg = m.LogisticAT(alpha = 1)  \n\n# KNN\nknn = KNeighborsClassifier(n_neighbors = 5)\n\n# Decision Tree\ndtree = DecisionTreeClassifier()\n```\n:::\n\n\n## k-Fold CV\n\n::: {#3cb846f6 .cell execution_count=22}\n``` {.python .cell-code}\n# Define the number of folds\nk = 5\n\n# Logistic Regression\nord_log_reg_scores = cross_val_score(ord_log_reg, X_train_pca, y_train, cv = k, scoring = 'f1_weighted')\nord_log_reg_f1_average = np.mean(ord_log_reg_scores)\n\n# KNN\nknn_scores = cross_val_score(knn, X_train_pca, y_train, cv = k, scoring = 'f1_weighted')\nknn_f1_average = np.mean(knn_scores)\n\n# Decision Tree\ndtree_scores = cross_val_score(dtree, X_train_pca, y_train, cv = k, scoring = 'f1_weighted')\ndtree_f1_average = np.mean(dtree_scores)\n```\n:::\n\n\n## F1-Scores\n\n::: {#4cb26442 .cell execution_count=23}\n``` {.python .cell-code}\nprint(f\"Average F1-Score for Ordinal Logistic Regression: {ord_log_reg_f1_average:.2f}\")\nprint(f\"Average F1-Score for KNN: {knn_f1_average:.2f}\")\nprint(f\"Average F1-Score for Decision Tree: {dtree_f1_average:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage F1-Score for Ordinal Logistic Regression: 0.39\nAverage F1-Score for KNN: 0.49\nAverage F1-Score for Decision Tree: 0.48\n```\n:::\n:::\n\n\n:::\n\n## Conclusions\n\n::: incremental\n-   **Model evaluation** determines how well a model predicts the data\n\n    -   **F1-Score** balances **Prediction** and **Recall** (best for [uneven classes]{.underline})\n\n-   **Cross validation** and **bootstrapping** are effective ways to prevent [overfitting]{.underline}\n\n    -   **k-Fold CV** is best for [larger datasets]{.underline}\n\n-   **Ordinal Logistic Regression** performed worst!\n\n    -   **Occam's Razor** loses...\n:::\n\n## In-class Exercise\n\n::: task\nGo to [ex-06](https://{{< var website.url >}}/exercises/ex-06.html) and perform the tasks\n:::\n\n",
    "supporting": [
      "06-model-eval_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}