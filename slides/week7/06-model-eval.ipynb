{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Model Evaluation\n",
        "subtitle: Lecture 6\n",
        "title-slide-attributes:\n",
        "  data-background-image: ../minedata-bg.png\n",
        "  data-background-size: 600px, cover\n",
        "  data-slide-number: none\n",
        "format: revealjs\n",
        "auto-stretch: false\n",
        "---"
      ],
      "id": "8b36b3dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Warm up\n",
        "\n",
        "## Announcements\n",
        "\n",
        "-   HW 03 is due Wed, Feb 28, 11:59pm\n",
        "\n",
        "-   RQ #3 is due Wed, Feb 28, 11:59pm\n",
        "\n",
        "## Setup {.smaller}\n"
      ],
      "id": "752ac412"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: setup\n",
        "\n",
        "# Import all required libraries\n",
        "# Data handling and manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning models\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import mord as m\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Model evaluation and validation methods\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Metrics for model evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Utility for data preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# For advanced visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Increase font size of all Seaborn plot elements\n",
        "sns.set(font_scale = 1.25)\n",
        "\n",
        "# Set Seaborn theme\n",
        "sns.set_theme(style = \"white\")"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation\n",
        "\n",
        "## Looking back\n",
        "\n",
        "**Can we [predict high-risk credit individuals]{.underline} by their [financial traits]{.underline}?**\n",
        "\n",
        "![](images/credit-risk.jpeg){fig-align=\"center\" width=\"1200\"}\n",
        "\n",
        "::: aside\n",
        "Source: [Lending Club](https://www.lendingclub.com/info/statistics.action)\n",
        ":::\n",
        "\n",
        "## What did we do?\n",
        "\n",
        "::: incremental\n",
        "-   Used several models\n",
        "\n",
        "-   Compared and contrasted efficacy\n",
        "\n",
        "-   Concluded that we couldn't predict credit risk\n",
        ":::\n",
        "\n",
        "## Refresher on our data (with ordinal risk) {.smaller}\n"
      ],
      "id": "db57957b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "loans_class = pd.read_csv(\"data/loans_class.csv\")\n",
        "\n",
        "loans_class.head()"
      ],
      "id": "a183fc26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Our preprocessing\n"
      ],
      "id": "ce27d8d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Encode categorical variables\n",
        "categorical_columns = loans_class.select_dtypes(include = ['object', 'category']).columns.tolist()\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoders = {col: LabelEncoder() for col in categorical_columns}\n",
        "for col in categorical_columns:\n",
        "    loans_class[col] = label_encoders[col].fit_transform(loans_class[col])\n",
        "    \n",
        "# Define features and target\n",
        "X = loans_class.drop('risk', axis = 1)\n",
        "y = loans_class['risk']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Reduce dimensionality to prevent overfitting\n",
        "pca = PCA(n_components = 2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "id": "626af9e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reminder: decision boundary\n"
      ],
      "id": "44dfa269"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def decisionplot(model, X, y, resolution=216):\n",
        "    # Split the data into features (X) and the class variable (y)\n",
        "    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
        "    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
        "                         np.linspace(y_min, y_max, resolution))\n",
        "\n",
        "    # Predict outcomes for each point on the grid\n",
        "    if isinstance(model, LinearDiscriminantAnalysis):\n",
        "        # For LDA, we need to use the decision_function method\n",
        "        Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    else:\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    \n",
        "    if isinstance(model, LinearDiscriminantAnalysis):\n",
        "    # Reshape LDA decision function output appropriately\n",
        "        Z = Z.reshape(-1, 1)\n",
        "    else:\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the actual data points\n",
        "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='k', s=20)\n",
        "\n",
        "    # Overlay the decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha = 0.5)\n",
        "    \n",
        "    # Calculate the accuracy\n",
        "    predictions = model.predict(X)\n",
        "    acc = accuracy_score(y, predictions)\n",
        "    \n",
        "  \n",
        "    # Set labels for axes\n",
        "    plt.xlabel(X.columns[0])\n",
        "    plt.ylabel(X.columns[1])\n",
        "\n",
        "    plt.show()"
      ],
      "id": "dd8204b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What models will we reuse? {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Ordinal Logistic Regression\n"
      ],
      "id": "b8ae279d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "ord_log_reg = m.LogisticAT(alpha = 1)  \n",
        "ord_log_reg.fit(X_train_pca, y_train)\n",
        "\n",
        "ordinal_predictions = ord_log_reg.predict(X_test_pca)\n",
        "print(\"Ordinal Logistic Regression Accuracy:\", accuracy_score(y_test, ordinal_predictions).round(3))\n",
        "\n",
        "decisionplot(ord_log_reg, pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']), y_train)\n",
        "plt.show()"
      ],
      "id": "88c1a900",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Nearest Neighbors\n"
      ],
      "id": "47cb2a9f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "predictions = knn.predict(X_test_pca)\n",
        "\n",
        "decisionplot(knn, pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']), y_train)\n",
        "plt.show()"
      ],
      "id": "3c15756f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Trees\n"
      ],
      "id": "61feff62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree.fit(X_train_pca, y_train)\n",
        "predictions = dtree.predict(X_test_pca)\n",
        "\n",
        "decisionplot(dtree, pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']), y_train)\n",
        "plt.show()"
      ],
      "id": "2294a066",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Model Evaluation {.smaller}\n",
        "\n",
        "> **Model selection** is the task of selecting a model from among various candidates on the basis of performance criterion to choose the best one. In the context of [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") and more generally [statistical analysis](https://en.wikipedia.org/wiki/Statistical_analysis \"Statistical analysis\"), this may be the selection of a [statistical model](https://en.wikipedia.org/wiki/Statistical_model \"Statistical model\") from a set of candidate models, given data. Typically, [Occam's Razor](https://en.wikipedia.org/wiki/Occam%E2%80%99s_Razor#Science_and_the_scientific_method) is the best approach\n",
        "\n",
        "::: fragment\n",
        "#### Broadly we will focus on two categories\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Evaluating performance**\n",
        "\n",
        "-   The process of assessing the performance of a machine learning model using various metrics, such as accuracy, precision, recall, and F1 score, to determine how effectively it makes predictions on new, unseen data.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Validation methods**\n",
        "\n",
        "-   The technique of verifying a machine learning model's performance and reliability on a separate dataset (validation set) that was not used during the model's training, to ensure that it generalizes well to new data.\n",
        ":::\n",
        "\n",
        "## Evaluating performance\n",
        "\n",
        "**Back to Confusion Matrices**\n",
        "\n",
        "![](images/confusion.jpeg){fig-align=\"center\" width=\"1067\"}\n",
        "\n",
        "## Accuracy {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "![](images/accuracy.png){fig-align=\"left\" width=\"612\"}\n",
        "\n",
        "**Error Rate** $= \\frac{FP + FN}{TP + TN + FP + FN}$\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Best for balanced datasets where each class is approximately equally represented.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Intuitive and easy to understand.\n",
        "\n",
        "-   Useful when the costs of false positives and false negatives are similar.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Can be misleading in imbalanced datasets (where one class significantly outnumbers the other).\n",
        "-   Doesn't consider the type of errors (false positives vs. false negatives).\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Precision {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "![](images/precision.png){fig-align=\"left\" width=\"612\"}\n",
        "\n",
        "**Error Rate** $= \\frac{TP}{TP + FP}$\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: The ratio of true positives to the sum of true and false positives. It's a measure of a classifier's exactness.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Important when the cost of false positives is high (e.g., in spam detection).\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Focuses on the positive class's predictive power.\n",
        "\n",
        "-   Useful in imbalanced datasets to evaluate the performance of the minority class.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Does not consider false negatives (i.e., how many positive cases were missed).\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Recall (Sensitivity) {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "![](images/recall.png){fig-align=\"left\" width=\"612\"}\n",
        "\n",
        "**Error Rate** $= \\frac{TP}{TP + FN}$\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: The ratio of true positives to the sum of true positives and false negatives. It's a measure of a classifier's completeness.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Crucial when the cost of false negatives is high (e.g., in disease screening).\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Ensures that most positive examples are correctly recognized.\n",
        "\n",
        "-   Very useful in imbalanced datasets for evaluating how well the minority class is being predicted\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Does not consider false positives (i.e., can lead to many false alarms).\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## F1 Score {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "![](images/f1-score.png){fig-align=\"left\" width=\"612\"}\n",
        "\n",
        "**Error Rate** $= 2\\times\\frac{precision \\times recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}$\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: The harmonic mean of precision and recall.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: When you need a balance between precision and recall.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Combines precision and recall into a single metric.\n",
        "\n",
        "-   Useful in imbalanced datasets or when both types of errors are equally costly.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Not as intuitive as accuracy.\n",
        "-   Harmonic mean can be influenced heavily by lower values of either precision or recall.\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## **AUC-ROC** {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## +/- Positives\n",
        "\n",
        "![](images/trueFalsePositives.png){fig-align=\"center\" width=\"888\"}\n",
        "\n",
        "## E.g., Classifiers\n",
        "\n",
        "![](images/classifiers.png){fig-align=\"center\" width=\"888\"}\n",
        "\n",
        "## Trade-off\n",
        "\n",
        "![](images/classifierMetrics.png){fig-align=\"center\" width=\"915\"}\n",
        "\n",
        "## ROC\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"53%\"}\n",
        "![](images/roc.png){fig-align=\"center\" width=\"838\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"47%\"}\n",
        "![](images/roc-1.png){fig-align=\"center\" width=\"635\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "::: incremental\n",
        "-   **Left side** = more \"confident\" thresholds: lower recall and fewer false positive errors\n",
        "\n",
        "-   **Right side** = \"less strict\" scenarios when the threshold is low: both recall and False Positive rates are higher, ultimately reaching 100%\n",
        ":::\n",
        ":::\n",
        "\n",
        "## ROC AUC\n",
        "\n",
        "![](images/roc-auc.png){fig-align=\"center\" width=\"888\"}\n",
        "\n",
        "::: incremental\n",
        "-   Single metric to summarize the performance of models\n",
        "\n",
        "-   Common calculation - [trapezoid rule](https://en.wikipedia.org/wiki/Trapezoidal_rule)\n",
        ":::\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: Measures the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Effective for balanced and imbalanced datasets, especially for binary classification problems.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Performance measurement for the classification model at various thresholds settings.\n",
        "\n",
        "-   AUC-ROC near 1 indicates a good ability to distinguish between positive and negative classes.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Can be overly optimistic in imbalanced datasets.\n",
        "\n",
        "-   Does not distinguish between types of errors.\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Applying model evaluation {.smaller}\n",
        "\n",
        "Using the decision tree model\n"
      ],
      "id": "c4eaf414"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree.fit(X_train_pca, y_train)\n",
        "predictions = dtree.predict(X_test_pca)"
      ],
      "id": "6766205a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying model evaluation {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Accuracy\n"
      ],
      "id": "7cca5084"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "id": "580b768f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision\n"
      ],
      "id": "821122c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "precision = precision_score(y_test, predictions, average = 'weighted') # Use 'weighted' for multiclass\n",
        "print(f\"Precision: {precision:.2f}\")"
      ],
      "id": "5d935d7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recall\n"
      ],
      "id": "1cbb44da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "recall = recall_score(y_test, predictions, average = 'weighted') # Use 'weighted' for multiclass\n",
        "print(f\"Recall: {recall:.2f}\")"
      ],
      "id": "b8426ec1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F1 Score\n"
      ],
      "id": "c634f830"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f1 = f1_score(y_test, predictions, average = 'weighted') # Use 'weighted' for multiclass\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "id": "4edeb7bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AUC-ROC\n"
      ],
      "id": "7eefde68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# Binarize the output for multiclass\n",
        "y_test_binarized = label_binarize(y_test, classes = np.unique(y_train))\n",
        "n_classes = y_test_binarized.shape[1]\n",
        "\n",
        "# Get the probability predictions for each class\n",
        "y_score = dtree.predict_proba(X_test_pca)\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Calculate macro-average ROC-AUC\n",
        "roc_auc_macro = np.mean(list(roc_auc.values()))\n",
        "print(f\"Macro-average ROC-AUC: {roc_auc_macro:.2f}\")\n",
        "\n",
        "# Calculate micro-average ROC-AUC\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_score.ravel())\n",
        "roc_auc_micro = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "print(f\"Micro-average ROC-AUC: {roc_auc_micro:.2f}\")"
      ],
      "id": "1b2bb185",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: incremental\n",
        "-   **Binarization**: `label_binarize` is used to binarize the labels in a one-vs-all fashion which is necessary for multiclass ROC calculation.\n",
        "\n",
        "-   **Macro-average**: computes the metric independently for each class and then takes the average (treating all classes equally).\n",
        "\n",
        "-   **Micro-average**:aggregates the contributions of all classes to compute the average metric.\n",
        ":::\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "::: incremental\n",
        "-   All values are relatively the same (\\~50%)\n",
        "\n",
        "-   Our risk column is unbalanced, so precision and recall are useful\n",
        "\n",
        "-   The F1 Score is best to analyze our data (balance precision and recall)\n",
        "\n",
        "-   ROC-AUC is effective in distinguishing classes\n",
        ":::\n",
        "\n",
        "## Cross validation {.smaller}\n",
        "\n",
        "::: fragment\n",
        "> A resampling method that evaluates machine learning models on a limited data sample. It involves partitioning a dataset into complementary subsets, performing the analysis on one subset (training set), and validating the analysis on the other subset (validation set).\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Widely used for assessing the effectiveness of predictive models, helping to safeguard against overfitting.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Provides a more accurate measure of a model's predictive performance compared to a simple train/test split.\n",
        "\n",
        "-   Utilizes the data efficiently as every observation is used for both training and validation.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Computationally intensive, especially for large datasets.\n",
        "\n",
        "-   Results can vary depending on how the data is divided.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Cross validation methods\n",
        "\n",
        "::: incremental\n",
        "-   Leave-One-Out Cross-Validation\n",
        "\n",
        "-   k-Fold Cross-Validation\n",
        ":::\n",
        "\n",
        "## Leave-One-Out Cross-Validation (LOOCV) {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/loocv.webp){fig-align=\"left\" width=\"653\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "**Overall score**:\n",
        "\n",
        "$\\frac{score_1 + score_2 + score_3 + score_4 + score_5 + score_6}{6}$\n",
        "\n",
        "::: fragment\n",
        "For classification the score is:\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "-   Accuracy\n",
        "-   Precision, Recall, F1-Score\n",
        "-   AUC-ROC\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: The number of folds equals the number of instances in the dataset. Each model is trained on all data points except one, which is used as the test set.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Useful for small datasets where maximizing the training data is important.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Utilizes the data to its maximum extent.\n",
        "\n",
        "-   Reduces bias as each data point gets to be in the test set exactly once.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Highly computationally expensive with large datasets.\n",
        "-   High variance in the estimate of model performance as the evaluation can be highly dependent on the data points chosen as the test set.\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## k-Fold Cross-Validation (k-Fold) {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/kfold.webp){fig-align=\"left\" width=\"653\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "**Overall score**:\n",
        "\n",
        "$\\frac{score_1 + score_2 + score_3}{3}$\n",
        "\n",
        "::: fragment\n",
        "For classification the score is:\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "-   Accuracy\n",
        "-   Precision, Recall, F1-Score\n",
        "-   AUC-ROC\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: The dataset is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Ideal for both small and medium-sized datasets and when the balance between bias and variance is crucial.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   Reduces the variance of a single trial of train/test split.\n",
        "\n",
        "-   More reliable estimate of out-of-sample performance than LOOCV.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   Still computationally intensive, especially with large k.\n",
        "-   Results can be dependent on the random division of the data into folds.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Bias-Variance Trade-off\n",
        "\n",
        "::: fragment\n",
        "**Definition**: Refers to managing the trade-off between the bias of the model (error due to overly simplistic assumptions) and its variance (error due to sensitivity to small fluctuations in the training set).\n",
        ":::\n",
        ":::\n",
        "\n",
        "## The bootstrap method {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## Visual\n",
        "\n",
        "![](images/bootstrap.png){fig-align=\"center\" width=\"1644\"}\n",
        "\n",
        "::: incremental\n",
        "1.  **Sample creation**\n",
        "\n",
        "2.  **Model training + evaluation**\n",
        "\n",
        "3.  **Performance aggregation**\n",
        ":::\n",
        "\n",
        "## Pros + Cons\n",
        "\n",
        "::: fragment\n",
        "**Definition**: A statistical technique using repeated sampling with replacement from a dataset to train and evaluate model performance.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Use Case**: Ideal for small datasets or when assessing model performance variability. Commonly used for accuracy estimation and model validation.\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Pros**:\n",
        "\n",
        "::: incremental\n",
        "-   **Variability estimation**: Offers insights into the model's performance variability.\n",
        "\n",
        "-   **Flexible**: Non-parametric and adaptable to various data distributions.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "**Cons**:\n",
        "\n",
        "::: incremental\n",
        "-   **Computationally demanding**: High computational cost due to repeated resampling and training.\n",
        "\n",
        "-   **Overfitting bias**: Can be overly optimistic for overfitting models.\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Applying resampling methods {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "## LOOCV\n"
      ],
      "id": "d72d4a19"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "y_train = y_train.to_numpy()\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "loo_f1_scores = []\n",
        "\n",
        "for train_index, test_index in loo.split(X_train_pca):\n",
        "    X_train_fold, X_test_fold = X_train_pca[train_index], X_train_pca[test_index]\n",
        "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    dtree.fit(X_train_fold, y_train_fold)\n",
        "    prediction = dtree.predict(X_test_fold)\n",
        "    f1 = f1_score(y_test_fold, prediction, average = 'weighted')\n",
        "    loo_f1_scores.append(f1)"
      ],
      "id": "d6b4b6d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loo_f1_average = np.mean(loo_f1_scores)\n",
        "print(f\"LOOCV F1-Score: {loo_f1_average:.2f}\")\n",
        "loocv_time = time.time() - start_time\n",
        "print(f\"LOOCV took {loocv_time:.2f} seconds\")"
      ],
      "id": "ef222342",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-Fold\n"
      ],
      "id": "3558fe32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "start_time = time.time()\n",
        "\n",
        "kf = KFold(n_splits = 5) \n",
        "kf_f1_scores = cross_val_score(dtree, X_train_pca, y_train, cv = kf, scoring = 'f1_weighted')"
      ],
      "id": "23443164",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kf_f1_average = np.mean(kf_f1_scores)\n",
        "print(f\"k-Fold F1-Score: {kf_f1_average:.2f}\")\n",
        "kfold_time = time.time() - start_time\n",
        "print(f\"k-Fold CV took {kfold_time:.2f} seconds\")"
      ],
      "id": "a3d23676",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bootstrap\n"
      ],
      "id": "b05956ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_time = time.time()\n",
        "n_iterations = 100  \n",
        "n_size = int(len(X_train_pca) * 0.50)  \n",
        "bootstrap_f1_scores = []\n",
        "\n",
        "for _ in range(n_iterations):\n",
        "    X_sample, y_sample = resample(X_train_pca, y_train, n_samples=n_size)\n",
        "    dtree.fit(X_sample, y_sample)\n",
        "\n",
        "    predictions = dtree.predict(X_test_pca)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "    bootstrap_f1_scores.append(f1)"
      ],
      "id": "56b658f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bootstrap_f1_average = np.mean(bootstrap_f1_scores)\n",
        "print(f\"Bootstrap F1-Score: {bootstrap_f1_average:.2f}\")\n",
        "bootstrap_time = time.time() - start_time\n",
        "print(f\"Bootstrap Method took {bootstrap_time:.2f} seconds\")"
      ],
      "id": "98ca57b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time spent\n"
      ],
      "id": "6680c695"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ratio = kfold_time / loocv_time\n",
        "print(f\"Ratio of k-Fold time to LOOCV time: {ratio:.4f}\")\n",
        "ratio = bootstrap_time / loocv_time\n",
        "print(f\"Ratio of Bootstrap time to LOOCV time: {ratio:.4f}\")\n",
        "ratio = kfold_time / bootstrap_time\n",
        "print(f\"Ratio of k-Fold time to Bootstrap time: {ratio:.4f}\")"
      ],
      "id": "c84942de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Comparing classification methods II\n",
        "\n",
        "**Model evaluation method**:\n",
        "\n",
        "-   F1-Score\n",
        "\n",
        "**Subset method**:\n",
        "\n",
        "-   k-Fold Cross Validation\n",
        "\n",
        "## Comparing classification methods II\n",
        "\n",
        "::: panel-tabset\n",
        "## Define models\n"
      ],
      "id": "bf3dffca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logistic Regression\n",
        "ord_log_reg = m.LogisticAT(alpha = 1)  \n",
        "\n",
        "# KNN\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "\n",
        "# Decision Tree\n",
        "dtree = DecisionTreeClassifier()"
      ],
      "id": "d6afcfee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-Fold CV\n"
      ],
      "id": "f8c9d314"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the number of folds\n",
        "k = 5\n",
        "\n",
        "# Logistic Regression\n",
        "ord_log_reg_scores = cross_val_score(ord_log_reg, X_train_pca, y_train, cv = k, scoring = 'f1_weighted')\n",
        "ord_log_reg_f1_average = np.mean(ord_log_reg_scores)\n",
        "\n",
        "# KNN\n",
        "knn_scores = cross_val_score(knn, X_train_pca, y_train, cv = k, scoring = 'f1_weighted')\n",
        "knn_f1_average = np.mean(knn_scores)\n",
        "\n",
        "# Decision Tree\n",
        "dtree_scores = cross_val_score(dtree, X_train_pca, y_train, cv = k, scoring = 'f1_weighted')\n",
        "dtree_f1_average = np.mean(dtree_scores)"
      ],
      "id": "beaf2bae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F1-Scores\n"
      ],
      "id": "ebae3106"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Average F1-Score for Ordinal Logistic Regression: {ord_log_reg_f1_average:.2f}\")\n",
        "print(f\"Average F1-Score for KNN: {knn_f1_average:.2f}\")\n",
        "print(f\"Average F1-Score for Decision Tree: {dtree_f1_average:.2f}\")"
      ],
      "id": "0b230a0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "::: incremental\n",
        "-   **Model evaluation** determines how well a model predicts the data\n",
        "\n",
        "    -   **F1-Score** balances **Prediction** and **Recall** (best for [uneven classes]{.underline})\n",
        "\n",
        "-   **Cross validation** and **bootstrapping** are effective ways to prevent [overfitting]{.underline}\n",
        "\n",
        "    -   **k-Fold CV** is best for [larger datasets]{.underline}\n",
        "\n",
        "-   **Ordinal Logistic Regression** performed worst!\n",
        "\n",
        "    -   **Occam's Razor** loses...\n",
        ":::"
      ],
      "id": "56835aca"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}